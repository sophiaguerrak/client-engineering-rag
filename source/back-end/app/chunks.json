[
    "Front cover\nSimplify Your AI Journey:\nUnleashing the Power of AI\nwith IBM watsonx.ai\nDeepak Rangarao Matthew Price\nPhillip Gerrard Shirley Shum\nCharley Beller Mark Simmonds\nCarl Broker\nDaniele Comi\nLakshmana Ekambaram\nShuvanker Ghosh\nKaren Medhat\nPayal Patel\nArtificial Intelligence\nData and AI",
    "Payal Patel\nArtificial Intelligence\nData and AI\nRedbooks",
    "--- Table 1 from page 1 ---\nFront cover\nSimplify Your AI Journey:\nUnleashing the Power of AI\nwith IBM watsonx.ai\nDeepak Rangarao Matthew Price\nPhillip Gerrard Shirley Shum\nCharley Beller Mark Simmonds\nCarl Broker\nDaniele Comi\nLakshmana Ekambaram\nShuvanker Ghosh\nKaren Medhat\nPayal Patel",
    "Shuvanker Ghosh\nKaren Medhat\nPayal Patel\nArtificial Intelligence\nData and AI\nRedbooks",
    "IBM Redbooks\nSimplify Your AI Journey: Unleashing the Power of AI\nwith IBM watsonx.ai\nJanuary 2025\nSG24-8574-00",
    "Note: Before using this information and the product it supports, read the information in “Notices” on\npagevii.\nFirst Edition (January 2025)\nThis edition applies to Version 2, Release 1, Modification x of IBM watsonx.ai.",
    "© Copyright International Business Machines Corporation 2025. All rights reserved.\nNote to U.S. Government Users Restricted Rights -- Use, duplication or disclosure restricted by GSA ADP Schedule\nContract with IBM Corp.",
    "Contents\nNotices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .vii\nTrademarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii",
    "Foreword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix",
    "Authors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .x\nNow you can become a published author, too! . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xii",
    "Comments welcome. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xii\nStay connected to IBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii",
    "Chapter 1. Competing with artificial intelligence. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.1 Competing with AI. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2",
    "1.2 Challenges in building and deploying AI models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.2.1 Technical considerations for building and deploying AI models . . . . . . . . . . . . . . . 5",
    "1.3 Opportunities around using AI on trusted data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.3.1 Enhancing decision-making with accurate insights. . . . . . . . . . . . . . . . . . . . . . . . . 6",
    "1.3.2 Driving operational efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.3.3 Accelerating innovation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7",
    "1.3.4 Enhancing governance and compliance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.3.5 Unlocking new revenue streams. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7",
    "1.3.6 Transforming industries with AI and trusted data . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.4 Improving AI model reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8",
    "1.4.1 Enabling cross-enterprise collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.4.2 Enhancing real-time decision-making. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9",
    "1.4.3 Scaling AI-driven ecosystems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.4.4 Driving sustainability and environmental, social, and governance goals . . . . . . . 10",
    "1.4.5 Personalizing customer experiences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n1.5 Creating new AI-enabled products and services. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11",
    "Chapter 2. Introducing IBM watsonx.ai. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.1 Overview of watsonx.ai. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14",
    "2.1.1 Key capabilities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.1.2 The watsonx.ai architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14",
    "2.1.3 watsonx.ai empowering IBM Software offerings. . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.1.4 Benefits of using watsonx.ai for businesses. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15",
    "2.2 Synergy between watsonx.ai and other components in the watsonx platform . . . . . . . 15\n2.2.1 Synergy between watsonx.ai and watsonx.data. . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2.2 Synergy between watsonx.ai and watsonx.governance. . . . . . . . . . . . . . . . . . . . 16",
    "2.3 Business impact of these synergies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nChapter 3. Tools for diverse data science teams. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17",
    "3.1 Key personas for watsonx.ai. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.1.1 Data scientists. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18",
    "3.1.2 Machine learning engineers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.1.3 AI engineers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19",
    "3.2 Low-code, no-code, and full-code tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3.2.1 No-code, low-code, and full-code tools on IBM watsonx.ai. . . . . . . . . . . . . . . . . . 20",
    "Chapter 4. Building and using artificial intelligence models . . . . . . . . . . . . . . . . . . . . 27\n© Copyright IBM Corp. 2025. iii",
    "4.1 Prerequisites and assumptions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4.2 How to use this chapter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28",
    "4.3 Building and using AI models in watsonx.ai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4.3.1 Overview of the watsonx.ai platform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28",
    "4.3.2 Key features and capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4.4 Getting started with watsonx.ai: Setting up the environment . . . . . . . . . . . . . . . . . . . . 29",
    "4.5 Data preparation and ingestion for AI model building. . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.5.1 Understanding the importance of data in AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34",
    "4.5.2 Preparing and cleaning data: data quality considerations. . . . . . . . . . . . . . . . . . . 35\n4.5.3 Handling missing data, outliers, and bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35",
    "4.5.4 Ingesting data into watsonx.ai Studio. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n4.5.5 Connecting to data repositories and cloud services . . . . . . . . . . . . . . . . . . . . . . . 36",
    "4.6 Building AI models in watsonx.ai. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n4.6.1 Choosing the right model for your use case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38",
    "4.6.2 Model creation workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n4.7 Deploying AI models in watsonx.ai. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40",
    "4.7.1 watsonx.ai Studio deployments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n4.8 watsonx.ai LLM deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45",
    "4.8.1 Model packaging and exporting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4.9 Operationalizing machine learning and LLM models . . . . . . . . . . . . . . . . . . . . . . . . . . 50",
    "4.9.1 Calling ML models by using API calls. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n4.9.2 Calling Prompt Lab LLM models by using API calls . . . . . . . . . . . . . . . . . . . . . . . 52",
    "4.9.3 IBM watsonx Assistant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.10 Additional information and where to go next. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54",
    "4.10.1 Additional support and documentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.10.2 watsonx.ai API reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55",
    "4.10.3 watsonx.ai data pipeline and orchestration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nChapter 5. Advanced capabilities of watsonx.ai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57",
    "5.1 Prompt engineering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n5.1.1 Prompting techniques. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58",
    "5.1.2 Importance of system tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n5.1.3 Model-specific peculiarities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59",
    "5.1.4 How watsonx.ai supports prompt engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5.2 Multitask prompt tuning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61",
    "5.2.1 Prompt tuning parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n5.2.2 Interdependencies and holistic tuning strategies . . . . . . . . . . . . . . . . . . . . . . . . . 63",
    "5.3 Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n5.3.1 Challenges with fine-tuning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64",
    "5.3.2 How watsonx.ai addresses fine-tuning challenges. . . . . . . . . . . . . . . . . . . . . . . . 65\n5.4 InstructLab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67",
    "5.4.1 Advantages of InstructLab . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n5.4.2 How to use InstructLab. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71",
    "5.4.3 InstructLab on watsonx.ai Software-as-a-Service. . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.4.4 InstructLab use case examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84",
    "Chapter 6. Artificial intelligence agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n6.1 What makes an AI agent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88",
    "6.2 Why AI agents are needed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n6.3 Multiple AI agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95",
    "6.4 AI agents on watsonx.ai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6.5 AI agents use case examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107",
    "Chapter 7. Use cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\niv Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "7.1 Using RAG to aid a medical school admissions office . . . . . . . . . . . . . . . . . . . . . . . . 110\n7.1.1 The challenge. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110",
    "7.1.2 The solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n7.1.3 Special considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111",
    "7.2 Embedding workflow automation to streamline recommendations. . . . . . . . . . . . . . . 111\n7.2.1 The challenge. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111",
    "7.2.2 The solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n7.2.3 Special considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113",
    "Abbreviations and acronyms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\nRelated publications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117",
    "IBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\nOnline resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117",
    "Help from IBM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\nContents v",
    "vi Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Notices\nThis information was developed for products and services offered in the US. This material might be available\nfrom IBM in other languages. However, you may be required to own a copy of the product or product version in\nthat language in order to access it.",
    "that language in order to access it.\nIBM may not offer the products, services, or features discussed in this document in other countries. Consult\nyour local IBM representative for information on the products and services currently available in your area. Any",
    "reference to an IBM product, program, or service is not intended to state or imply that only that IBM product,\nprogram, or service may be used. Any functionally equivalent product, program, or service that does not",
    "infringe any IBM intellectual property right may be used instead. However, it is the user’s responsibility to\nevaluate and verify the operation of any non-IBM product, program, or service.\nIBM may have patents or pending patent applications covering subject matter described in this document. The",
    "furnishing of this document does not grant you any license to these patents. You can send license inquiries, in\nwriting, to:\nIBM Director of Licensing, IBM Corporation, North Castle Drive, MD-NC119, Armonk, NY 10504-1785, US",
    "INTERNATIONAL BUSINESS MACHINES CORPORATION PROVIDES THIS PUBLICATION “AS IS”\nWITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED\nTO, THE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A",
    "PARTICULAR PURPOSE. Some jurisdictions do not allow disclaimer of express or implied warranties in\ncertain transactions, therefore, this statement may not apply to you.\nThis information could include technical inaccuracies or typographical errors. Changes are periodically made",
    "to the information herein; these changes will be incorporated in new editions of the publication. IBM may make\nimprovements and/or changes in the product(s) and/or the program(s) described in this publication at any time\nwithout notice.",
    "without notice.\nAny references in this information to non-IBM websites are provided for convenience only and do not in any\nmanner serve as an endorsement of those websites. The materials at those websites are not part of the",
    "materials for this IBM product and use of those websites is at your own risk.\nIBM may use or distribute any of the information you provide in any way it believes appropriate without\nincurring any obligation to you.",
    "incurring any obligation to you.\nThe performance data and client examples cited are presented for illustrative purposes only. Actual\nperformance results may vary depending on specific configurations and operating conditions.",
    "Information concerning non-IBM products was obtained from the suppliers of those products, their published\nannouncements or other publicly available sources. IBM has not tested those products and cannot confirm the",
    "accuracy of performance, compatibility or any other claims related to non-IBM products. Questions on the\ncapabilities of non-IBM products should be addressed to the suppliers of those products.",
    "Statements regarding IBM’s future direction or intent are subject to change or withdrawal without notice, and\nrepresent goals and objectives only.\nThis information contains examples of data and reports used in daily business operations. To illustrate them",
    "as completely as possible, the examples include the names of individuals, companies, brands, and products.\nAll of these names are fictitious and any similarity to actual people or business enterprises is entirely\ncoincidental.\nCOPYRIGHT LICENSE:",
    "coincidental.\nCOPYRIGHT LICENSE:\nThis information contains sample application programs in source language, which illustrate programming\ntechniques on various operating platforms. You may copy, modify, and distribute these sample programs in",
    "any form without payment to IBM, for the purposes of developing, using, marketing or distributing application\nprograms conforming to the application programming interface for the operating platform for which the sample",
    "programs are written. These examples have not been thoroughly tested under all conditions. IBM, therefore,\ncannot guarantee or imply reliability, serviceability, or function of these programs. The sample programs are",
    "provided “AS IS”, without warranty of any kind. IBM shall not be liable for any damages arising out of your use\nof the sample programs.\n© Copyright IBM Corp. 2025. vii",
    "Trademarks\nIBM, the IBM logo, and ibm.com are trademarks or registered trademarks of International Business Machines\nCorporation, registered in many jurisdictions worldwide. Other product and service names might be",
    "trademarks of IBM or other companies. A current list of IBM trademarks is available on the web at “Copyright\nand trademark information” at https://www.ibm.com/legal/copytrade.shtml\nThe following terms are trademarks or registered trademarks of International Business Machines Corporation,",
    "and might also be trademarks or registered trademarks in other countries.\nCloudant® IBM Instana™ Orchestrate®\nCognos® IBM Spectrum® Redbooks®\nDataStage® IBM Watson® Redbooks (logo) ®\nDb2® Informix® SPSS®\nIBM® InfoSphere® Turbonomic®\nIBM API Connect® Instana® z/OS®\nIBM Cloud® Netezza®",
    "IBM Cloud® Netezza®\nThe following terms are trademarks of other companies:\nThe registered trademark Linux® is used pursuant to a sublicense from the Linux Foundation, the exclusive\nlicensee of Linus Torvalds, owner of the mark on a worldwide basis.",
    "Microsoft, and the Windows logo are trademarks of Microsoft Corporation in the United States, other\ncountries, or both.\nJava, and all Java-based trademarks and logos are trademarks or registered trademarks of Oracle and/or its\naffiliates.",
    "affiliates.\nRed Hat, Fedora, OpenShift, are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in\nthe United States and other countries.\nRStudio, and the RStudio logo are registered trademarks of RStudio, Inc.",
    "Other company, product, or service names may be trademarks or service marks of others.\nviii Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Foreword\nThis IBM Redbooks® publication is part of a trilogy that positions and explains IBM watsonx,\nwhich is IBM’s strategic artificial intelligence (AI) and data platform. Each book focuses on\none of the three main components of the watsonx platform:",
    "(cid:2) IBM watsonx.ai: A next-generation enterprise studio for AI builders to train, validate, tune,\nand deploy both traditional machine learning (ML) and new generative AI (gen AI)\ncapabilities that are powered by foundation models (FMs).",
    "(cid:2) IBM watsonx.data: A fit-for-purpose data store that is built on an open lakehouse\narchitecture, and is optimized for different and governed data and AI workloads.\n(cid:2) IBM watsonx.governance: A set of AI Governance capabilities that enables trusted AI",
    "workflows, which help organizations implement and comply with ever-changing industry\nand government regulations.\nOrganizations have long recognized the value that IBM Redbooks publications provide in\nguiding them with best practices, frameworks, clear explanations, and use cases as part of",
    "their solution evaluations and implementations.\nThis trilogy of books was possible because of close collaboration among many skilled and\ntalented authors that were selected from IBM Technical Sales, IBM Development, IBM Expert",
    "Labs, IBM Client Success Management, and consulting services organizations to use their\ndiverse skills, experiences, and technical knowledge across the watsonx platform.\nThanks to the authors, contributors, reviewers, and the IBM Redbooks team for their",
    "dedication, time, and effort in making this publication a valuable asset that organizations can\nuse as part of their journey to AI.\nThanks to Mark Simmonds and Deepak Rangarao for taking the lead in shaping this request\ninto yet another successful IBM Redbooks project.",
    "into yet another successful IBM Redbooks project.\nSteve Astorino, IBM General Manager - Development, Data, AI, and Sustainability\nPreface\nIBM watsonx is IBM’s strategic AI and data platform. This book focuses on watsonx.ai, one of",
    "the three main components of the platform. IBM watsonx.ai is a next-generation enterprise\nstudio that you can use to train, validate (test), tune, and deploy both traditional ML and new\ngen AI capabilities, which are powered by FMs through an open and intuitive user interface",
    "(UI). This AI studio provides a range of FMs, training and tuning tools, and a cost-effective\ninfrastructure that facilitates the entire data and AI lifecycle, from data preparation through\nmodel development, deployment, and monitoring. The studio also includes an FM library that",
    "provides IBM® curated and trained FMs. FMs use a large, curated set of enterprise data that\nis backed by a robust filtering and cleansing process, and with an auditable data lineage.\nThese models are trained on language and other modalities, such as code, time-series data,",
    "tabular data, geospatial data, and IT events data.\n© Copyright IBM Corp. 2025. ix",
    "Here are some examples of the model categories:\n(cid:2) fm.code: Models that automatically generate code for developers through a\nnatural-language interface to boost developer productivity and enable the automation of\nmany IT tasks.",
    "many IT tasks.\n(cid:2) fm.NLP: A collection of large language models (LLMs) for specific or industry-specific\ndomains that use curated data to help mitigate bias and quickly make domains\ncustomizable by using client data.",
    "customizable by using client data.\n(cid:2) fm.geospatial: Models that are built on climate and remote sensing data to help\norganizations understand and plan for changes in natural disaster patterns, biodiversity,\nland use, and other geophysical processes that might impact their businesses",
    "The watsonx.ai studio builds on Hugging Face open-source libraries, which offer thousands of\nHugging Face open models and datasets. Users can leverage the power of IBM Granite\nLLMs, along with the latest Mistral, Llama, and other third-party LLMs. It is part of IBM's",
    "commitment to deliver an open ecosystem approach that enables users to leverage the best\nmodels and architecture for their unique business needs.\nThis IBM Redbooks publication provides a broad understanding of watsonx.ai concepts, its",
    "architecture, and the services that are available with the product. Also, several common use\ncases and scenarios are included that should help you better understand the capabilities of\nthis product. Code samples of common scenarios are available at this GitHub repository. For",
    "more examples, which include using Instructlab and AI agents, see this GitHub repository.\nThis publication is for watsonx customers who seek best practices and real-world examples of\nhow to best implement their solutions while optimizing the value of their existing and future",
    "technology, AI, data, and skills investments.\nNote: Here are the other books in the trilogy:\n(cid:2) Simplify Your AI Journey: Ensuring Trustworthy AI with IBM watsonx.governance,\nSG24-8573\n(cid:2) Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.data,\nSG24-8570\nAuthors",
    "SG24-8570\nAuthors\nThis book was produced by a team of specialists from around the world:\nDeepak Rangarao is an IBM Distinguished Engineer and CTO who is responsible for\nTechnical Sales-Cloud Paks. He leads the technical sales team that helps organizations",
    "modernize their technology landscape with IBM Cloud Paks. He has broad, cross-industry\nexperience in the data warehousing and analytics space from building analytic applications at\nlarge organizations and performing technical pre-sales for start-ups and large enterprise",
    "software vendors. Deepak has co-authored several books on many topics, such as OLAP\nanalytics, change data capture, data warehousing, and object storage. He is a regular\nspeaker at technical conferences. He is a certified technical specialist in Red Hat OpenShift,",
    "Apache Spark, Microsoft SQL Server, and web development technologies.\nx Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Phillip Gerrard is a Project Leader for the International Technical Support Organization\nworking out of Beaverton, Oregon. As part of IBM for over 15 years, he has authored and\ncontributed to hundreds of technical documents that were published to IBM.com, and worked",
    "directly with IBM's largest customers to resolve critical situations. As a team lead and subject\nmatter expert (SME) for the IBM Spectrum® Protect support team, he is experienced in\nleading and growing international teams of talented IBM employees by developing and",
    "implementing team processes, and creating and delivering education. Phillip holds a degree\nin computer science and business administration from Oregon State University.\nCharley Beller is a Principal Data Scientist and IBM Master Inventor. He works with clients",
    "as the Worldwide Solution Engineering Lead for watsonx.ai and AI Assistants within\nTechnology Expert Labs. Charley has been working with IBM language technologies since\njoining the Watson group in 2014. He is an inventor with over 100 patents, and holds a PhD in\nCognitive Science.",
    "Cognitive Science.\nCarl Broker is an AI Architect at IBM who specializes in enterprise gen AI solutions. With a\nbackground in both gen AI and traditional data science, Carl leads design sessions and\ndevelops proof-of-concepts for clients. Before this role, he worked as an AI Engineer and",
    "Data Scientist at IBM, focusing on predictive modeling and AI-driven solutions. Carl holds a\nMaster of Science degree from Johns Hopkins University.\nDaniele Comi is a Data Scientist, AI Engineer, and Software Engineer at IBM Italy, with over",
    "3 years of experience in data analytics, ML, and deep learning (DL). His expertise spans the\nentire spectrum of AI, from architectural design to scientific research, with a focus on ML,\nreinforcement learning (RL), and DL. Daniele holds a master's degree in Computer Science",
    "Engineering, with a specialty in AI frameworks and models. At IBM, Daniele has been a key\nmember of the AI and gen AI team in Italy, where he has designed and implemented complex\nAI and gen AI architectures for many industry applications. His technical expertise also",
    "includes Fully Homomorphic Encrypted AI, which enables secure AI solutions that help\nensure data privacy.\nLakshmana Ekambaram is an IBM Senior Technical Leader with over 30 years of\nexperience in database development, advanced analytics, and building hybrid cloud",
    "solutions. He is part of the IBM Expert Labs SWAT organization where he leads the data\nfabric and trusted AI journey for customers worldwide. He has developed many IBM\ncertification courses and co-authored books about data science, AI, and data fabric.",
    "Shuvanker Ghosh is a certified Executive Architect and Worldwide Platform Leader for Data\nand AI in Worldwide Solution Architecture in IBM Technology Expert Labs. With 18 years of\nexperience at IBM, he serves as a trusted advisor to clients, offering thought leadership on",
    "IBM's Data and AI portfolio. He guides organizations in their responsible AI journey, helping\nthem adopt best practices. His current focus is on defining solution blueprints and\narchitectural patterns that assist clients in addressing their business challenges through",
    "responsible and trustworthy AI solutions. He possesses extensive expertise in the IBM Data\nand AI portfolio, including the watsonx platform and IBM Cloud Pak for Data. Shuvanker has\nsuccessfully led and delivered complex programs that involve multiple teams, providing",
    "technical management, architecture, technology thought leadership, and software\ndevelopment methodologies and processes. His experience spans various industries,\nincluding retail, finance, insurance, healthcare, telecommunications, and government.\nForeword xi",
    "Karen Medhat is a Customer Success Manager Architect in the UK and the youngest\nIBM Certified Thought Leader Level 3 Technical Specialist. She is the Chair of the\nIBM Technical Consultancy Group and an IBM Academy of technology member. She holds a",
    "MSc degree with honors in Engineering in AI and Wireless Sensor Networks from the Faculty\nof Engineering, Cairo University, and a BSc degree with honors in Engineering from the\nFaculty of Engineering, Cairo University. She co-creates curriculum and exams for different",
    "IBM professional certificates. She also created and co-created courses for the IBM Skills\nAcademy in various areas of IBM technologies. She serves on the review board of\ninternational conferences and journals in AI and wireless communication. She also is an",
    "IBM Inventor who is experienced in creating applications architecture and leading teams of\ndifferent scales to deliver customers' projects successfully. She frequently mentors IT\nprofessionals to help them define their career goals, learn new technical skills, or acquire",
    "professional certifications. She has authored publications on cloud, IoT, AI, wireless networks,\nmicroservices architecture, and blockchain.\nPayal Patel works in Data and AI Technical Content Development at IBM, where she creates",
    "technical learning materials for sellers, IBM Business Partners, and clients to enable them to\nget the most value out of IBM Data and AI products and solutions. She has worked in various\nroles at IBM, which include marketing analytics and as a Solutions Architect in IBM",
    "Technology Expert Labs, with a focus on Data and AI. She has worked in various technical\nroles across the financial services, insurance, and technology industries. She holds a\nBachelor of Science degree in Information Science from UNC Chapel Hill, and a Masters in",
    "Analytics degree from North Carolina State University.\nMatthew Price is a Senior watsonx Client Success Manager with 20 years of experience in IT\nand 10 years of experience focusing on Watson technologies. His previous experience",
    "includes writing the base code that went on to become the IBM Watson® Assistant for\nCitizens application, which is IBM’s no-charge offering that was released during 2020 to help\nbusiness and government agencies navigate the pandemic. His previous publications",
    "centered on application migration and the cloud.\nShirley Shum is a Senior Software Engineer for the IBM Fusion team. She has worked as a\ntechnical lead on IBM Storage products, such as IBM Storage Insights and Fusion. Her areas",
    "of expertise include Kafka, complex event processing, backup and restore, and AI solutions,\nsuch as watsonx.ai and InstructLab on the Red Hat OpenShift platform.\nMark Simmonds is a Program Director with IBM Data and AI. He writes extensively on AI,",
    "data science, and data fabrics, and holds multiple author recognition awards. He previously\nworked as an IT architect leading complex infrastructure design and corporate technical\narchitecture projects. He is a member of the British Computer Society, holds a bachelor's",
    "degree in Computer Science, is a published author, and a prolific public speaker.\nNow you can become a published author, too!\nHere’s an opportunity to spotlight your skills, grow your career, and become a published",
    "author—all at the same time! Join an IBM Redbooks residency project and help write a book\nin your area of expertise, while honing your experience using leading-edge technologies. Your\nefforts will help to increase product acceptance and customer satisfaction, as you expand",
    "your network of technical contacts and relationships. Residencies run from two to six weeks\nin length, and you can participate either in person or as a remote resident working from your\nhome base.\nFind out more about the residency program, browse the residency index, and apply online at:",
    "ibm.com/redbooks/residencies.html\nxii Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Comments welcome\nYour comments are important to us!\nWe want our books to be as helpful as possible. Send us your comments about this book or\nother IBM Redbooks publications in one of the following ways:\n(cid:2) Use the online Contact us review Redbooks form found at:\nibm.com/redbooks",
    "ibm.com/redbooks\n(cid:2) Send your comments in an email to:\nredbooks@us.ibm.com\n(cid:2) Mail your comments to:\nIBM Corporation, IBM Redbooks\nDept. HYTD Mail Station P099\n2455 South Road\nPoughkeepsie, NY 12601-5400\nStay connected to IBM Redbooks\n(cid:2) Find us on LinkedIn:",
    "(cid:2) Find us on LinkedIn:\nhttps://www.linkedin.com/groups/2130806\n(cid:2) Explore new Redbooks publications, residencies, and workshops with the IBM Redbooks\nweekly newsletter:\nhttps://www.redbooks.ibm.com/subscribe\n(cid:2) Stay current on recent Redbooks publications with RSS Feeds:",
    "https://www.redbooks.ibm.com/rss.html\nForeword xiii",
    "xiv Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "1\nCompeting with artificial\nChapter 1.\nintelligence\nIn today's fast-paced digital landscape, artificial intelligence (AI) has emerged as a\ngame-changer that is revolutionizing the way businesses operate, innovate, and compete. As",
    "AI technologies continue to advance and become increasingly ubiquitous, organizations are\nfaced with the daunting task of competing with AI-driven rivals while also leveraging AI to stay\nahead of the competition. This chapter delves into the world of competing with AI by exploring",
    "the challenges, opportunities, and strategies that organizations can employ to remain\ncompetitive in an AI-dominated market.\nThe following topics are described in this chapter:\n(cid:2) 1.1, “Competing with AI” on page2\n(cid:2) 1.2, “Challenges in building and deploying AI models” on page4",
    "(cid:2) 1.3, “Opportunities around using AI on trusted data” on page5\n(cid:2) 1.4, “Improving AI model reliability” on page8\n(cid:2) 1.5, “Creating new AI-enabled products and services” on page11\n© Copyright IBM Corp. 2025. 1",
    "1.1 Competing with AI\nTo harness the competitive advantage of using AI, organizations must first understand the AI\nlandscape and the various types of AI that exist. Organizations must also be aware of the\nvarious AI technologies that are being used to drive innovation and competitiveness, which",
    "include the following ones:\n(cid:2) Machine learning (ML): ML represents a pivotal subset of AI where algorithms are\ndeveloped and trained to recognize patterns and make data-driven predictions or\ndecisions. Unlike traditional programming, ML systems learn iteratively from data,",
    "improving performance with experience. These systems rely on vast datasets to develop\nstatistical models that enable predictions across diverse applications, such as anomaly\ndetection, natural language processing (NLP), and image recognition. The training",
    "process involves feeding labeled (supervised learning) or unlabeled (unsupervised\nlearning) data to the algorithm. Over time, the system refines its parameters to minimize\nerrors and maximize predictive accuracy. ML algorithms are central to AI's practical",
    "applications, and they drive everything from recommendation systems to fraud detection\nin modern business ecosystems.\n(cid:2) Deep learning (DL): DL is an advanced branch of ML that employs artificial neural\nnetworks that are modeled after the human brain's structure and functioning. Unlike",
    "traditional ML, which often depends on manual feature engineering, DL automates the\nextraction of complex features from raw data through multiple layers of interconnected\nneurons. DL excels in handling unstructured data such as images, audio, and text, making",
    "it instrumental in solving tasks like computer vision, speech recognition, and language\ntranslation. For example, convolutional neural networks (CNNs) specialize in image\nprocessing by identifying spatial hierarchies in pixels, and recurrent neural networks",
    "(RNNs) and transformers tackle sequential data with unparalleled efficiency. By leveraging\nhigh-performance computing and large datasets, DL approximates nonlinear functions to\nenable machines to solve intricate, high-dimensional problems.",
    "(cid:2) Unsupervised learning: Unsupervised learning focuses on deriving patterns and\nstructures from unlabeled datasets. This method trains algorithms to identify inherent\ngroupings, clusters, or associations in data without human-provided annotations. Common",
    "techniques include clustering algorithms, such as k-means and hierarchical clustering,\nand dimensionality reduction methods, such as Principal Component Analysis (PCA) and\nt-SNE. Applications of unsupervised learning are vast, ranging from customer",
    "segmentation in marketing to anomaly detection in cybersecurity. These systems are\nparticularly valuable in exploratory data analysis, where insights emerge from raw data\nwithout prior assumptions. By uncovering hidden relationships, unsupervised learning",
    "enhances your understanding of data and informs decision-making processes.\n(cid:2) Reinforcement learning (RL): RL is a paradigm of ML where algorithms learn optimal\nbehaviors by interacting with an environment and receiving feedback in the form of",
    "rewards or penalties. RL systems employ agents that act based on policies, and aim to\nmaximize cumulative rewards over time. Core to RL are concepts such as the\nexploration-exploitation tradeoff, Markov decision processes (MDPs), and value functions.",
    "Techniques such as Q-learning and Deep Q-Networks (DQNs) extend RL’s capabilities to\nenable applications in robotics, game playing, and autonomous vehicles. RL's emphasis\non learning through trial and error aligns it closely with real-world problem-solving, where",
    "dynamic environments require adaptive strategies.\n2 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "(cid:2) Foundation models: Foundation models (FMs) represent a transformative leap in AI and\nML, characterized by their scalability, versatility, and ability to generalize across tasks.\nThese models, such as Granite, are pre-trained on vast and diverse datasets, enabling",
    "them to adapt to specific applications with minimal fine-tuning. Unlike traditional ML\nmodels that are tailored to single tasks, FMs leverage their pre-trained knowledge to excel\nin multiple domains. This adaptability is achieved through transfer learning, where the",
    "model's pre-trained weights are fine-tuned on domain-specific data sets. FMs empower\norganizations to reduce the cost and complexity of training AI systems while achieving\nstate-of-the-art performance in tasks like natural language understanding, summarization,\nand multimodal reasoning.",
    "and multimodal reasoning.\nLeveraging AI for competitive advantage\nTo thrive in an AI-driven landscape, organizations must strategically harness AI technologies\nto drive innovation, enhance operational efficiency, and improve decision-making. Key areas\nof focus include the following ones:",
    "of focus include the following ones:\n(cid:2) AI-powered automation: Automation that is fueled by AI enables the running of repetitive\nand high-volume tasks with speed and precision, such as applications in Robotic Process",
    "Automation (RPA), intelligent document processing, and automated workflows. By\noffloading routine operations, organizations can redirect human resources to strategic and\ncreative endeavors, which foster innovation and competitive differentiation.",
    "(cid:2) AI-driven analytics: AI-powered analytics transform raw data into actionable insights,\nwhich equips businesses to make data-informed decisions. Predictive analytics, which is\npowered by ML, enables forecasting trends and identifying potential challenges, and",
    "prescriptive analytics suggests optimal courses of action. These capabilities enable\norganizations to stay ahead in dynamic markets by responding proactively to opportunities\nand risks.\n(cid:2) AI-based innovation: AI acts as a catalyst for innovation to enable organizations to",
    "conceptualize and deliver groundbreaking products, services, and business FMs. From\npersonalized healthcare solutions to autonomous logistics systems, AI's potential to\nredefine industries is immense. By embedding AI in their innovation processes,",
    "companies can create unique value propositions that resonate with customers and\nstakeholders alike.\nBy embracing these AI strategies, organizations can position themselves as leaders in the era\nof digital transformation. As AI continues to evolve, its synergy with trusted data will unlock",
    "unprecedented opportunities, which will reshape the competitive landscape and drive\nsustainable growth.\nChapter 1. Competing with artificial intelligence 3",
    "1.2 Challenges in building and deploying AI models\nBuilding and deploying AI models is a complex and challenging endeavor that requires\nexpertise, resources, and infrastructure. Despite the potential benefits of AI, many",
    "organizations struggle to overcome the numerous hurdles that are associated with AI model\ndevelopment and deployment. Here are some of the key challenges:\n(cid:2) Data quality and availability: AI models require vast amounts of high-quality, relevant, and",
    "diverse data to learn, train, and validate. However, many organizations face challenges in\ncollecting, processing, and integrating data from disparate sources, which can lead to\nissues with data quality, consistency, and availability. Furthermore, data privacy and",
    "security concerns can limit access to sensitive data, which can hinder the development of\naccurate and reliable AI models.\n(cid:2) Model complexity and interpretability: As AI models become increasingly complex, they",
    "can be difficult to interpret and understand, which makes it challenging to identify biases,\nerrors, or flaws in the decision-making process. The lack of transparency and\nexplainability in AI models can lead to mistrust, regulatory issues, and reputational",
    "damage. Moreover, the complexity of AI models can make it difficult to integrate them with\nexisting systems, processes, and infrastructure.\n(cid:2) Talent acquisition and retention: The development and deployment of AI models require",
    "specialized skills and expertise, which include data science, ML, and software\nengineering. However, the demand for AI talent far exceeds the supply, which can lead to\nchallenges in acquiring and retaining top talent. Furthermore, the constant evolution of AI",
    "technologies means that professionals must continually update their skills to remain\nrelevant, which can add to the talent acquisition and retention challenges.\n(cid:2) Infrastructure and scalability: AI models require significant computational resources,",
    "memory, and storage to process and analyze large datasets. However, many\norganizations lack the necessary infrastructure to support the development and\ndeployment of AI models, which can lead to issues with scalability, performance, and",
    "reliability. Furthermore, the integration of AI models with existing systems and processes\ncan be complex, requiring significant investment in infrastructure and architecture.\n(cid:2) Bias and fairness: AI models can perpetuate and amplify existing biases and inequalities if",
    "they are trained on biased data or designed with a particular world view. The lack of\ndiversity and inclusion in AI development teams can exacerbate these issues, which can\nlead to unfair outcomes and reputational damage. Furthermore, the identification and",
    "mitigation of bias in AI models can be challenging, which requires significant expertise and\nresources.\n(cid:2) Regulatory compliance: The development and deployment of AI models are subject to\nvarious regulations and laws, which include data protection, intellectual property, and",
    "anti-discrimination legislation. However, the rapidly evolving nature of AI technologies can\nmake it challenging to ensure regulatory compliance, particularly in industries with strict\nregulations, such as healthcare and finance.",
    "regulations, such as healthcare and finance.\n(cid:2) Model maintenance and updates: AI models require continuous maintenance and updates\nto help ensure that they remain accurate, reliable, and relevant. However, the process of",
    "updating AI models can be complex and require significant resources and expertise.\nFurthermore, the integration of updated AI models with existing systems and processes\ncan be challenging, which can lead to issues with compatibility and performance.",
    "4 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "(cid:2) Explainability and transparency: The lack of explainability and transparency in AI models\ncan make it challenging to understand the decision-making process, which can lead to\nmistrust and reputational damage. Furthermore, the identification and mitigation of errors",
    "or biases in AI models can be difficult, which can require expertise and resources.\n(cid:2) Cybersecurity: AI models can be vulnerable to cyberthreats, such as data poisoning,\nmodel hijacking, and adversarial attacks. The identification and mitigation of these threats",
    "can be challenging and require expertise and resources. Furthermore, the integration of AI\nmodels with existing security systems and processes can be complex, which can lead to\nissues with compatibility and performance.\n1.2.1 Technical considerations for building and deploying AI models",
    "When building and deploying AI models, there are several technical considerations that must\nbe accounted for:\n(cid:2) Data preprocessing: Ensuring that the data that is used to train and test the model is\naccurate, complete, and relevant.",
    "accurate, complete, and relevant.\n(cid:2) Model selection: Choosing the most suitable algorithm and model architecture for the\nproblem being addressed.\n(cid:2) Hyper-parameter tuning: Optimizing the model's hyper-parameters to achieve the best\npossible performance.",
    "possible performance.\n(cid:2) Model evaluation: Evaluating the model's performance by using metrics such as accuracy,\nprecision, and recall.\n(cid:2) Model deployment: Deploying the model in a production-ready environment, such as a\ncloud-based API or a containerized application.",
    "cloud-based API or a containerized application.\n(cid:2) Model monitoring: Continuously monitoring the model's performance and updating it as\nnecessary to help ensure that it remains accurate and relevant.\nIn addition to these technical considerations, organizations must also consider the following",
    "items:\n(cid:2) Data governance: Establishing policies and procedures for data management, which\ninclude data quality, security, and compliance.\n(cid:2) Model governance: Establishing policies and procedures for model development,",
    "deployment, and maintenance, which include model validation, testing, and updating.\n(cid:2) Infrastructure governance: Establishing policies and procedures for infrastructure\nmanagement, which include infrastructure provisioning, scaling, and security.",
    "By being conscious of these technical considerations and establishing effective governance\npolicies and procedures, organizations can help ensure the successful development and\ndeployment of AI models that drive business value and competitiveness.\n1.3 Opportunities around using AI on trusted data",
    "1.3 Opportunities around using AI on trusted data\nAI thrives on data. However, the effectiveness of AI systems is not solely dependent on the\nvolume of data but also on its quality and trustworthiness. Trusted data (data that is accurate,",
    "consistent, secure, and compliant) serves as the foundation for reliable AI-driven insights.\nOrganizations today are exploring opportunities to harness AI on trusted data to drive\noperational efficiency, enhance decision-making, and unlock new revenue streams. This",
    "section explores the myriad possibilities that AI unlocks when it operates on a foundation of\nhigh-quality, trusted data.\nChapter 1. Competing with artificial intelligence 5",
    "1.3.1 Enhancing decision-making with accurate insights\nThe fusion of AI and trusted data is reshaping decision-making processes across industries,\nand enabling organizations to derive precise and actionable insights. This transformation is",
    "critical in domains where decisions significantly impact outcomes, such as healthcare,\nfinance, and supply chain management. By leveraging high-quality, trusted data, AI systems\ncan identify patterns, predict outcomes, and provide recommendations that drive superior\ndecisions.",
    "decisions.\nFor example, in healthcare, AI systems that are powered by trusted clinical and patient data\nenhance diagnostic precision, predict patient outcomes with remarkable accuracy, and\nsuggest personalized treatment plans that are tailored to individual needs. In the financial",
    "sector, AI models that are trained on trusted datasets excel in detecting fraudulent activities,\nassessing credit risks, and automating sophisticated trading strategies that are based on\ndynamic market trends. These examples illustrate how trusted data amplifies the reliability",
    "and impact of AI-driven decision-making, minimizing risks and maximizing outcomes.\nOne of the most transformative applications of AI on trusted data is in improving\ndecision-making. High-quality data enables AI algorithms to deliver precise and actionable",
    "insights, which are beneficial in industries like healthcare, finance, and supply chain\nmanagement, where even minor errors in decision-making can have significant\nconsequences.\n(cid:2) Healthcare: Trusted data enables AI systems to accurately predict patient outcomes,",
    "suggest personalized treatment plans, and enhance diagnostic accuracy.\n(cid:2) Finance: In financial services, AI models that are trained on trusted data can detect fraud,\nassess credit risks, and automate trading strategies based on market predictions.\n1.3.2 Driving operational efficiency",
    "1.3.2 Driving operational efficiency\nAI's ability to automate and optimize complex processes is magnified when it is built on a\nfoundation of trusted data. By eliminating inefficiencies and reducing the need for human",
    "intervention, organizations can achieve unprecedented levels of operational efficiency.\nIn the manufacturing and energy sectors, predictive maintenance systems leverage trusted\nsensor data to foresee equipment failures, which enable preemptive interventions that",
    "minimize downtime and reduce maintenance costs. Similarly, AI-powered customer service\nplatforms, which are underpinned by reliable customer interaction data, provide accurate,\ncontext-aware responses that deliver personalized experiences while alleviating the workload",
    "of human agents. These advancements highlight the transformative potential of combining AI\nwith trusted data to streamline operations across industries.\nWhen AI is applied to trusted data, it automates complex processes, which reduce the need\nfor human intervention, and improves efficiency:",
    "for human intervention, and improves efficiency:\n(cid:2) Predictive maintenance: In the manufacturing and energy sectors, AI systems use trusted\nsensor data to predict equipment failures, which minimize downtime and optimizes\nmaintenance schedules.",
    "maintenance schedules.\n(cid:2) Customer service automation: AI-powered chatbots, which are fueled by reliable customer\ndata, provide accurate responses and deliver personalized experiences, which reduce the\nburden of human agents.",
    "burden of human agents.\n6 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "1.3.3 Accelerating innovation\nThe convergence of AI and trusted data is a catalyst for innovation, which unlocks hidden\npatterns and opportunities that were previously inaccessible. By analyzing vast amounts of\nhigh-quality data, AI systems empower organizations to develop groundbreaking products",
    "and solutions.\nFor example, in product development, companies use AI to analyze customer feedback,\nmarket trends, and usage data that is extracted from trusted sources to create offerings that\nresonate with consumer preferences. In the realm of scientific research, AI accelerates",
    "discovery processes by interpreting extensive experimental datasets, which lead to\nadvancements in fields such as drug development and material science. Trusted data\nenhances the accuracy of these insights and helps ensure the reproducibility of outcomes,\nwhich drive sustained innovation.",
    "which drive sustained innovation.\nThe combination of AI and trusted data fosters innovation by uncovering hidden patterns and\nopportunities that were previously inaccessible:\n(cid:2) Product development: Companies leverage AI to analyze customer feedback and market",
    "trends from trusted datasets, which helps the companies to design products that align with\nconsumer preferences.\n(cid:2) Research and development: In scientific research, AI accelerates discovery by analyzing\nlarge volumes of trusted experimental data, which can lead to breakthroughs in areas",
    "such as drug discovery and material science.\n1.3.4 Enhancing governance and compliance\nIn an era where regulatory landscapes are becoming increasingly stringent, trusted data\nplays a pivotal role in helping ensure that AI systems operate within legal and ethical",
    "frameworks. Governance and compliance initiatives are fortified by AI systems that\ncontinuously monitor operations, detect anomalies, and flag potential risks.\nFor example, compliance monitoring AI tools analyze operational data to help ensure",
    "adherence to industry regulations and standards, which reduce the risk of noncompliance\npenalties. Also, the usage of diverse and representative trusted datasets mitigates biases in\nAI model training, which foster fairness and ethical outcomes in critical applications such as",
    "hiring or loan approvals. Trusted data serves as a cornerstone for responsible AI development\nand deployment.\nTrusted data helps ensure that AI systems operate within legal and ethical boundaries, which\ncritical factors in maintaining customer trust and avoiding regulatory penalties:",
    "(cid:2) Compliance monitoring: AI models can continuously analyze operational data to help\nensure adherence to regulations by flagging any potential compliance risks.\n(cid:2) Bias mitigation: Trusted data, when diverse and representative, helps train AI models that",
    "are fair and unbiased, which helps ensure ethical outcomes in areas like hiring or loan\napprovals.\n1.3.5 Unlocking new revenue streams\nThe monetization of trusted data through AI-driven services and products has emerged as a",
    "significant avenue for revenue generation. Organizations across sectors are capitalizing on\nthis synergy to create innovative business models.\nChapter 1. Competing with artificial intelligence 7",
    "For example, in telecommunications and retail, companies offer AI-powered insights or\nanalytics as services to their partners and clients, which transform data into a valuable asset.\nMoreover, trusted customer data enables hyper-targeted marketing campaigns, which",
    "enhance conversion rates and foster customer loyalty. By harnessing the power of trusted\ndata, organizations can unlock untapped revenue opportunities while delivering value to\nstakeholders.\nOrganizations are monetizing their trusted data through AI-driven services and products:",
    "(cid:2) Data monetization: Companies in sectors such as telecommunications and retail generate\nnew revenue by offering AI-powered insights or analytics as a service to their partners and\nclients.\n(cid:2) Personalized marketing: AI leverages trusted customer data to deliver hyper-targeted",
    "marketing campaigns that increase conversion rates and customer loyalty.\n1.3.6 Transforming industries with AI and trusted data\nThe integration of AI with trusted data is revolutionizing industries in unique and profound",
    "ways. Retailers use transaction and customer data to fuel recommendation engines, which\nenhance sales and customer satisfaction. In agriculture, AI models analyze environmental\nand crop data to optimize farming practices and maximize yields. Similarly, in the energy",
    "sector, AI systems leverage consumption and grid data to predict demand, optimize\ndistribution, and enhance energy efficiency. These transformative applications underscore the\nversatility and impact of trusted data-driven AI across diverse domains.",
    "Different industries are leveraging AI and trusted data in unique ways:\n(cid:2) Retail: Trusted transaction and customer data power recommendation engines that boost\nsales and improve customer experiences.",
    "sales and improve customer experiences.\n(cid:2) Agriculture: AI models analyze trusted environmental and crop data to optimize farming\npractices and increase yield.\n(cid:2) Energy: AI systems use trusted consumption and grid data to predict demand and\noptimize energy distribution.",
    "optimize energy distribution.\n1.4 Improving AI model reliability\nThe reliability and interpretability of AI models are enhanced when they are trained and\nvalidated on trusted data. High-quality data helps ensure that AI systems deliver consistent",
    "and accurate outputs, which foster stakeholder trust and facilitate broader adoption.\nFor example, explainable AI (XAI) models rely on trusted data to generate transparent and\ninterpretable insights, which address concerns about the “black-box” nature of AI. Also,",
    "trusted data simplifies model auditing and debugging by enabling the identification of\ninconsistencies and anomalies, which leads to continuous performance improvements. By\nprioritizing data quality, organizations can overcome one of the primary challenges of scaling\nAI systems.",
    "AI systems.\n8 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Trusted data enhances the reliability and interpretability of AI models, addressing one of the\nmajor challenges in deploying AI at scale:\n(cid:2) Explainability and trust: AI models that are trained on high-quality data provide more",
    "consistent and interpretable outputs, which enable stakeholders to trust and adopt\nAI-driven solutions.\n(cid:2) Model auditing and debugging: Trusted data helps identify inconsistencies and anomalies\nin AI predictions, which make it easier to debug models and improve their performance\nover time.",
    "over time.\n1.4.1 Enabling cross-enterprise collaboration\nTrusted data serves as a bridge for collaboration across organizational silos and with external\npartners. This capability enhances operational transparency and fosters innovation by\nenabling seamless data sharing and integration.",
    "enabling seamless data sharing and integration.\nEnterprises can leverage trusted data infrastructures to break down silos so that\ncross-functional teams can work collaboratively on shared objectives. Secure data exchange",
    "mechanisms further facilitate partnerships across geographies, which helps ensure\ncompliance with data privacy regulations and fosters rust among stakeholders. Such\ncollaborative ecosystems are critical for driving comprehensive digital transformation\ninitiatives.",
    "initiatives.\nAI on trusted data enables organizations to collaborate more effectively across departments\nand even with external partners:\n(cid:2) Data sharing across silos: Enterprises can break down data silos and enable\ncross-functional collaboration to enhance operational transparency.",
    "(cid:2) Secure data exchange: Trusted data infrastructures help ensure that shared data between\npartners or across geographies remain secure and compliant.\n1.4.2 Enhancing real-time decision-making\nThe ability to make real-time decisions is a cornerstone of modern business strategies, and",
    "trusted data is a key enabler of this capability. By processing and analyzing data streams in\nreal time, AI systems empower organizations to act swiftly and effectively.\nIn industries like finance, dynamic pricing models use real-time trusted data to optimize",
    "stock-pricing strategies based on demand and inventory levels. Financial institutions employ\nAI systems to perform instant risk assessments to mitigate fraud and help ensure secure\ntransactions. These applications demonstrate how trusted data enhances the agility and",
    "responsiveness of AI-driven decision-making processes.\nReal-time analytics that are powered by trusted data enable businesses to make faster, more\ninformed decisions:\n(cid:2) Dynamic pricing: In e-commerce or travel industries, AI leverages real-time trusted data to",
    "optimize pricing strategies based on demand and inventory levels.\n(cid:2) Real-time risk assessment: Financial institutions use AI to perform instant risk\nassessments for transactions to help mitigate fraud or credit risks.\nChapter 1. Competing with artificial intelligence 9",
    "1.4.3 Scaling AI-driven ecosystems\nLarge-scale AI implementations depend on the robustness and reliability of trusted data\necosystems. By building scalable infrastructures that integrate trusted data with advanced AI\ncapabilities, organizations can unlock the full potential of AI-driven solutions.",
    "AI as a Service (AIaaS) platforms exemplify this integration by offering modular services such\nas ML models, NLP, and predictive analytics that are powered by trusted data. In parallel, the\nintegration of IoT devices with AI systems generates vast volumes of real-time data to enable",
    "actionable insights in sectors like logistics, healthcare, and smart cities. These scalable\necosystems are the foundation for sustained growth and innovation.\nTrusted data serves as the backbone for large-scale AI implementations, fostering robust AI\necosystems:",
    "ecosystems:\n(cid:2) AI as a Service (AIaaS): Companies are building scalable platforms where trusted data\npowers modular AI services like ML models, NLP, and predictive analytics.\n(cid:2) Integration with IoT: IoT devices generate vast amounts of data. Trusted IoT data enables",
    "AI systems to deliver real-time analytics for industries like logistics, healthcare, and smart\ncities.\n1.4.4 Driving sustainability and environmental, social, and governance goals\nAI-powered sustainability initiatives are gaining momentum, with trusted data playing a",
    "central role in achieving environmental, social, and governance (ESG) objectives. By\nanalyzing environmental and operational datasets, AI systems help organizations optimize\nresource usage, reduce carbon footprints, and enhance supply chain transparency.",
    "For example, sustainability analytics tools use trusted data to identify inefficiencies and\nrecommend strategies for improving energy efficiency. Similarly, AI systems provide visibility\ninto supply chains to enable organizations to address waste and improve sustainability",
    "practices. By aligning AI initiatives with ESG goals, organizations can demonstrate their\ncommitment to responsible and ethical operations.\nAI, combined with trusted data, helps organizations meet ESG objectives:",
    "(cid:2) Sustainability analytics: AI models analyze trusted environmental and operational data to\noptimize resource usage and reduce carbon footprints.\n(cid:2) Supply chain transparency: Trusted data provides visibility into the supply chain so that AI",
    "can identify inefficiencies, reduce waste, and improve sustainability practices.\n1.4.5 Personalizing customer experiences\nCustomer-centric industries are leveraging AI's ability to deliver highly personalized\nexperiences, which is a capability that is rooted in trusted data. By analyzing customer",
    "behavior, preferences, and interactions, AI systems create tailored experiences that enhance\nsatisfaction and loyalty.\nFor example, adaptive AI systems use real-time trusted data to modify services dynamically\nto help ensure relevance and engagement. Behavioral analytics enable companies to predict",
    "customer needs, which reduce churn and fosters long-term relationships. Trusted data\nempowers organizations to elevate customer experiences to new heights.\n10 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Customer-centric industries are capitalizing on AI's ability to deliver highly personalized\nexperiences through trusted data:\n(cid:2) Adaptive AI systems: Real-time, trusted customer data enables AI systems to adapt and\ntailor services to create more engaging user experiences.",
    "(cid:2) Behavioral analytics: Trusted behavioral data enables companies to predict customer\npreferences, which reduce churn and increase satisfaction.\n1.5 Creating new AI-enabled products and services\nTrusted data serves as the foundation for developing innovative AI-enabled products and",
    "services that redefine industries. By harnessing historical and real-time data, organizations\ncan anticipate needs and deliver solutions proactively.\nFor example, proactive support systems leverage AI to predict and address issues before",
    "they escalate, which enhances customer satisfaction and operational efficiency. Custom AI\nsolutions, which are tailored to specific market niches, further demonstrate the transformative\npotential of trusted data. As organizations continue to invest in data governance and quality,",
    "the opportunities for creating AI-driven innovations will expand.\nBy aligning AI initiatives with business objectives and prioritizing trusted data infrastructures,\norganizations can unlock unparalleled levels of efficiency, innovation, and growth. As the",
    "complexity of data landscapes continues to evolve, the role of trusted data in enabling AI to\nachieve its full potential becomes increasingly indispensable.\nTrusted data opens doors to innovations that can redefine industries:",
    "(cid:2) Proactive support systems: AI systems, which are trained on historical and real-time data,\npredict customer or machine needs before problems occur to offer preemptive solutions.\n(cid:2) Custom AI solutions: Organizations use their proprietary trusted data to build AI products",
    "that are tailored to niche market requirements.\nBy continuously investing in trusted data infrastructures and aligning AI initiatives with\nbusiness goals, organizations can unlock new levels of efficiency, growth, and innovation.AI",
    "and trusted data offers a powerful opportunity to drive growth, innovation, and operational\nexcellence. Organizations that prioritize data governance, ensure data quality, and build AI\nsystems on trusted data are better positioned to harness these opportunities. As the volume",
    "and complexity of data continue to grow, the role of trusted data in enabling AI to deliver its full\npotential will become more critical.\nChapter 1. Competing with artificial intelligence 11",
    "12 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "2\nIntroducing IBM watsonx.ai\nChapter 2.\nIBM watsonx is IBM’s next-generation platform that is designed to help businesses accelerate\ntheir journey into artificial intelligence (AI)-driven insights, decision-making, and automation.",
    "The platform offers a comprehensive suite of tools and services that are tailored to simplify\nand streamline the development and deployment of AI solutions. It is built on three\nfoundational pillars:\n(cid:2) IBM watsonx.data: A scalable data lakehouse that is designed for efficient and secure",
    "data management to enable hybrid cloud deployments and optimize data for AI workloads.\n(cid:2) IBM watsonx.governance: Provides robust governance to help ensure that AI models\nremain ethical, transparent, and compliant with regulatory standards. It also helps",
    "businesses monitor and mitigate AI-related risks.\n(cid:2) IBM watsonx.ai: A cutting-edge AI development and deployment environment. It supports\nthe full lifecycle of AI, from model training and fine-tuning to deployment and monitoring.",
    "The seamless integration of these components enables enterprises to leverage trusted data\n(watsonx.data), enforce governance and ethical standards (watsonx.governance), and\ndevelop AI-powered solutions (watsonx.ai). Together, they form a comprehensive ecosystem",
    "for deploying enterprise-grade AI at scale.\nThe following topics are described in this chapter:\n(cid:2) 2.1, “Overview of watsonx.ai” on page14\n(cid:2) 2.2, “Synergy between watsonx.ai and other components in the watsonx platform” on\npage15",
    "page15\n(cid:2) 2.3, “Business impact of these synergies” on page16\n© Copyright IBM Corp. 2025. 13",
    "2.1 Overview of watsonx.ai\nwatsonx.ai serves as the core engine of the watsonx platform, which is focused on the rapid\ndevelopment and deployment of AI models. Its architecture is designed to support various AI",
    "workloads, such as traditional machine learning (ML), deep learning (DL), and generative AI\n(gen AI).\n2.1.1 Key capabilities\nHere are the key capabilities of watsonx.ai:\n(cid:2) Foundation models (FMs): watsonx.ai provides access to pre-trained FMs, such as large",
    "language models (LLMs) and vision models, which can be fine-tuned for specific business\nneeds.\n(cid:2) Machine learning operations (MLOps): The platform integrates tools for version control,\nmodel monitoring, and deployment to streamline AI lifecycle management.",
    "(cid:2) Multi-cloud compatibility: Supports hybrid and multi-cloud environments so that\nbusinesses can run AI workloads wherever they see fit.\n(cid:2) Extensibility: Developers can bring their own models or integrate open-source frameworks\nlike PyTorch and TensorFlow.",
    "like PyTorch and TensorFlow.\n2.1.2 The watsonx.ai architecture\nThe watsonx.ai architecture components include the following items:\n(cid:2) Model Studio: An interface for training, fine-tuning, and deploying models.",
    "(cid:2) Inference Engine: Optimized for running AI models in production environments to help\nensure low latency and high scalability.\n(cid:2) Integration Layer: Enables seamless integration with watsonx.data for real-time data\naccess and watsonx.governance for compliance.",
    "access and watsonx.governance for compliance.\n2.1.3 watsonx.ai empowering IBM Software offerings\nThe watsonx.ai FMs are being infused throughout all of IBM's major software offerings. The\nFMs are as follows:",
    "FMs are as follows:\n(cid:2) IBM watsonx Code Assistant: Uses gen AI so that developers can automatically generate\ncode by using a natural-language prompt.\n(cid:2) IBM watsonx AIOps Insights: Includes FMs for code and natural language processing",
    "(NLP) to provide greater visibility into performance across IT environments.\n(cid:2) IBM watsonx Assistant and IBM watsonx Orchestrate®: Boosted by an NLP FM, IBM's\ndigital labor products enhance employee productivity and customer service experiences.",
    "(cid:2) Environmental Intelligence Suite: powered by the IBM geospatial FM,\nIBM EIS Builder Edition creates tailored solutions that address and mitigate environmental\nrisks.\n14 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "2.1.4 Benefits of using watsonx.ai for businesses\nAdopting watsonx.ai offers several key advantages for enterprises looking to harness the\npower of AI:\n(cid:2) Accelerated AI development: watsonx.ai simplifies the development process with",
    "pre-trained FMs and built-in tools for training and deployment. Businesses can achieve\nfaster time-to-value by reducing the complexity of building AI from scratch.\n(cid:2) Enhanced productivity and efficiency: Through automation of repetitive tasks, watsonx.ai",
    "enables teams to focus on higher-value activities. gen AI capabilities can handle complex\nprocesses, which improve the mean time to resolution for IT incidents and streamlining\ncustomer service.\n(cid:2) Scalable and cost-efficient: watsonx.ai support for hybrid cloud and open architecture",
    "provides cost flexibility. Businesses can choose the most economical deployment\nenvironment and scale AI workloads as needed.\n(cid:2) Trust and governance: With watsonx.governance tightly integrated, watsonx.ai helps\nensure that AI models operate transparently and ethically. Businesses can meet",
    "regulatory compliance standards, which mitigate risks that are associated with biased or\nunexplainable AI decisions.\n(cid:2) Business innovation: watsonx.ai enables companies to explore new AI-driven\nopportunities, such as personalizing customer experiences, optimizing supply chains, and",
    "driving data-driven decision-making.\nwatsonx.ai is a transformative tool that empowers businesses to unlock the full potential of AI.\nBy integrating seamlessly with watsonx.data and watsonx.governance, it offers a unified",
    "platform that combines innovation, efficiency, and compliance. Organizations adopting\nwatsonx.ai can expect to gain a competitive edge through smarter automation, better\ndecision-making, and faster scaling of AI solutions.\n2.2 Synergy between watsonx.ai and other components in the\nwatsonx platform",
    "watsonx platform\nThis section covers the following topics:\n(cid:2) Synergy between watsonx.ai and watsonx.data\n(cid:2) Synergy between watsonx.ai and watsonx.governance\n2.2.1 Synergy between watsonx.ai and watsonx.data",
    "2.2.1 Synergy between watsonx.ai and watsonx.data\nwatsonx.ai and watsonx.data streamline the development and deployment of AI models by\nensuring that AI systems are powered by high-quality, trusted data. Here is how their synergy\ncreates value:",
    "creates value:\n(cid:2) Efficient AI model development: watsonx.data provides a robust and scalable data\nlakehouse that is optimized for AI workloads. This lakehouse helps ensure that watsonx.ai\nhas instant access to vast amounts of clean, organized, and queryable data, which",
    "accelerates training and fine-tuning of AI models.\n(cid:2) Real-time data for AI: watsonx.data facilitates real-time data streaming, which enables\nwatsonx.ai to build and run AI models on up-to-date information. This approach enables",
    "dynamic AI use cases, such as predictive maintenance and fraud detection.\nChapter 2. Introducing IBM watsonx.ai 15",
    "(cid:2) Hybrid and multi-cloud flexibility: Both watsonx.ai and watsonx.data support deployment\nacross hybrid and multi-cloud environments, which help ensure scalability and cost\nefficiency while keeping data sovereignty intact.",
    "efficiency while keeping data sovereignty intact.\n(cid:2) Unified data governance: With watsonx.data acting as the backbone, organizations can\nensure data integrity, enhance data sharing, and maintain a single source of truth for AI\nmodels that are developed in watsonx.ai.",
    "models that are developed in watsonx.ai.\n2.2.2 Synergy between watsonx.ai and watsonx.governance\nThe relationship between watsonx.ai and watsonx.governance helps ensure that AI models\nare high-performing, compliant, ethical, and transparent. Here is how they complement each\nother:",
    "other:\n(cid:2) Ethical AI deployment: watsonx.governance provides guardrails for AI models that are\ndeveloped and deployed through watsonx.ai. These guardrails include bias detection,\nexplainability, and compliance tracking to help ensure that AI decisions are fair and aligned",
    "with regulatory standards.\n(cid:2) Lifecycle management and monitoring: watsonx.governance tracks the entire lifecycle of\nAI models by monitoring performance, drift, and adherence to governance policies. This\napproach enables watsonx.ai users to maintain the integrity of deployed models over time.",
    "(cid:2) Transparency and auditability: Models that are built on watsonx.ai benefit from the\nwatsonx.governance robust reporting and audit capabilities, which provide stakeholders\nwith clear insights into how AI models make decisions, which help ensure trustworthiness.",
    "(cid:2) Risk mitigation and remediation: If there are anomalies or breaches in governance\npolicies, watsonx.governance enables quick remediation. This capability is crucial for\nmission-critical applications where trust in AI outputs is paramount.\n2.3 Business impact of these synergies",
    "2.3 Business impact of these synergies\nBy leveraging the combined strengths of watsonx.ai, watsonx.data, and watsonx.governance,\nenterprises can achieve the following goals:\n(cid:2) Help ensure that their AI models are trained on trusted, compliant data.",
    "(cid:2) Maintain high performance and ethical standards across AI deployments.\n(cid:2) Accelerate innovation while minimizing the risks that are associated with AI adoption.\nThis holistic approach empowers organizations to maximize ROI on their AI investments and",
    "gain a competitive edge in their industries.\n16 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "3\nTools for diverse data science\nChapter 3.\nteams\nData science teams today are diverse in terms of skill sets, backgrounds, and experiences.\nThese teams are also diverse in terms of the types of solutions that are implemented.",
    "Common roles of data science teams are data scientists, machine learning (ML) engineers,\nand artificial intelligence (AI) engineers. Therefore, different types of tools and solutions are\nneeded to support data science teams.",
    "needed to support data science teams.\nThis chapter describes a few of the key personas for IBM watsonx.ai and how the different\ntypes of tools that are available on watsonx.ai support these individuals in their day-to-day\nwork.\nThe following topics are described in this chapter:",
    "(cid:2) 3.1, “Key personas for watsonx.ai” on page18\n(cid:2) 3.2, “Low-code, no-code, and full-code tools” on page20\n© Copyright IBM Corp. 2025. 17",
    "3.1 Key personas for watsonx.ai\nMany organizations are building data science teams. Depending on the level of data science\nmaturity within the organization, the types of roles and the experience of individuals in these",
    "roles can vary. Common roles in data science teams include data analysts, data scientists,\nML engineers, and AI engineers. Other roles in organizations that can benefit from\nIBM watsonx.ai include, but are not limited to, data science leaders, directors of enterprise",
    "architecture, and line-of-business users. This section goes through a few of these key\npersonas by providing an overview of each role’s responsibilities, common challenges that\nindividuals in this role often face, and how IBM watsonx.ai is designed to enable individuals in\nthese roles.",
    "these roles.\n3.1.1 Data scientists\nData scientists use data to solve problems and improve decision making within an\norganization. Depending on the team and organization, their responsibilities include data\ncollection, model development, data analysis, communication of findings, and providing",
    "recommendations. Data scientists often have a background in mathematics, specifically\nstatistics and linear algebra, and programming.\nIndividuals in these roles often face challenges that are related to a lack of self-service",
    "access to the correct, or accurate data, for cleaning, transforming, and generating insights.\nThey also lack integrated tools across the model lifecycle, and depending on their level of\nexperience and skill set, they might lack experience with creating models by using certain",
    "tools or programming languages.\nIBM watsonx.ai addressees this situation with various tools within the platform to provide data\nscientists with the flexibility that they need to build models. These tools include no-code,",
    "low-code, and full-code solutions such as AutoAI, IBM SPSS® Modeler, and Jupyter\nNotebooks. watsonx.ai provides many tools to select from because the correct tool to use\nvaries based on factors such as the individuals’ level of expertise and the project\nrequirements.",
    "requirements.\nData scientists often collaborate with others in their team, such as business stakeholders,\ndata science leaders, or fellow data scientists. watsonx.ai enables storing and sharing of\nassets among users within an organization through projects. In watsonx.ai, projects are",
    "collaborative workspaces where individuals can work with and share data and other assets to\naccomplish a specific goal.\n3.1.2 Machine learning engineers\nML engineers work with data scientists and other IT experts, such as software developers to",
    "automate and move ML models into production. Typically, ML engineers have a background in\ncomputer science, mathematics, statistics, or software engineering. ML engineers are\nresponsible for the data science pipeline, which can include sourcing and preparing data,",
    "building and training models, deploying models to production, and maintaining and improving\nexisting ML systems.\nCommon challenges that are faced by ML engineers include difficulty in defining short and\nlong-term goals, difficulty in scaling ML models, general lack of support for the services that",
    "are used by various teams, and incompatibility between tools.\n18 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "IBM watsonx.ai helps overcome these challenges by providing various no-code, low-code,\nand full-code tools, which are compatible with each one. Also, with IBM Watson Machine\nLearning on IBM watsonx.ai, the model deployment process is simplified through capabilities",
    "such as deployment spaces. Regardless of the tool on watsonx.ai that is used to develop the\nmodel, ML engineers can deploy the model, which makes it simpler to deploy and manage\nML assets.\n3.1.3 AI engineers\nAI engineers are responsible for developing, maintaining, and implementing AI systems.",
    "Common tasks that are performed by AI engineers include building and maintaining AI\nsystems, and tuning AI models. AI engineers generally have a background in computer\nscience, mathematics, software engineering, or programming.",
    "Because the AI engineer role is new to many organizations, a common challenge many\nteams and individuals in this role face involves having varying levels of technical skills and\nexperience to implement AI solutions. Also, other challenges include selecting and fine-tuning",
    "models for a specific use case, and managing the cost to implement and maintain an AI\nsolution.\nIBM watsonx.ai helps AI engineers overcome these challenges in a few ways, starting with\nthe foundation model (FM) library that is provided by IBM. This library includes a diverse",
    "selection of AI models, such as the following ones:\n(cid:2) IBM trained models (the Granite and Slate model series)\n(cid:2) IBM selected open-source models through Hugging Face\n(cid:2) Third-party models such as llama and mixtral",
    "Teams can choose among many different FMs for their use case, and choose a model that is\nbest suited for their use case. Teams are not locked into one specific vendor or model series.\nAlso, for more advanced AI engineers, they can upload and deploy their own FMs.",
    "With watsonx.ai, there are different ways for AI engineers to work with models. The Prompt\nLab tool in watsonx.ai enables AI engineers to write effective prompts (by using a GUI) to\ndeploy to FMs for inferencing. For individuals who have more programming experience and",
    "technical expertise, there is also the programmatic alternative to the Prompt Lab, where users\ncan prompt FMs by using the Python library or REST API.\nYou can use the tuning studio in watsonx.ai to tune a smaller FM to improve its performance.",
    "AI engineers can tune a smaller FM to achieve comparable results to larger models in the\nsame model family, which can lead to reduced inference costs in the long term.\nChapter 3. Tools for diverse data science teams 19",
    "3.2 Low-code, no-code, and full-code tools\nBecause the demand for data scientists has grown over the years, finding experienced data\nscientists is difficult for many organizations. For organizations that are newer to data science,",
    "they find that experienced data scientists often request higher salaries or tend to seek\nopportunities with interesting and advanced problems to solve.\nTherefore, it is important for organizations to find ML and AI platforms that support individuals",
    "with varying skill sets. There are many different types of tools for implementing data science,\nmachine learning operations (MLOps), and generative AI (gen AI) solutions. At a high level,\nthey can fall into one of three categories: no-code, low-code, or full-code.",
    "(cid:2) With no-code tools, users can create solutions and applications without writing code. The\ntool provides a GUI and includes “drag-and-drop” features so that users can build models\nwith little to no programming knowledge.",
    "with little to no programming knowledge.\n(cid:2) Low-code tools provide a visual approach to development so that users can generate\nsolutions, such as models, with minimal hand-coding. Like no-code tools, these tools",
    "typically provide a GUI and include “drag-and-drop” features. Low-code tools enable users\nwith minimal coding experience, such as citizen data scientists or business analysts to\nquickly build and implement a solution.",
    "quickly build and implement a solution.\n(cid:2) With full-code tools, users can write their own code to develop solutions. These tools are\ntypically leveraged by experienced data scientists. Full-code tools enable greater flexibility",
    "and more customization, but require deeper programming knowledge and expertise.\n3.2.1 No-code, low-code, and full-code tools on IBM watsonx.ai\nWhen to use a no-code, low-code, or full-code tool varies by project requirements, team skill",
    "set, and the time and cost that a specific solution has for an organization. IBM watsonx.ai\nprovides data scientists, ML engineers, and AI engineers with the flexibility of choose the right\ntool for a use case by providing various no-code, low-code, and full-code tools as part of the",
    "overall platform.\nNo-code solutions on IBM watsonx.ai\nThis section highlights a few of the no-code tools that are available on the IBM watsonx.ai\nplatform.\nAutoAI\nAutoAI is a no-code tool that data scientists can use to develop and prototype models quickly,",
    "without requiring the user to code or have programming knowledge. AutoAI helps data\nscientists and data science teams compare the results of multiple models efficiently, thus\nproviding teams with the opportunity to save time and money.\nFigure3-1 on page21 shows an AutoAI experiment on watsonx.ai.",
    "20 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Figure 3-1 AutoAI experiment on IBM watsonx.ai\nAlthough AutoAI is a no-code tool, the ML pipelines that are generated by an AutoAI\nexperiment can be exported as a notebook. Therefore, more experienced data scientists can",
    "view the code “under the hood” and make modifications and updates to the underlying code.\nFigure3-2 shows an example notebook that was created from a pipeline that was generated\nfrom an AutoAI experiment.\nFigure 3-2 Pipeline notebook that was generated from an AutoAI experiment",
    "Chapter 3. Tools for diverse data science teams 21",
    "AutoAI for Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) is an AI framework for improving the quality of large\nlanguage model (LLM)-generated responses by grounding the model in external sources of",
    "knowledge to supplement the LLM’s internal representation of information. The key benefits\nof RAG solutions include reducing the chance that an LLM leaks sensitive data or ‘hallucinate’\nincorrect or misleading information, and reducing the need for users to continuously train the",
    "model on new data, thus resulting in lower computational and financial costs. For more\ninformation about RAG, see What is Retrieval-Augmented Generation?\nMany organizations are implementing RAG solutions as part of their gen AI efforts. Although",
    "RAG has many benefits, there are challenges too, such as creating a robust and scalable\npipeline, and the time to deliver and implement RAG solutions.\nAutoAI for RAG is a tool that was created by IBM. Similar to AutoAI, AutoAI for RAG is",
    "intended to help AI engineers quickly build RAG solutions. With AutoAI for RAG, AI engineers\ncan quickly build and test multiple RAG pipelines without writing code. From the pipelines that\nare generated, the AI engineer can assess the performance of each pipeline, select the best",
    "pipeline for their team and project, and deploy it into a production or non-production\nenvironment.\nFigure3-3 shows an AutoAI for RAG experiment.\nFigure 3-3 AutoAI for RAG experiment on IBM watsonx.ai\nFor more information about AutoAI for RAG, view the documentation and demo video at",
    "Creating a RAG experiment (fast path) (Beta).\nSynthetic Data Generator\nThe Synthetic Data Generator is a no-code tool that you can use to generate tabular data for\nmodel training. Users have two options to generate synthetic data by using the graphical flow",
    "editor in the Synthetic Data Generator tool:\n(cid:2) Generate synthetic tabular data based on production data, with the goal of masking and\nmimicking this data.\n(cid:2) Generate synthetic data from a custom data schema that is defined by the user by using\nvisual flows and modeling algorithms.",
    "visual flows and modeling algorithms.\n22 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "--- Table 1 from page 38 ---",
    "Figure3-4 shows a view of the Synthetic Data Generator interface on watsonx.ai.\nFigure 3-4 Synthetic Data Generator on IBM watsonx.ai\nData Refinery\nData Refinery is a no-code tool that you can use to prepare and visualize data without writing",
    "any code. With this tool, you can prepare the data for analysis by applying operations such as\nfilters and aggregations, and you can generate visualizations such as pie charts and bar\ncharts to extract insights and share findings with stakeholders.",
    "Figure3-5 shows an example of a Data Refinery flow on watsonx.ai.\nFigure 3-5 Data Refinery flow on IBM watsonx.ai\nChapter 3. Tools for diverse data science teams 23",
    "Prompt Lab\nPrompt Lab is a tool in IBM watsonx.ai that you can use to experiment with prompting\ndifferent FMs, and create and share effective prompts to submit to deployed FMs for\ninferencing. Prompt Lab is a no-code tool that provides AI engineers with three different edit",
    "modes for prompt editing: Chat, Structured, and Freeform. This flexibility enables novice and\nexperienced AI engineers to get the most out of Prompt Lab and watsonx.ai. The Chat mode\nenables users to converse with a FM of their choice. The Structured mode is great for novice",
    "users because it helps them create effective prompts by providing defined fields, and also by\nproviding sample templates to build on. The Freeform mode is great for more experienced AI\nengineers who know how to format a prompt; with this option, users submit prompts in plain\ntext.",
    "text.\nFigure3-6 shows the Chat mode in Prompt Lab on watsonx.ai.\nFigure 3-6 Prompt Lab Chat on IBM watsonx.ai\nLow-code solutions on IBM watsonx.ai\nThis section highlights a few of the low-code tools that are available on the IBM watsonx.ai\nplatform.\nSPSS Modeler",
    "platform.\nSPSS Modeler\nSPSS Modeler is a low-code tool that you can use to transform data and build models with\nlittle to no-programming experience. You can drag-and-drop various nodes onto the canvas to\ncreate a flow to perform various tasks, such as importing data, merging data, and creating",
    "models.\nFigure3-7 on page25 shows an example SPSS Modeler flow in watsonx.ai.\n24 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Figure 3-7 SPSS Modeler flow on IBM watsonx.ai\nFull-code solutions on IBM watsonx.ai\nThis section highlights a few of the full-code tools that are available on the IBM watsonx.ai\nplatform.\nJupyter Notebook editor",
    "platform.\nJupyter Notebook editor\nThe Jupyter Notebook editor is a full-code tool that is available on IBM watsonx.ai. Jupyter\nNotebooks enable more experienced data scientists and developers to write and run code\ndirectly on the IBM watsonx.ai platform. Teams can build more customized and flexible",
    "solutions.\nFigure3-8 shows an example of a Jupyter Notebook on the watsonx.ai platform.\nFigure 3-8 Jupyter Notebook on IBM watsonx.ai\nChapter 3. Tools for diverse data science teams 25",
    "RStudio\nRStudio, like the Jupyter Notebook editor, is a full-code tool that is available on\nIBM watsonx.ai. RStudio enables individuals with programming experience in R to visualize\ndata, create models, and build solutions by using the R programming language on the\nIBM watsonx.ai platform.",
    "IBM watsonx.ai platform.\nFigure3-9 shows the RStudio interface on the watsonx.ai platform.\nFigure 3-9 RStudio on IBM watsonx.ai\nProgrammatic alternative to Prompt Lab\nIBM watsonx.ai has a Python library and REST API that you can use to prompt FMs. This",
    "approach is an alternative to the GUI in the Prompt Lab that is used to prompt FMs. This\noption is great for users who have more programming or technical experience, and for\nprojects and teams that might require an alternative to the Prompt Lab.",
    "For more information about using the REST API or Python library to prompt FMs, see Coding\ngenerative AI solutions.\n26 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "4\nBuilding and using artificial\nChapter 4.\nintelligence models\nThis chapter serves as a resource for setting up, building, fine-tuning, and deploying artificial\nintelligence (AI) models within the IBM watsonx.ai ecosystem. By exploring key features, and",
    "best practices, it aims to empower users (beginners or experienced practitioners) to harness\nthe power of AI for developing effective business solutions and enhancing their AI initiatives.\nThe following topics are described in this chapter:\n(cid:2) 4.1, “Prerequisites and assumptions” on page28",
    "(cid:2) 4.2, “How to use this chapter” on page28\n(cid:2) 4.3, “Building and using AI models in watsonx.ai” on page28\n(cid:2) 4.4, “Getting started with watsonx.ai: Setting up the environment” on page29\n(cid:2) 4.5, “Data preparation and ingestion for AI model building” on page34",
    "(cid:2) 4.6, “Building AI models in watsonx.ai” on page38\n(cid:2) 4.7, “Deploying AI models in watsonx.ai” on page40\n(cid:2) 4.8, “watsonx.ai LLM deployment” on page45\n(cid:2) 4.9, “Operationalizing machine learning and LLM models” on page50",
    "(cid:2) 4.10, “Additional information and where to go next” on page54\n© Copyright IBM Corp. 2025. 27",
    "4.1 Prerequisites and assumptions\nIn this chapter, it is assumed that you have met the following prerequisites:\n(cid:2) An active IBM Cloud® account: You have an active IBM Cloud account. If you do not have\none, see watsonx.ai.",
    "one, see watsonx.ai.\n(cid:2) Familiarity with Jupyter Notebooks: You understand how to navigate and work with Jupyter\nNotebooks for data exploration and model development.\n(cid:2) Proficiency in Python: You have basic to intermediate knowledge of Python programming",
    "because many examples, scripts, and workflows in this chapter use Python code.\n(cid:2) Cloud computing concepts: You have a basic understanding of cloud computing principles,\nincluding concepts such as APIs, data storage, computing resources, and cloud-based\nenvironments.",
    "environments.\n(cid:2) Knowledge of large language models (LLMs): A general understanding of LLMs, their\ncapabilities, and use cases is beneficial for building and fine-tuning AI models within\nwatsonx.ai.\n(cid:2) You read Chapters 1 - 3 of this book.\n4.2 How to use this chapter",
    "4.2 How to use this chapter\nThis chapter is structured to help users of varying expertise levels by providing a general\napproach to understanding, building, deploying, and optimizing AI models by using the\nwatsonx.ai platform.",
    "watsonx.ai platform.\nFor beginners, start with the overview sections to familiarize yourself with the platform's\nfeatures and capabilities. Progress through the chapter sequentially, beginning with\nenvironment setup and basic concepts before delving into more complex topics, such as",
    "model building and optimization.\nFor experienced users, you can go directly to specific chapters of interest, such as model\noptimization techniques, deployment strategies, or advanced configurations. Each section is",
    "modular and provides targeted information and best practices that you can apply immediately\nto your projects.\nThroughout the chapter, you find practical examples, hands-on exercises, and links to more\nresources to help ensure a well-rounded learning experience.",
    "4.3 Building and using AI models in watsonx.ai\nThis section covers the following topics:\n(cid:2) Overview of the watsonx.ai platform\n(cid:2) Key features and capabilities\n4.3.1 Overview of the watsonx.ai platform",
    "4.3.1 Overview of the watsonx.ai platform\nwatsonx.ai is an enterprise-grade AI studio that you use to streamline the development,\ntraining, tuning, and deployment of AI models. It includes generative AI (gen AI) capabilities\nthat are powered by foundation models (FMs).",
    "that are powered by foundation models (FMs).\n28 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "4.3.2 Key features and capabilities\nHere are the key features and capabilities of watsonx.ai:\n(cid:2) Foundation models: Access various powerful, low-cost, and fit-for-purpose models, such\nas the IBM Granite series and other LLMs for tasks such as content generation,",
    "summarization, and classification.\n(cid:2) Data preparation: Use tools for refining and visualizing data to help ensure high-quality\ninputs for model training.\n(cid:2) Model development: Build machine learning (ML) models by using open-source",
    "frameworks with options for code-based, automated, or visual data science approaches.\n(cid:2) Prompt Lab: Experiment with gen AI prompts to enable tasks like question answering,\ncontent generation, summarization, text classification, and data extraction.",
    "(cid:2) Tuning Studio: Fine-tune FMs to customize outputs for specific use cases to enhance\nmodel performance and accuracy.\n(cid:2) InstructLab: At the time of writing, this feature is planned to be integrated into watsonx.ai\nin the future.",
    "in the future.\n4.4 Getting started with watsonx.ai: Setting up the environment\nTo set up the watsonx.ai environment, you must have an IBM Cloud account. To register for\nan account, see Create an IBM Cloud account.\nAt the time of writing, here are the high-level steps to provision watsonx.ai in a",
    "Software-as-a-Service (SaaS) environment:\n1. Set up your IBM Cloud account.\n2. Create a Project in watsonx:\na. Log in to https://dataplatform.cloud.ibm.com/login\nb. Expand the ‘hamburger’ navigation menu, as shown in Figure4-1.\nFigure 4-1 watsonx navigation menu icon",
    "Figure 4-1 watsonx navigation menu icon\nc. Select Projects→ View all projects, as shown in Figure4-2.\nFigure 4-2 View all projects menu\nChapter 4. Building and using artificial intelligence models 29",
    "d. Click New project +, as shown in Figure4-3.\nFigure 4-3 New project+ menu\ne. Enter the project name and, if applicable, upload local files. Click Create, as shown in\nFigure4-4.\nFigure 4-4 Create menu\n3. Provision watsonx.ai Studio:\na. Log in to https://cloud.ibm.com/.",
    "a. Log in to https://cloud.ibm.com/.\nb. To add services from the catalog, use the search box that is shown in Figure4-5.\nFigure 4-5 watsonx.ai utilities search box\nc. Enter “watsonx.ai Studio” and choose the studio from the catalog, as shown in\nFigure4-6.\nFigure 4-6 wastonx.ai utilities list",
    "Figure4-6.\nFigure 4-6 wastonx.ai utilities list\nd. Select a pricing plan (Lite or Professional), and then click Create.\nFigure 4-7 watsonx.ai pricing plan selection\n4. Generate a watsonx.ai API key:\na. Go to https://cloud.ibm.com/iam/apikeys.\nb. Click Create, as shown in Figure4-8.",
    "b. Click Create, as shown in Figure4-8.\nFigure 4-8 Create API key menu option\n30 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "c. Enter the relevant information, and then click Create, as shown in Figure4-9.\nFigure 4-9 Create IBM Cloud API key\nd. After the API key is successfully created, copy or download the key and save it locally,\nas shown in Figure4-10\nFigure 4-10 API key creation",
    "Figure 4-10 API key creation\n5. Open watsonx.ai Studio and associate the watsonx.ai service:\na. In the upper left of your screen, click the four horizontal lines, as shown in Figure4-11\nFigure 4-11 watsonx.ai studio menu icon\nChapter 4. Building and using artificial intelligence models 31",
    "b. Select Resource list, as shown in Figure4-12.\nFigure 4-12 watsonx.ai studio Resource list option\nc. Expand AI / Machine Learning, as shown in Figure4-13.\nFigure 4-13 AI / Machine Learning menu option\nd. Click the watsonx.ai Studio record, as shown in Figure4-14.",
    "Figure 4-14 watsonx.ai Studio record icon\ne. Click View full details, as shown in Figure4-15.\nFigure 4-15 watsonx.ai studio details\nf. Select Launch in→ IBM watsonx, as shown in Figure4-16.\nFigure 4-16 watsonx.ai Launch in option\nFigure4-17 shows the Welcome to watsonx window.",
    "Figure4-17 shows the Welcome to watsonx window.\nFigure 4-17 Welcome to watsonx window\n32 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "g. Click the + in the Projects table.\nh. Enter a project name, for example, test_project, as shown in Figure4-18.\nFigure 4-18 Define details window\ni. Select Storage (if required).\nj. Click Create.\nk. Select Chat and build prompts with foundations models.",
    "l. Click Associate service to associate a Watson ML service to the project, as shown in\nFigure4-19.\nFigure 4-19 Clicking Associate service\nm. Select the displayed Machine Learning service and click Associate, as shown in\nFigure4-20.\nFigure 4-20 watsonx machine learning: Associate service",
    "Chapter 4. Building and using artificial intelligence models 33",
    "n. Go to the Assets tab, and then click New asset, as shown in Figure4-21.\nFigure 4-21 watsonx projects: All assets\no. Select Chat and build prompts with foundation models, as shown in Figure4-22.\nFigure 4-22 Chat and build prompts with foundation models tile",
    "4.5 Data preparation and ingestion for AI model building\nThis section describes the following topics:\n(cid:2) Understanding the importance of data in AI\n(cid:2) Preparing and cleaning data: data quality considerations\n(cid:2) Handling missing data, outliers, and bias",
    "(cid:2) Handling missing data, outliers, and bias\n(cid:2) Ingesting data into watsonx.ai Studio\n(cid:2) Connecting to data repositories and cloud services\n4.5.1 Understanding the importance of data in AI",
    "4.5.1 Understanding the importance of data in AI\nData is the backbone of AI. It shapes the accuracy, effectiveness, and reliability of AI models.\nHigh-quality data enables AI to learn patterns, make predictions, and deliver meaningful",
    "insights. Understanding the importance of data in AI goes beyond mere volume; it involves\nensuring data accuracy, consistency, and fairness. Poor data quality, bias, or gaps can lead to\nflawed models, incorrect predictions, and potential ethical issues. Therefore, this section",
    "focuses on the robust data preparation, cleaning, and validation that is essential for building\nAI systems that are trustworthy, transparent, and impactful in real-world applications.\n34 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "4.5.2 Preparing and cleaning data: data quality considerations\nData quality is a critical factor in the success of AI models because it directly influences their\nperformance and accuracy. Here are some key considerations:",
    "(cid:2) Accuracy: Ensuring that data is correct, consistent, and error-free is vital for creating\nreliable AI models. Inaccurate data can lead to faulty predictions and flawed\ndecision-making.\n(cid:2) Completeness: AI models rely on comprehensive datasets to learn effectively. Missing",
    "data can skew results, which cause incomplete or biased predictions.\n(cid:2) Consistency: Data must be consistent across different sources and formats to help ensure\nthe AI model functions as expected.\n(cid:2) Relevance: Data should be pertinent to the problem the AI aims to solve. Irrelevant data",
    "might introduce noise and reduce model effectiveness.\n(cid:2) Bias and fairness: Addressing data biases and ensuring fairness are critical for building\nethical and unbiased AI systems. Give careful attention to the diversity and representation",
    "in data to avoid discrimination and ensure balanced outcomes.\nHigh data quality maximizes the accuracy, transparency, and applicability of AI systems,\nwhich ultimately enhances their value and impact.\n4.5.3 Handling missing data, outliers, and bias",
    "4.5.3 Handling missing data, outliers, and bias\nHandling missing data, outliers, and bias is crucial for ensuring the accuracy and fairness of\nAI models. Here is how each one is managed:\n(cid:2) Handling missing data:",
    "(cid:2) Handling missing data:\n– Imputation techniques: Missing data can be filled by using statistical techniques such\nas mean, median, or mode imputation, or more complex methods like k-nearest\nneighbors (KNNs) or predictive models.",
    "neighbors (KNNs) or predictive models.\n– Data removal: Sometimes, records with significant missing values are removed if they\nare unlikely to add useful information.\n– Domain knowledge: Input from domain experts can help determine whether missing\ndata should be treated differently based on context.",
    "(cid:2) Handling outliers:\n– Detection: Outliers are identified by using methods like Z-scores, Interquartile Range\n(IQR), or visualization techniques like boxplots.\n– Treatment: Once detected, outliers can be managed by removal, transformation (for",
    "example, log transformation), or capping values based on acceptable thresholds.\n– Contextual consideration: Not all outliers are problematic; they might represent\ngenuine data points. Understanding their impact is crucial before deciding on a course\nof action.\n(cid:2) Addressing bias:",
    "of action.\n(cid:2) Addressing bias:\n– Data auditing: Systematic review of datasets to identify sources of bias, such as\nunderrepresentation of specific groups.\n– Data balancing: Techniques like oversampling, undersampling, or generating synthetic\ndata (for example, SMOTE) can help balance datasets.",
    "Chapter 4. Building and using artificial intelligence models 35",
    "– Algorithmic bias mitigation: Algorithms can be fine-tuned by using fairness constraints\nor reweighting schemes to minimize bias during model training.\n– Regular monitoring: Bias can emerge or change over time, which requires continuous",
    "assessment and model updates to maintain fairness and inclusivity.\nBy handling missing data, outliers, and bias, AI models can provide more accurate, reliable,\nand ethical outcomes, which ultimately increase their utility and impact across diverse\napplications.",
    "applications.\n4.5.4 Ingesting data into watsonx.ai Studio\nThis section describes the following topics:\n(cid:2) Supported data formats and sources\n(cid:2) Manually uploading data to watsonx.ai Studio\nSupported data formats and sources",
    "Supported data formats and sources\nwatsonx.ai Studio supports seamless ingestion of diverse data formats and sources to\nfacilitate efficient model development and deployment. Supported formats include structured",
    "data (CSV, JSON, and Parquet), semi-structured data (XML and Avro), and unstructured data\n(plain text and PDF). Data can be sourced from cloud storage platforms (such as AWS S3\nand IBM Cloud Object Storage), databases (such as PostgreSQL and MySQL), APIs, and\non-premises file systems.",
    "on-premises file systems.\nThe ingestion process is optimized for scalability and can handle large datasets while\nensuring data integrity and compatibility with downstream AI workflows. Integration with\nwatsonx.data and watsonx.governance tools enables a secure, governed data pipeline, which",
    "enhances traceability and compliance.\nManually uploading data to watsonx.ai Studio\nYou can upload files through the watsonx.ai Studio interface by dragging the files there, as\nshown in Figure4-23.\nFigure 4-23 watsonx Studio data upload window\n4.5.5 Connecting to data repositories and cloud services",
    "watsonx.ai provides robust connectivity options to integrate with various data repositories and\ncloud services, which help ensure smooth data access for AI model development. Users can\nconnect to cloud storage solutions such as IBM Cloud Object Storage, AWS S3, Azure Blob",
    "Storage, and Google Cloud Storage, and traditional databases like PostgreSQL, MySQL, and\nMongoDB.\nFigure4-24 on page37 shows the watsonx cloud services connections window.\n36 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Figure 4-24 watsonx cloud services connections\nAt the time of publication, here are the repositories and services that are supported:\n(cid:2) Amazon Redshift\n(cid:2) Amazon S3\n(cid:2) Apache Cassandra\n(cid:2) Apache Derby\n(cid:2) Apache HDFS\n(cid:2) Apache Hive\n(cid:2) Apache Impala",
    "(cid:2) Apache Hive\n(cid:2) Apache Impala\n(cid:2) Apache Kafka\n(cid:2) Box\n(cid:2) DataStax Enterprise\n(cid:2) Denodo\n(cid:2) Dremio\n(cid:2) Dropbox\n(cid:2) Elasticsearch\n(cid:2) Google BigQuery\n(cid:2) Google Cloud Pub/Sub\n(cid:2) Google Cloud Storage\n(cid:2) Google Locker",
    "(cid:2) Google Locker\n(cid:2) Greenplum Database\n(cid:2) IBM Cloud Data Engine\n(cid:2) IBM Cloud Object Storage\n(cid:2) IBM Cloudant®\n(cid:2) IBM Cognos® Analytics\n(cid:2) IBM Data Virtualization Manager for IBM z/OS®\n(cid:2) IBM DataStage® for Cloud Pak for Data\n(cid:2) IBM Db2®",
    "(cid:2) IBM Db2®\n(cid:2) IBM Informix®\n(cid:2) IBM InfoSphere® DataStage\n(cid:2) IBM Match 360\n(cid:2) IBM MQ\n(cid:2) IBM Netezza® Performance Server\n(cid:2) IBM Planning Analytics\n(cid:2) IBM watsonx.data\n(cid:2) MariaDB\n(cid:2) Microsoft Azure Blob Storage\n(cid:2) Microsoft Azure Cosmos DB",
    "(cid:2) Microsoft Azure Cosmos DB\n(cid:2) Microsoft Azure Data Lake Storage\nChapter 4. Building and using artificial intelligence models 37",
    "(cid:2) Microsoft Azure Databricks\n(cid:2) Microsoft Azure Files\n(cid:2) Microsoft Azure SQL Database\n(cid:2) Microsoft Azure Synapse Analytics\n(cid:2) Microsoft Power BI\n(cid:2) Microsoft SQL Server\n(cid:2) Milvus\n(cid:2) MongoDB\n(cid:2) MySQL\n(cid:2) Oracle Database\n(cid:2) PostgreSQL",
    "(cid:2) Oracle Database\n(cid:2) PostgreSQL\n(cid:2) Presto\n(cid:2) Salesforce API\n(cid:2) SAP ASE\n(cid:2) SAP IQ\n(cid:2) SAP S/4HANA\n(cid:2) SingleStoreDB\n(cid:2) Snowflake\n(cid:2) Tableau\n(cid:2) Teradata database\n(cid:2) Vertica",
    "(cid:2) Teradata database\n(cid:2) Vertica\nwatsonx.ai supports secure connection protocols, which include API-based integrations,\nJDBC/ODBC drivers, and file transfer mechanisms like SFTP. With built-in authentication and",
    "access controls, watsonx.ai helps ensure data security while maintaining flexibility for\nenterprise-scale data workflows. By streamlining access to data repositories and services,\nthe platform empowers teams to leverage their existing data infrastructure efficiently.",
    "4.6 Building AI models in watsonx.ai\nThis section describes the following topics:\n(cid:2) Choosing the right model for your use case\n(cid:2) Model creation workflow\n4.6.1 Choosing the right model for your use case\nwatsonx.ai supports many model types to meet diverse business needs:",
    "(cid:2) Supervised models for predictive tasks by using labeled data.\n(cid:2) Unsupervised models for discovering patterns in unlabeled data.\n(cid:2) Reinforcement learning (RL) models for decision-making through rewards and penalties.",
    "(cid:2) Large language models (LLMs) for natural language understanding and generation.\nPretrained LLMs streamline AI development, which enables faster implementation and\npowerful insights across applications.",
    "powerful insights across applications.\nIn addition to its native capabilities, watsonx.ai embraces flexibility with the IBM Bring Your\nOwn Model (BYOM) feature, which enables users to integrate and fine-tune their own LLMs",
    "within the platform for customized solutions. Furthermore, watsonx.ai supports integration\nwith Hugging Face, which enables access to a vast library of pretrained models and tools.\nThis collaboration accelerates development by leveraging open-source innovations while",
    "maintaining watsonx.ai enterprise-grade security and scalability.\n38 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "4.6.2 Model creation workflow\nThis section describes the workflow of model creation.\nModel selection and configuration\nSelecting the appropriate AI model is key to achieving your business objectives, and\nwatsonx.ai provides the flexibility to support both LLM and non-LLM models. Whether your",
    "needs involve predictive analytics, pattern discovery, decision-making, or natural language\nprocessing (NLP), watsonx.ai helps ensure that the correct tools are at your fingertips.\nFor non-LLM tasks, the platform accommodates models such as regression, classification,",
    "clustering, and RL, which enable a wide range of traditional ML applications.\nWhen working with LLMs, watsonx.ai offers various pretrained models in different sizes, from\nlightweight options for tasks that require efficiency and speed to larger, more complex models",
    "that are ideal for nuanced language understanding and generation. Choosing the correct size\ndepends on your specific use case, with smaller models excelling in cost-effective,\nlower-latency scenarios and larger models delivering superior accuracy and depth for\nintricate applications.",
    "intricate applications.\nWith watsonx.ai, you can confidently match the model type and size to your project’s unique\nrequirements to help ensure scalability, efficiency, and impact.\nTraining the model\nTraining your AI model is a crucial step in tailoring it to your specific business needs.",
    "watsonx.ai provides powerful tools and workflows for training both non-LLM models and LLM\nmodels, which help ensure flexibility and precision at every stage of development.\nNon-LLM models\nFor traditional ML tasks, watsonx.ai offers robust training capabilities that leverage tools like",
    "watsonx.ai Studio and AutoAI to streamline and enhance the development process.\nwatsonx.ai Studio\nwatson.ai Studio provides a collaborative environment for data scientists, developers, and\nanalysts to prepare data, build, and train ML models. With features like Jupyter Notebooks,",
    "Python libraries, and model monitoring, watson.ai Studio is designed for flexibility and\nscalability to accommodate projects of any complexity.\nFor more information about watsonx.ai Studio, see IBM Watson Studio.\nAutoAI",
    "AutoAI\nIf you want o accelerate the development process, AutoAI automates key stages of ML, which\ninclude feature engineering, algorithm selection, and hyperparameter optimization. It\nsimplifies the model-building process to make it accessible to users with varying technical",
    "expertise while still delivering highly accurate results.\nFor more information about AutoAI, see IBM AutoAI.\nLLM models\nwatsonx.ai provides advanced features for training and fine-tuning LLMs to deliver seamless\ncustomization and performance.",
    "customization and performance.\nChapter 4. Building and using artificial intelligence models 39",
    "Prompt Lab\nPrompt Lab is an environment for creating and testing prompts that are tailored to specific\ntasks. With Prompt Lab, users can interact with pretrained LLMs, evaluate their outputs, and\nrefine instructions for optimal results, all without extensive coding expertise.",
    "Within Prompt Lab, you can explore and train various training LLM training methodologies,\nsuch as Zero-Shot, Few-Shot, Multi-Shot, and Retrieval-Augmented Generation (RAG).\nFor more information about Prompt Lab, see Prompt Lab.\nTuning Studio",
    "Tuning Studio\nFor deeper customization, Tuning Studio enables fine-tuning of LLMs on your proprietary\ndata, which helps ensure that the model adapts to your specific domain while maintaining\nhigh performance. This feature is ideal for organizations seeking more targeted insights and",
    "applications from their AI.\nFor more information about Tuning Studio, see Tuning Studio.\nInstructLab\nAt the time of writing, InstructLab is not available.\nInstructLab will revolutionize the training process by enabling users to craft task-specific",
    "instructions, which further enhance the precision of LLMs in generating accurate and\nactionable outputs. This tool simplifies the process of aligning model behavior with unique\nbusiness objectives.\nWith these comprehensive training tools, watsonx.ai empowers users to harness the full",
    "potential of their models, whether refining traditional ML algorithms or unleashing the power\nof cutting-edge LLMs.\n4.7 Deploying AI models in watsonx.ai\nThis section explores the process of deploying AI models in the watsonx.ai platform. It",
    "provides a detailed overview of two major deployment options: Studio and Prompt Lab. From\ndeploying models as APIs for real-time inference to batch processing workflows, the\nwatsonx.ai platform helps ensure flexibility and scalability. Readers gain insights into selecting",
    "the most appropriate deployment strategy for their use case while also learning best practices\nto optimize performance and reliability in production environments.\n4.7.1 watsonx.ai Studio deployments\nDeploying models in watsonx.ai Studio involves several key steps to help ensure that your AI",
    "assets are effectively managed and operational. Here is a concise guide to help you through\nthe process:\n1. Create a deployment space: Begin by establishing a deployment space within the studio.\nThis space serves as a collaborative environment where you can manage and deploy your",
    "AI assets. To set up a deployment space, go to the Deployment Space section in the\nStudio interface and follow the prompts to create a space (Figure4-25 on page41).\n40 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Figure 4-25 watsonx.ai studio projects: Deployments\n2. Promote or import your model: When your deployment space is ready, add your trained\nmodel to it. If your model is in a project, promote it to the deployment space. Alternatively,",
    "you can import models that are trained externally by uploading them directly into the\ndeployment space. Ensure that the model files are in a compatible format and that any\nnecessary dependencies are addressed.\na. Click the three dots, and then click Promote to space (Figure4-26).",
    "Figure 4-26 Model interface window\nNote: To help ensure that your trained model is in the right format and compressed, see\nAdding a model by using UI.\nb. Enter deployment_test in to the Name field, select Production under Deployment\nstage, and then click Create (Figure4-27).",
    "stage, and then click Create (Figure4-27).\nFigure 4-27 Create a deployment space\nChapter 4. Building and using artificial intelligence models 41",
    "3. Create the deployment: With your model in the deployment space, initiate the deployment\nprocess:\na. Go to the Deployments window (Figure4-28).\nFigure 4-28 Deployments window\na. Select deployment_test\nb. Click the generated model’s service (Figure4-29).\nFigure 4-29 Services",
    "Figure 4-29 Services\nc. Click New deployment, which opens the window that is shown in Figure4-30 on\npage43.\n42 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Figure 4-30 Create a deployment\nd. Under Deployment type, select either Online or Batch, and enter\nregression_model_deployment under Name and regression_model_service under\nServing name. Click Create.\nOnline deployment is ideal for real-time processing, where the model handles input",
    "data and provides immediate predictions. Batch deployment is suitable for processing\nlarge datasets in bulk, which generate predictions for a collection of inputs at\nscheduled intervals or on-demand.\nChapter 4. Building and using artificial intelligence models 43",
    "4. Test the deployment: After deployment, validate the model’s functions by going to the\nnewly created deployment and clicking regression_model_deployment, as shown in\nFigure4-31.\nFigure 4-31 Deployment overview",
    "Figure4-31.\nFigure 4-31 Deployment overview\nwatsonx.ai Studio provides multiple ways to test the deployment, such as the Test tab and\ncode snippets (Figure4-32).\nFigure 4-32 Deployment testing\n44 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "By following these steps, you can effectively deploy and manage your AI models within\nwatsonx.ai Studio, which helps ensure that the models are ready for production use and\ncapable of delivering valuable insights.\n4.8 watsonx.ai LLM deployment",
    "4.8 watsonx.ai LLM deployment\nDeploying models in watsonx.ai Studio involves several key steps to help ensure that your AI\nassets are effectively managed and operational. This section provides a concise guide to help\nyou through the process:\n4.8.1 Model packaging and exporting",
    "4.8.1 Model packaging and exporting\nTo package and export a model, complete the following steps:\n1. Go to the Prompt Lab window, and if you do not already have a prompt set up, populate it\nby using the example that is shown in Figure4-33. In this lab, you use a text classification\nprompt.",
    "prompt.\nFigure 4-33 watsonx Prompt Lab\nChapter 4. Building and using artificial intelligence models 45",
    "2. Generate a response by the prompt. Keep the decoding method as Greedy, and set the\nmax tokens to 5 to produce Positive and Negative text only (Figure4-34).\nFigure 4-34 Model parameters\n3. Click Generate, which tests the prompt. Then, click the View code icon (Figure4-35).",
    "Figure 4-35 View code icon\n4. Copy the code to a notepad application (Figure4-36).\nFigure 4-36 Code example\n46 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "The code (Figure4-37) is an example of a REST call that starts the model. watsonx.ai\nalso provides a Python API for model invocation, which you review later in this lab.\nThe header of the REST request includes the URL where the model is hosted and a",
    "placeholder for the authentication token. At the time of writing, all users share a single\nmodel inference endpoint. In the future, IBM plans to provide dedicated model endpoints.\nSecurity is managed by the IBM Cloud authentication token, which is described later in\nthis section.",
    "this section.\nThe body of the request contains the entire prompt.\nFigure 4-37 Curl command example\n5. At the end of the request, you specify the model parameters and the project ID, as shown\nin Figure4-38.\nFigure 4-38 Curl command details",
    "in Figure4-38.\nFigure 4-38 Curl command details\nChapter 4. Building and using artificial intelligence models 47",
    "To look up the project ID, select Project→ General→ Manage in the watsonx.ai project,\nas shown in Figure4-39.\nFigure 4-39 Projects Manage view\n6. Save the newly configured prompt as a notebook. Select Standard notebook as the\nasset type and then select Save as, as shown in Figure4-40.",
    "Figure 4-40 Notebook save icon\n7. You will now create an authentication token. Open the navigation menu (four horizontal\nbars) in the upper left of the watsonx interface and select Access (IAM), as shown in\nFigure4-41 on page49.",
    "Figure4-41 on page49.\n48 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Figure 4-41 Access (IAM) menu item\n8. Select API Keys→ Create. Give the token a name and save it in a notepad (Figure4-42).\nYou use the token in a Python notebook.\nFigure 4-42 API keys\n9. Go to your watsonx project and open the notebook that you saved in step 8 (Figure4-43).",
    "Figure 4-43 watsonx Studio projects overview\nChapter 4. Building and using artificial intelligence models 49",
    "10.Review the sample notebook.\nThis notebook acts as a client application that starts the deployed LLM with a Python SDK.\nYou use the notebook as a client for simplicity of testing during this lab.\nEnterprise client applications can be implemented in Python, Java, .NET, and many other",
    "programming languages. LLMs that are deployed in watsonx.ai can be started either with\nREST calls or the Python SDK.\nRun the notebook to test the LLM with your prompts.\n4.9 Operationalizing machine learning and LLM models",
    "Now that you have a machine learning (ML) model or LLM built, you now enter the operational\nphase. Much like how traditional application development created the need for formal DevOps\ntools and systems, so too have AI models created the need for ModelOps. ModelOps is the",
    "practice of enabling the deployment and management of models throughout the application\ndevelopment and deployment lifecycle with the goal of operationalizing models in production.\nAs a joint endeavor with traditional DevOps, ModelOps takes the feedback and",
    "measurements that are taken in the DevOps lifecycle to iterate on the training, testing and\ndeploying stages of the ModelOps lifecycle.\nKey stages in the ModelOps lifecycle include governance, monitoring, deployment of",
    "infrastructure, and model versioning. IBM offers various tools to help facilitate these\nprocesses, each of which has its own place in the lifecycle. Examples of such tools include\nthe following ones:\n(cid:2) IBM watsonx.gov helps govern and monitor model key performance indicators (KPIs).",
    "(cid:2) IBM Instana®™ helps monitor LLM performance, responsiveness, and throughput at the\napplication level.\n(cid:2) IBM Turbonomic® can help dynamically scale up and down infrastructure as the workload\nagainst your models changes.",
    "against your models changes.\n(cid:2) IBM API Connect® provides a GUI wizard to create AI-aware APIs and products, plus\nintegration with AI services to forward requests and manage responses.\nTo demonstrate how these models can be deployed into existing applications, we briefly walk",
    "through how to call these models, and integrate them into the DevOps and ModelOps\nlifecycle.\n4.9.1 Calling ML models by using API calls\nWhen you have built a model, your ML model is live and ready to perform as an inference",
    "endpoint. This endpoint is your gateway to interact with the model so that you can send data\nand receive predictions in return. Here, we walk through how you can use it effectively.\nSecuring your API key\nBefore making any calls to your endpoint, you need an API key for secure access. For more",
    "information about generating this key, see 4.8, “watsonx.ai LLM deployment” on page45.\nHere is a quick overview:\n1. Go to your deployed model in watsonx.ai Studio.\n2. Go to the Access tab under the Deployment settings.",
    "3. Generate your API key and store it securely. (You use it to authenticate your requests.)\n50 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Making an API call\nWith your API key in hand, you are ready to communicate with your model. Figure4-44 shows\nan example of a simple POST request (by using a cURL command) to send input data and\nretrieve predictions.\nFigure 4-44 watsonx.ai regression model deployment tool",
    "Note: The watsonx.ai user interface (UI) prepopulates different ways to call the model,\nsuch as cURL, Java, JavaScript, Python, Scala, and others.\nChapter 4. Building and using artificial intelligence models 51",
    "Visualizing the process\nFigure4-45 shows a simple way to visualize what occurs during an API call.\nFigure 4-45 API call visualization\nYour application sends input data to the endpoint; the model processes the data; and the",
    "results are sent back as predictions. Whether you are handling real-time data (through online\ndeployment) or batch processing, this streamlined interaction helps ensure that you can make\nthe most of your deployed model.\n4.9.2 Calling Prompt Lab LLM models by using API calls",
    "Calling an LLM model from Prompt Lab is similar to calling an LLM model from watsonx.ai’s\nStudio; the only difference is where to find the code within the UI to do so. To accomplish this\ntask, complete the following steps:",
    "task, complete the following steps:\n1. Go to Prompt Lab, and then build your prompt in either the Chat, Structured, or Freeform\ntab.\n2. In the upper right, click the View code icon (Figure4-46).\nFigure 4-46 View code icon",
    "Figure 4-46 View code icon\nThe Prompt Lab automatically creates everything that you need to copy and paste your\nprompt into the application of your choosing (in either cURL, Node.js, or Python), as shown in\nFigure4-47 on page53.",
    "Figure4-47 on page53.\n52 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Figure 4-47 watsonx.ai Prompt Lab code window\nNote: Include your bearer access token. For more information about this token within the\nIBM ecosystem, see Generating a bearer token.\n4.9.3 IBM watsonx Assistant\nOperationalizing AI and ML models by using IBM technology helps ensure seamless",
    "deployment and management across enterprise environments while delivering measurable\nbusiness outcomes. IBM watsonx.ai provides a robust platform for building, fine-tuning, and\ndeploying AI models to enable data scientists to leverage pre-trained models or create",
    "custom solutions. Once models are developed, they can be containerized and deployed by\nusing Red Hat OpenShift, which is the IBM enterprise Kubernetes platform, which helps\nensure scalability, high availability, and integration with an existing infrastructure. IBM Watson",
    "Studio simplifies model lifecycle management by providing end-to-end capabilities for version\ncontrol, testing, and collaboration. Real-time monitoring is enabled through IBM Instana\nObservability so that teams can track KPIs, detect anomalies, and maintain model health in\nproduction environments.",
    "production environments.\nChapter 4. Building and using artificial intelligence models 53",
    "Figure4-48 shows an overview of IBM watsonx Assistant.\nFigure 4-48 IBM watsonx Assistant overview\nEnsuring the ongoing success of operational AI and ML solutions requires integrating\ngovernance, automation, and business alignment. IBM watsonx.governance enforces",
    "responsible AI principles by providing tools for bias detection, lineage tracking, and\ncompliance management, which help organizations meet regulatory requirements and ethical\nstandards. Automated deployment pipelines with IBM DevOps for AI streamline continuous",
    "integration and continuous delivery (CI/CD), which enables rapid updates and retraining to\naddress data drift or evolving business needs. Feedback loops that are powered by\nIBM Watson Discovery facilitate continuous improvement by analyzing real-world user",
    "interactions to enhance model performance. By leveraging IBM’s AI and hybrid cloud\ncapabilities, organizations can operationalize AI and ML solutions effectively, which drive\ninnovation while helping ensure reliability and trustworthiness.",
    "For more information about tools that are related to operationalizing your models, see 4.10.3,\n“watsonx.ai data pipeline and orchestration” on page55.\n4.10 Additional information and where to go next\nIn this chapter, you learned the following things:",
    "(cid:2) How to set up your environment, which includes IBM Cloud accounts and project\nconfiguration.\n(cid:2) The key features of watsonx.ai, such as FMs, Prompt Lab, and Tuning Studio.\n(cid:2) The importance of data quality, cleaning, and ingestion for AI model development.",
    "(cid:2) Building and training AI models, which include traditional ML and LLMs, by using tools like\nAutoAI, Tuning Studio, and InstructLab.\n(cid:2) Deploying AI models as APIs for real-time or batch processing and integrating them with\nenterprise systems.",
    "enterprise systems.\n54 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "4.10.1 Additional support and documentation\nwatsonx.ai has extensive support and documentation to help users maximize the platform's\ncapabilities. The IBM watsonx Documentation Portal offers a comprehensive collection of",
    "resources, which include detailed user guides, tutorials, API references, and best practices.\nWhether you are starting or looking to optimize your AI workflows, the portal helps ensure that\nyou have the guidance that you need to succeed.\nHighlights include the following resources:",
    "Highlights include the following resources:\n(cid:2) Getting Started Guides: Step-by-step instructions for onboarding and initial setup.\n(cid:2) Model Development Resources: In-depth documentation about training, fine-tuning, and\ndeploying both LLM and non-LLM models.",
    "deploying both LLM and non-LLM models.\n(cid:2) Troubleshooting and FAQs: Solutions to common issues and tips for resolving challenges\nefficiently.\n(cid:2) Integration Guidance: Instructions for incorporating watsonx.ai into existing workflows and\nleveraging tools, such as Hugging Face and BYOM.",
    "leveraging tools, such as Hugging Face and BYOM.\nThis rich repository of knowledge empowers users at every skill level to confidently build,\ndeploy, and scale AI solutions with watsonx.ai.\n4.10.2 watsonx.ai API reference",
    "4.10.2 watsonx.ai API reference\nFor comprehensive guidance about using watsonx.ai capabilities, the IBM watsonx API\nDocumentation offers detailed information about available APIs, including endpoints, request",
    "parameters, and response structures. This resource is essential for developers that want to\nintegrate watsonx.ai into their applications by providing clear instructions and examples to\nfacilitate seamless implementation. Whether you are working with FMs, performing text",
    "inference, or managing deployments, this documentation serves as a valuable reference to\nhelp ensure effective and efficient usage of watsonx.ai features.\n4.10.3 watsonx.ai data pipeline and orchestration\nFor more information about data pipelining and orchestration, see the following resources:",
    "(cid:2) IBM Orchestration Pipelines:\nhttps://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-orchestrati\non-overview.html?context=wx\n(cid:2) IBM Seismic: Introducing AI Agent Orchestration (IBMid required):\nhttps://ibm.seismic.com/Link/Content/DCbHf2RCFTPf3G9Cc2PBGggJWfGV",
    "(cid:2) Instana:\nhttps://www.ibm.com/products/instana/generative-ai-monitoring\n(cid:2) Turbonomic:\nhttps://community.ibm.com/community/user/aiops/blogs/cheuk-hung-lam/2024/05/28/\nturbonomic-tackles-gpus-for-genai-workloads\nhttps://www.ibm.com/case-studies/ibm-big-ai-models-turbonomic",
    "Chapter 4. Building and using artificial intelligence models 55",
    "56 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "5\nAdvanced capabilities of\nChapter 5.\nwatsonx.ai\nwatsonx.ai embodies IBM’s extensive expertise in artificial intelligence (AI) and foundation\nmodels (FMs), which combine advanced AI research with practical tools to make large",
    "language models (LLMs) efficient and versatile across many applications. Underlying\nwatsonx.ai is its integration of FMs that are tuned to accelerate and optimize business\noperations. These models are positioned to perform at the intersection of language",
    "understanding, structured data processing, and knowledge retrieval, which enhance the\nability to extract, refine, and use vast amounts of unstructured data.\nThe platform's capabilities extend beyond simple text processing to include complex",
    "interactions between structured and unstructured data sources, which enable the model to\ndraw relevant information and learn domain-specific knowledge. For example, watsonx.ai\nsupports both general-purpose and highly specialized model architectures, which facilitate",
    "the design of task-optimized LLMs that serve nuanced business needs while ensuring data\nprivacy and regulatory compliance. Its multi-modal capabilities enable seamless handling of\ndiverse data types (such as text, image, and audio inputs) and applications to traverse",
    "disparate data landscapes cohesively, which achieves a high level of contextual relevance\nand adaptability.\nThe watsonx.ai platform has several capabilities to support advanced use cases such as\nprompt engineering, multi-task prompt tuning, and fine-tuning.",
    "The following topics are described in this chapter:\n(cid:2) 5.1, “Prompt engineering” on page58\n(cid:2) 5.2, “Multitask prompt tuning” on page61\n(cid:2) 5.3, “Fine-tuning” on page64\n(cid:2) 5.4, “InstructLab” on page67\n© Copyright IBM Corp. 2025. 57",
    "5.1 Prompt engineering\nPrompt engineering within the watsonx.ai ecosystem serves as an essential component in\nharnessing the full potential of language models. By precisely framing prompts, users can\nguide LLM responses toward relevance and coherence, which greatly enhances the utility of",
    "generated outputs. The process of prompt engineering in watsonx.ai is highly nuanced, and it\ninvolves detailed adjustments to phrasing, context, and iterative feedback mechanisms to\nyield wanted outputs consistently. Prompt engineering plays a pivotal role in directing LLMs to",
    "perform specific tasks with high precision, a task that requires linguistic adjustments and a\ndeep understanding of the underlying model dynamics.\nThe reason why prompt engineering is important is because it is a way to make generalist",
    "models (LLMs) perform a specific task. Without quality information, well-defined instructions,\nand a clear set of examples, the model might misbehave and hallucinate in various ways.\nPrompt engineering is about writing something in a better, clearer, and cleaner form. It also",
    "involves using specific system tokens that are for only the model that is used and, if available,\na series of examples that provide better information to the model so that it can perform as\nintended. This approach is explored more in-depth in this section.\n5.1.1 Prompting techniques",
    "5.1.1 Prompting techniques\nThere are three main prompting techniques:\n(cid:2) Zero-shot prompting\n(cid:2) One-shot prompting\n(cid:2) Few-shot prompting\nThese techniques are not learning techniques, but prompting techniques only, which improve",
    "model performances at inference-time without modifying the original model.\nThese techniques leverage the model's existing capabilities without requiring fine-tuning or\nparameter updates, which make them lightweight and adaptable solutions for various use",
    "cases. This section provides an explanation of each prompting technique, and when it is best\nto use each approach.\n(cid:2) Zero-shot prompting: Relies solely on the model's pre-trained knowledge to generate\nresponses without providing any task-specific examples in the prompt. Instead, the input",
    "typically includes clear instructions or a well-defined query that guides the model to\nperform the task, for example, asking a model to summarize a paragraph or translate a\nsentence into another language without providing prior examples. The effectiveness of this",
    "approach hinges on the clarity and precision of the prompt and the model's inherent ability\nto generalize across diverse tasks.\n(cid:2) One-shot prompting: In this approach, a single example of the task is embedded within the",
    "prompt, alongside the query or instructions. This example serves as a reference for the\nmodel to infer the behavior. By including a single demonstration, one-shot prompting can\nenhance performance for tasks that require nuanced or domain-specific understanding",
    "because it provides a concrete context for the model to interpret the instructions.\n58 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "(cid:2) Few-shot prompting: Expands on this concept by incorporating multiple examples of the\ntask in the prompt. The additional examples provide a richer context and help the model\nbetter understand complex patterns or subtle variations in the task. Few-shot prompting is",
    "useful for tasks that require multi-step reasoning, handling of ambiguous inputs, or\nunderstanding domain-specific jargon. However, it demands careful prompt construction\nto balance informativeness and brevity because excessive length can lead to token\nlimitations or diminished performance.",
    "limitations or diminished performance.\nFigure 5-1 LLM prompting method types overview\n5.1.2 Importance of system tokens\nIn addition to zero-shot, one-shot, and few-shot prompting techniques, system tokens (also",
    "known as system-level instructions or control tokens) play a critical role in crafting effective\nprompts. These tokens provide metadata or guidance to steer the behavior of the language\nmodel at a higher level, often defining the context, tone, or expected behavior of the model",
    "during inference. Importantly, the implementation and interpretation of these tokens can vary\nacross different models, making their effective use model specific.\nSystem tokens enable users to establish a “role” or context for the model, shaping its",
    "responses beyond what is specified in the natural language prompt.\nBy embedding these tokens, users can accomplish the following goals:\n(cid:2) Control output behavior: Ensure the order of prompt completions between the main\nprompt areas of System, Assistant, and User",
    "prompt areas of System, Assistant, and User\n(cid:2) Reduce ambiguity: Guide the model's interpretation of the task, especially in contexts\nwhere instructions alone might be misinterpreted.\n(cid:2) Enhance few-shot learning: When combined with example-based prompting, system",
    "tokens can provide an overarching framework that amplifies the impact of the examples.\n5.1.3 Model-specific peculiarities\nDifferent language models interpret and use system tokens in unique ways due to their",
    "architecture and pre-training data. The best practices for incorporating system tokens are to\nunderstand model documentation. Because the behavior of system tokens is\nmodel-dependent, consulting the model's technical documentation is essential to",
    "understanding how tokens are implemented and what variations are supported.\nChapter 5. Advanced capabilities of watsonx.ai 59",
    "5.1.4 How watsonx.ai supports prompt engineering\nRegarding prompt engineering, the simplest way to interact with LLMs is extensive but\npeculiar. Fortunately, the watsonx.ai platform enables prompt engineering by providing a",
    "series of tools for its usage. Figure5-2 shows a series of these tools in the Prompt Lab\nsection of watsonx.ai.\nFigure 5-2 watsonx.ai Prompt Lab dashboard\nPrompt Lab is the main area to access and interact with LLMs. Here, it is possible to interact",
    "with models and perform prompt engineering techniques. In Prompt Lab, users have three\nmodes to select from to interact with LLMs:\n(cid:2) Chat mode: Provides a simplistic, multimodal chatbot-like interaction with capabilities such",
    "as memory and document understanding. It is good for model interaction and for simple\nsystem prompt definition and a zero-shot-prompting approach.\n(cid:2) Structured mode: Provides a way to set up your prompt to create a particular prompt",
    "engineering setting within the main model Instructions and Examples, where the examples\nare the input/output series that is provided in a few-shot-prompting setting. It automatically\napplies the system tokens for a specific model to best fit the one/few-shot-prompting\nsetting.",
    "setting.\n(cid:2) Freeform mode: Provides more advanced users with the option to be free of crafting their\nraw prompt by using all the possible prompt engineering capabilities that leverage raw\nsystem tokens. Although this mode leverages the power of prompt engineering and",
    "freedom, it requires specific skills in understanding system tokens, what they are for a\nspecific model, and how to use them.\nAlthough prompt engineering provides a faster way to the objective, it is not always the best",
    "tool to use or the most capable tool that is available. As task difficulty and complexity\nincreases, watsonx.ai can use more advanced model enhancing techniques. We explore\nthese techniques in the following sections of this chapter.",
    "60 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "5.2 Multitask prompt tuning\nMultitask prompt tuning within watsonx.ai builds on traditional prompt engineering by\nimplementing adaptive mechanisms to refine the prompt’s interpretative accuracy over time.\nThis approach differs fundamentally from prompt engineering because it modifies prompt",
    "content and continuously aligns the model’s interpretative layers with domain-specific\nexpectations. In essence, prompt tuning enables models to retain learned adjustments across\nsessions, which support consistency and reduce the need for extensive re-engineering.",
    "Prompt tuning leverages techniques such as embedding adjustments and parameter scaling\nto influence the model’s internal state and guide responses within boundaries. Using the\nwatsonx.ai dynamic configuration settings, developers can set up continuous tuning",
    "processes that adapt prompts based on evolving business contexts, which lead to a finely\ncalibrated model that reflects current operational realities. This capability enables rapid\nadaptation without costly retraining, and the watsonx.ai architecture permits this tuning to",
    "take place seamlessly, which enables real-time adjustments as new data is ingested or as\nuser preferences change.\nIn this setting, it is not the LLM that is modified. Instead, a dedicated, smaller LLM is trained to",
    "generate the best possible prompt adjustment for each prompt in the input. The smaller LLM\nleverages the system tokens that are available for each LLM on watsonx.ai and produces\nnew, compatible virtual tokens to enhance the performances. The smaller LLM is trained by",
    "using a loss function that accounts for the resulting response from the immutable (in this\nsetting) generative LLM model that you want to improve, and adapts its weights to create\nbetter prompts through a tunable soft prompt, as shown in Figure5-3.\nFigure 5-3 Prompt tuning overview",
    "Figure 5-3 Prompt tuning overview\nWith prompt tuning, you create a model that automatizes the prompt engineering task, which\nmakes it dynamically adaptive to new, incoming inputs over time. The key benefit of prompt",
    "tuning is performing tuning in ways that are better than what experts can do for certain tasks,\nthat is, the best token leading to a successful completion of the input task. Prompt tuning can\ndo this task because it leverages 100 - 10000 examples to learn which token is the best one",
    "to add to a starting pre-engineered prompt to minimize the loss of the generative model.\nChapter 5. Advanced capabilities of watsonx.ai 61",
    "The watsonx.ai platform provides a simplistic way of using prompt tuning, as shown in\nFigure5-4.\nFigure 5-4 Prompt tuning in watsonx.ai Tuning Studio\n5.2.1 Prompt tuning parameters\nIn watsonx.ai Tuning Studio, you can use multitask prompt tuning by leveraging various",
    "prompt tuning parameters. The process of optimizing hyperparameters for prompt tuning,\nsuch as batch size, the number of epochs, the learning rate, and accumulation steps, plays a\ncritical role in achieving task-specific adaptation and helping ensure effective usage of LLMs.",
    "Each of these parameters impacts the training process in unique ways by influencing the\ngeneralization ability, stability, computational efficiency, and performance of the fine-tuned\nprompts.\n(cid:2) Batch size refers to the number of training samples that are processed simultaneously",
    "during each forward and backward pass through the model. It is a fundamental factor in\ndetermining the balance between computational efficiency and the quality of gradient\nupdates. Larger batch sizes tend to stabilize gradient updates by averaging over more",
    "samples, which enable faster convergence. However, they often require significant\ncomputational resources and might overlook fine-grained variations in the dataset, which\nmight potentially limit the prompt's ability to address nuanced tasks. Conversely, smaller",
    "batch sizes enable greater granularity in gradient computations, which is advantageous for\nsmall datasets or specific tasks. However, small batches introduce noisier gradient\nupdates, which require more iterations to converge effectively. To optimize batch size,",
    "practitioners should aim for a balance that satisfies computational feasibility while meeting\nthe requirements of the task. Dynamic batch sizing (adjusting the batch size during\ntraining) can further stabilize the learning process and enhance overall effectiveness.",
    "(cid:2) The number of epochs represents the total number of complete passes that the training\nalgorithm makes through the dataset. This parameter directly influences how thoroughly\nthe model explores the data to refine its prompt parameters. A higher number of epochs",
    "allows the model to capture intricate patterns, which improves task-specific adaptation.\nHowever, this approach comes with the risk of overfitting, especially with smaller datasets,\nwhich reduce the generalization of the prompts. Conversely, a lower number of epochs",
    "minimizes the risk of overfitting but might lead to underoptimized prompts that fail to\nleverage the model's full potential. To strike the right balance, monitor validation loss and\napply early stopping criteria to help ensure that training halts before overfitting occurs.",
    "Using pre-trained checkpoints can also reduce the need for extensive epochs because\nthese starting points encapsulate foundational knowledge that accelerates convergence.\n62 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "(cid:2) The learning rate governs the size of the updates that are made to prompt parameters\nduring each optimization step. It influences the speed and stability of the training process.\nA high learning rate expedites convergence, which reduces training time but risks",
    "overshooting optimal solutions, and can lead to suboptimal performance or even\ndivergence. Conversely, a low learning rate enables a more precise exploration of the\nparameter space, which increases the likelihood of finding an optimal solution at the cost",
    "of prolonged training. Effective strategies include employing learning rate schedules, such\nas cosine decay or step-based decay, which adjust the learning rate dynamically during\ntraining. Warm-up strategies, where the learning rate gradually increases at the start of",
    "training, can also mitigate initial instability and improve overall training robustness.\n(cid:2) The concept of accumulation steps addresses memory constraints by enabling gradient\naccumulation across several mini-batches before updating the model’s parameters. This",
    "approach effectively simulates larger batch sizes without exceeding hardware memory\nlimits, which make it valuable for memory-constrained environments. Accumulation steps\nsmooth gradient updates by averaging across multiple mini-batches, which improve",
    "stability at the cost of increased training time. Optimizing this parameter involves selecting\nan accumulation step size that balances memory efficiency with the effective batch size.\nCombining this approach with batch size tuning can further optimize resource usage and\nenhance performance.",
    "enhance performance.\n5.2.2 Interdependencies and holistic tuning strategies\nThese hyperparameters are interdependent because changes in one can influence the\nbehavior of others. For example, increasing the number of epochs without modifying the",
    "learning rate might lead to overfitting, and combining a high batch size with too few epochs\nmight result in undertrained prompts. To navigate these interdependencies, practitioners can\nemploy regularization techniques such as data augmentation to counteract overfitting in",
    "high-epoch scenarios. Gradient clipping can also be used to prevent instability during training,\nparticularly when high accumulation steps are involved.\nPerformance metrics, which include task-specific measures like accuracy or F1-scores,",
    "should guide the evaluation of prompt tuning effectiveness. Monitoring loss convergence and\ngradient stability help ensure that the chosen hyperparameters lead to tangible\nimprovements.\nCarefully calibrating batch size, the number of epochs, the learning rate, and accumulation",
    "steps enables precise optimization of prompt tuning, which unlocks the full potential of LLMs\nfor specific tasks. By managing these parameters holistically, practitioners can achieve\nperformance gains while balancing computational efficiency and resource constraints.",
    "Chapter 5. Advanced capabilities of watsonx.ai 63",
    "5.3 Fine-tuning\nFine-tuning within the watsonx.ai ecosystem represents the next layer of model\nspecialization, where foundation LLMs undergo retraining on domain-specific datasets to\nenhance accuracy and relevance for specific applications. Fine-tuning goes beyond prompt",
    "adjustments by modifying the model’s weights to encode new knowledge or adapt to complex\nindustry-specific language structures, terminologies, and operational protocols. This process\nis beneficial for industries that require high precision in terminology and context, such as",
    "healthcare, finance, and legal services. Fine-tuning within the watsonx.ai ecosystem\nexemplifies a sophisticated approach to model specialization, which enables foundation LLMs\nto adapt to domain-specific needs through retraining. Unlike prompt tuning, which focuses on",
    "lightweight modifications to steer model behavior, fine-tuning directly alters the model’s\nweights to encode new knowledge or align with complex industry-specific language structures\nand terminologies.\n5.3.1 Challenges with fine-tuning",
    "5.3.1 Challenges with fine-tuning\nTo fully appreciate the significance of watsonx.ai capabilities, it is essential to understand the\ninherent complexity of fine-tuning in general. At its core, fine-tuning involves retraining a",
    "model’s internal weights on carefully curated datasets to refine its understanding of specific\nterminologies, language patterns, or operational protocols. This process differs from\nlightweight techniques like prompt tuning, which adjusts model behavior externally without",
    "altering its core structure. Fine-tuning, by contrast, modifies the model itself, embedding new\nknowledge directly into its architecture.\nWhile this approach enables unparalleled precision and customization, it introduces many\nchallenges. Here are some common challenges with fine-tuning:",
    "(cid:2) Data management: One of the primary difficulties with fine-tuning. Fine-tuning demands\ndatasets that are relevant and meticulously prepared, which includes ensuring that the\ndata is formatted correctly, free from bias, and representative of the target domain. Even",
    "determining the appropriate size of the dataset requires careful consideration because too\nlittle data risks underfitting, and too much can lead to overfitting or unnecessary\ncomputational burdens. For example, in watsonx.ai, datasets are limited to 200 MB for",
    "JSON or JSONL files, or up to 10,000 examples when sourced from connected data\nstores. These constraints are carefully balanced to optimize efficiency without sacrificing\nperformance, but managing these parameters manually would be daunting for most users.",
    "(cid:2) Precise calibration of numerous hyperparameters: These hyperparameters include the\nlearning rate, batch size, number of training epochs, and strategies for regularization,\namong others. Each of these parameters is interdependent, which means that altering one",
    "can have cascading effects on others. Achieving the optimal configuration often involves\nextensive trial-and-error or the usage of advanced hyperparameter optimization\ntechniques like Bayesian search. This complexity is compounded when working with large",
    "models, which can have billions of parameters, which require significant computational\nresources and expertise to manage effectively.\n64 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "(cid:2) Computationally demanding workloads requiring a high-performance architecture: Large\nmodels can have billions of parameters, which require a significant amount of\ncomputational resources. Even with the right data and parameters in place, fine-tuning",
    "remains computationally demanding. Training these large LLMs requires access to\nhigh-performance hardware, such as multi-GPU, along with robust memory and storage\ncapabilities. For organizations without dedicated AI infrastructure, these requirements are\noften prohibitive.",
    "often prohibitive.\n(cid:2) Ensuring stability during the training process: Large-scale optimization algorithms are\nprone to issues like exploding or vanishing gradients, so achieving convergence without\ndiverging from the optimal solution requires careful tuning and monitoring.",
    "5.3.2 How watsonx.ai addresses fine-tuning challenges\nBy automating the complexities of fine-tuning, watsonx.ai transforms what was once a\nlabor-intensive and technically demanding process into an accessible, streamlined",
    "experience. This approach lowers the barrier to entry for organizations looking to adopt AI\nand enables experienced practitioners to focus on higher-level strategic goals rather than\ngetting bogged down in technical minutiae. With its combination of cutting-edge technologies,",
    "managed infrastructure, and user-centric design, watsonx.ai empowers businesses to\nharness the full potential of fine-tuning, which unlocks new levels of precision, efficiency, and\ninnovation in AI-driven solutions.\nwatsonx.ai provides the following features:",
    "watsonx.ai provides the following features:\n(cid:2) Hardware and resource allocation automation: The platform’s automation begins with its\nability to manage hardware and resource allocation seamlessly. Users do not need to",
    "worry about provisioning servers, configuring GPUs, or scaling their setups to\naccommodate large datasets or models. Instead, watsonx.ai handles these tasks behind\nthe scenes, helping ensure that every fine-tuning operation runs on optimized\nconfigurations.",
    "configurations.\n(cid:2) Supervised Fine-Tuning Trainer (SFTTrainer): At the heart of watsonx.ai fine-tuning\ncapabilities is the SFTTrainer, which is a powerful tool that is developed in collaboration\nwith Hugging Face. This framework simplifies the optimization of model weights by",
    "automating key aspects of the training process, which includes the application of\nadvanced learning rate schedules and warm-up strategies. These techniques are crucial\nfor maintaining stability during training, particularly when dealing with complex or",
    "high-dimensional data. By leveraging SFTTrainer, watsonx.ai helps ensure that models\nconverge rapidly and reliably without the need for extensive manual intervention.\n(cid:2) Low-rank adaptation (LoRA) and quantized low-rank adaptation (QLoRA): In addition to",
    "SFTTrainer, watsonx.ai incorporates cutting-edge techniques like LoRA andQLoRA.\nThese methods represent a paradigm shift in fine-tuning efficiency. Rather than retraining\nall of a model’s parameters, LoRA focuses on fine-tuning small modular blocks of weights",
    "while freezing most the model. This approach reduces the computational and memory\nrequirements of the process, which makes fine-tuning accessible even on\nresource-constrained hardware. QLoRA goes a step further by lowering the precision of",
    "certain parameters during training, which further optimizes performance without\ncompromising accuracy. These innovations enable watsonx.ai to deliver results faster and\nwith fewer resources than traditional approaches.",
    "with fewer resources than traditional approaches.\n(cid:2) Tuning Studio integration: Another key advantage of watsonx.ai is its integration with the\nTuning Studio, which provides access to a library of pre-configured model templates.",
    "These templates enable users to build on pre-existing architectures that are optimized for\nspecific tasks or domains. This approach eliminates the need to design custom models\nfrom scratch, which reduces the time and expertise that are required to initiate fine-tuning\nprojects.",
    "projects.\nChapter 5. Advanced capabilities of watsonx.ai 65",
    "(cid:2) Custom FMs: For organizations with unique requirements, watsonx.ai also supports the\nimport of custom FMs if they have fewer than 20 billion parameters. This approach opens\nthe possibility of automatically fine-tuning tons of available models on Hugging Face and",
    "on watsonx.ai. This flexibility helps ensure that the platform can accommodate a wide\nrange of use cases and industries.\n(cid:2) Monitoring and optimization: Throughout the fine-tuning process, watsonx.ai provides\nrobust tools for monitoring and optimization. Real-time performance tracking enables",
    "users to assess key metrics, such as validation loss and gradient stability, which helps\nensure that the model improves as expected. The platform also employs advanced\nearly-stopping mechanisms to prevent overfitting by halting training when further iterations",
    "would yield diminishing returns. These features enhance the efficiency of the process and\nimprove the quality of the final model.\nFigure5-5 shows the watsonx.ai fine-tuning process.\nFigure 5-5 Prompt fine-tuning pipeline process overview",
    "In conclusion, watsonx.ai revolutionizes the traditionally complex and resource-intensive\nprocess of fine-tuning LLMs by introducing a seamlessly automated, highly efficient, and\nscalable solution. By leveraging advanced tools like the SFTTrainer, innovative techniques",
    "such as LoRA and QLoRA, and a robust Tuning Studio, watsonx.ai enables businesses to\nachieve unparalleled levels of customization and precision in their AI solutions. Its ability to\nmanage every aspect of the fine-tuning lifecycle (from data preparation and parameter",
    "optimization to resource allocation and model monitoring) removes significant technical\nbarriers, which democratize access to AI specialization for organizations of all sizes.\nThis comprehensive platform empowers businesses to tailor FMs to their unique",
    "domain-specific needs, whether in healthcare, finance, legal services, or other fields that\nrequire high precision. By doing so, the platform enhances the relevance and accuracy of AI\napplications and accelerates time-to-value while reducing the costs that are associated with",
    "traditional fine-tuning methods. watsonx.ai stands as a testament to IBM's commitment to\ninnovation and accessibility by providing a robust foundation for businesses to unlock the\ntransformative potential of AI with confidence and ease.",
    "66 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "5.4 InstructLab\nInstructLab represents a groundbreaking shift in the way LLMs are fine-tuned by making the\nprocess more accessible, flexible, and efficient. At its core, InstructLab leverages a unique\ncombination of community-driven input, synthetic data generation (SDG), and iterative",
    "training methodologies to refine LLMs in a way that dramatically lowers the barriers to entry\nfor fine-tuning tasks (see Figure5-6). This process makes it simpler for developers and\nsubject matter experts (SMEs) to improve model outputs. It also accelerates the fine-tuning",
    "cycle and reduces the computational overhead that is traditionally associated with\ncustomizing models.\nFigure 5-6 The InstructLab large language model development kit functions overview\nThe InstructLab approach to fine-tuning is heavily grounded in the concept of",
    "taxonomy-driven knowledge curation. Taxonomies, in this context, are structured frameworks\nof concepts and relationships that organize information into logical categories and\nsubcategories. These taxonomies are built collaboratively by SMEs, and they serve as the",
    "foundation for the knowledge that is used to tune the model. For example, if a business\nwanted to fine-tune an LLM on customer support for a specific industry, an SME in that\nindustry would work with a taxonomy that represents common questions, issues, and",
    "terminology that are relevant to the field. By formalizing domain knowledge in a taxonomy,\nInstructLab helps ensure that the model fine-tuning process is precise, efficient, and\ncontextually relevant.\nWhat makes this approach powerful is that the knowledge that is curated in these taxonomies",
    "is used to generate synthetic data. Unlike traditional fine-tuning methods that rely heavily on\nvast amounts of manually labeled training data, InstructLab uses SDG to produce the training\nexamples that are needed to adjust model behavior. This task is accomplished by feeding the",
    "curated taxonomies into the system (composed of Knowledge and Skills taxonomies), which\ncontains question-answer pairs and other types of data.\nChapter 5. Advanced capabilities of watsonx.ai 67",
    "An example of Skills taxonomies is provided in Figure5-7, where the Skill that is defined here\nis made to make a model learn to better interpret particular tables.\nFigure 5-7 Skill Taxonomy example\nThis data generation process is not random: It follows predefined patterns based on the",
    "taxonomy’s structure, which helps ensure that the synthetic examples are highly relevant to\nthe domain. This synthetic data serves as a proxy for real-world data, which enables\nInstructLab to adapt to niche topics or specialized knowledge areas without requiring the",
    "collection of large-scale, expensive datasets. The synthetic data can also be customized and\ncontrolled by the SME, which means that they can influence the generation process to help\nensure that the model is trained on the most important or critical examples. The ability to",
    "generate synthetic training data directly from taxonomies is what makes InstructLab so\nefficient: It drastically reduces the need for large-scale manual data curation and opens.\nFurthermore, it solves the ever-existing problem of not having enough data for fine-tuning a",
    "model that is tailored for a particular business need in the generative AI (gen AI) era.\nThe InstructLab approach is built around iterative feedback and instruction training, which are\ntwo techniques that further streamline the fine-tuning process. Instruction training involves",
    "providing LLMs with explicit instructions (similar to how humans learn new tasks) about how\nto generate responses based on the synthetic data. This process is highly flexible because\nthe SMEs can continually tweak the instructions and the knowledge base based on the",
    "evolving needs of the model and the domain it is being trained on. This process uses a\n2-phase approach with a replay that serves the purpose of ensuring high diversity and quality\nin the synthetically generated instruction-tuning dataset while ensuring training stability. It",
    "also prevents catastrophic forgetting, which is a common situation that happens in the FM\nfine-tuning process when it is not well controlled.\nOnce the initial synthetic data is generated from the curated taxonomy, the InstructLab",
    "system trains the model by providing it with a series of questions and answers that align with\nthe knowledge base. The process is iterative, which means that the model does not undergo\na single round of training and then stop. Instead, as new feedback is gathered from the",
    "model’s performance, the data is refined, and further training is conducted. This iterative\nfeedback loop is crucial in guiding the model toward better understanding, more accurate\noutputs, and more aligned responses to specific use cases.",
    "68 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "The power of iterative feedback lies in its ability to refine the model's responses over time,\nwhich enhances its accuracy and applicability to real-world problems. SMEs can continuously\nassess the model’s performance in relation to specific tasks or topics, and helps ensure that",
    "the model becomes progressively better at understanding the subtleties of the domain and\ngenerating more precise, contextually relevant responses.\nAnother key feature of InstructLab is its model-neutral and open-source nature. Unlike",
    "proprietary fine-tuning solutions that are often tightly coupled with specific models or\nplatforms, InstructLab enables users to contribute to the fine-tuning of various LLMs\nregardless of their underlying architecture. InstructLab is built on the premise that fine-tuning",
    "should be an open, community-driven process. It supports a wide range of open-source LLMs\nfrom repositories like Hugging Face, which enables users to choose the model that best suits\ntheir needs, and even enabling them to experiment with models from different frameworks.",
    "The open-source nature of InstructLab also means that users have full transparency into the\nfine-tuning process and the ability to modify it as needed. Anyone from hobbyists to industry\nexperts can contribute to improving the system, whether by adding new taxonomies,",
    "adjusting training data, or developing new techniques for SDG. This approach makes\nInstructLab a true community-driven initiative that is always evolving based on the needs and\ncontributions of its users.\nFigure5-8 shows a community-driven InstructLab fine-tuning process example.",
    "Figure 5-8 Community-driven InstructLab fine-tuning process example\nUsing InstructLab involves several key steps, each of which is designed to make the process\nas efficient and accessible as possible:\n1. Users download a base model from a supported repository, such as Hugging Face, and",
    "initialize the InstructLab command-line interface (CLI).\n2. Once the environment is set up, the user creates a knowledge base, which is stored in a\nstructured directory that follows the taxonomy format. This knowledge base is populated",
    "with question-answer pairs, references to external documents (for example, PDFs or\nmarkdown files), and metadata to describe the knowledge, such as the domain and\nrelevant attributes.\n3. Now, you generate synthetic data based on the knowledge base. This synthetic data is",
    "used to train the model, with the InstructLab system automatically generating examples\nthat adhere to the structure of the taxonomy. The SDG process is designed to produce\nhigh-quality training samples by following the patterns and relationships that are defined in",
    "the taxonomy, which reduces the time and cost of manual data creation and helps ensure\nthat the model receives high-quality, domain-specific examples.\nChapter 5. Advanced capabilities of watsonx.ai 69",
    "4. Once the synthetic data is generated, the model is trained by using the InstructLab training\nframework. This training can be done on a local machine or in a cloud environment, with\nthe flexibility to use GPUs to accelerate the process.",
    "5. After training is complete, the model undergoes a testing phase to ensure that it performs\nas expected. InstructLab enables users to run tests that evaluate how well the model\nanswers questions and generates responses, which help identify areas for further\nimprovement.",
    "improvement.\n6. The final stage is deploying the trained model, which can be done through the InstructLab\nserving tools. Once the model is deployed, users can interact with it through the CLI or\nthrough an application interface, which enables them to assess how well it handles",
    "real-world queries and performs in live environments.\nFigure5-9 shows a high-level view of the InstructLab pipeline.\nFigure 5-9 High-level overview of the InstructLab pipeline\n5.4.1 Advantages of InstructLab\nThe InstructLab methodology provides numerous advantages over traditional fine-tuning",
    "techniques:\n(cid:2) Synthetic data generation (SDG): Facing and solving an older problem of data availability,\nInstructLab offers within its automatic capabilities a process to enhance, in cardinality and\nvariability, a training dataset for LLM fine-tuning by using SDG, which is integrated into its",
    "core.\n(cid:2) Cost and complexity reduction: One of the most significant benefits of InstructLab is its\nability to drastically reduce the cost and complexity of fine-tuning. By relying on SDG\nrather than manually labeled datasets, InstructLab eliminates one of the most",
    "time-consuming and expensive aspects of model customization.\n(cid:2) Agile development: The iterative feedback loops and the ability to work collaboratively with\nSMEs enable the fine-tuning process to be far more agile, with models refined and\nimproved continuously based on real-time insights.",
    "70 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "(cid:2) Flexible model selection and customization: InstructLab is model-neutral, which means\nthat users can select and fine-tune various LLMs based on their specific needs. Whether it\nis a general-purpose model like Granite or a more specialized compatible model that is",
    "found on Hugging Face for a particular domain, InstructLab enables users to adapt and\nimprove the model that best fits their use case.\n(cid:2) Broader audience participation: The open-source nature and simplicity of the workflow in",
    "InstructLab make it possible for people without deep machine learning (ML) expertise to\nparticipate in model development and fine-tuning. This approach is a significant step\ntoward democratizing AI and ensuring that more organizations, regardless of size or",
    "expertise, can harness the power of LLMs for their specific needs.\nThe InstructLab innovative approach to model fine-tuning (leveraging taxonomy-driven\nknowledge curation, SDG, and iterative training) marks a transformative shift in how LLMs",
    "can be customized and applied across a wide range of domains. By reducing the barriers to\nentry, lowering computational costs, and enabling highly specialized, domain-specific training,\nInstructLab is positioning itself as a key enabler of accessible, efficient, and scalable AI",
    "development. This open-source, community-driven initiative is paving the way for a new\ngeneration of AI practitioners to fine-tune and enhance LLMs without requiring extensive\ntechnical expertise, which expands the scope and impact of AI in real-world applications.\n5.4.2 How to use InstructLab",
    "5.4.2 How to use InstructLab\nInstructLab is a model-neutral, open-source AI project that facilitates contributions to LLMs. It\nis a new community-based approach to build truly open-source LLMs. InstructLab uses a",
    "synthetic-data-based alignment tuning method to train LLMs. The InstructLab tuning method\nis driven by manually created taxonomies. InstructLab provides a process for optimizing and\ntuning LLMs by collecting knowledge and skills as part of a taxonomy tree.",
    "To start the InstructLab process, ilab must be installed. You can download it from its official\nrepository.\nilab is a CLI tool that you can use to perform the following actions:\n(cid:2) Download a pre-trained LLM.\n(cid:2) Chat with the LLM.",
    "(cid:2) Chat with the LLM.\n(cid:2) Add new knowledge and skills to the pre-trained LLM by adding information to the\ncompanion taxonomy repository.\nAfter you add knowledge and skills to the taxonomy, you can perform the following actions:",
    "(cid:2) Use ilab to generate new synthetic training data based on the changes in your local\ntaxonomy repository.\n(cid:2) Retrain the LLM with the new training data.\n(cid:2) Chat with the retrained LLM to see the results.\nChapter 5. Advanced capabilities of watsonx.ai 71",
    "Figure5-10 shows the ilab flow of commands, which show how to start processing data for\nsynthetic data generation and the fine-tuning process.\nFigure 5-10 The ilab flow of commands\nBefore you begin working with ilab, ensure that your system meets the following\nrequirements:",
    "requirements:\n(cid:2) Operating system: Linux (tested on Fedora), or macOS with Apple M1/M2/M3 chipsets.\n(cid:2) Disk space: Minimum of 250 GB. 500 GB is recommended for complete workflows.\n(cid:2) Python Version 3.10 or 3.11. At the time of writing, Python 3.12+ is unsupported due to",
    "dependency constraints.\n(cid:2) C++ compiler: Ensure that a modern GCC version or equivalent is installed.\nIf you use Python environment management tools, ensure that they build libraries that are\nimplemented in C by including flags during Python compilation. For example, when using",
    "pyenv, you use the following string:\nPYTHON_CONFIGURE_OPTS=\"--enable-framework\" pyenv install 3.11.5\n72 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "To install the required tools, run the following command:\nsudo dnf install gcc gcc-c++ make git python3.11 python3.11-devel\nilab can be installed with various configurations, depending on your hardware and preferred",
    "accelerators. Different hardware setups often require specific steps to optimize performance\nand ensure compatibility with the chosen accelerators, such as Apple Metal, AMD ROCm, or\nNVIDIA CUDA.\nIf a GPU is available, you can leverage more processing power by using the following",
    "commands for initialization:\npython3.11 -m venv --upgrade-deps venv\nsource venv/bin/activate\npip cache remove llama_cpp_python\nCMAKE_ARGS=\"-DGGML_CUDA=on -DGGML_NATIVE=off\" pip install 'instructlab[cuda]'\npip install vllm@git+https://github.com/opendatahub-io/vllm@v0.6.2",
    "After you install ilab, proceed with the first initialization by running the following command:\nIlab config init\nDuring initialization, ilab prompts you to perform specific tasks that influence how the\nenvironment is configured. By following these prompts, you can tailor ilab to meet your",
    "needs by setting up essential components like the taxonomy repository, model paths, and\ntraining profiles. For example, selecting a training profile helps ensure compatibility with your\nhardware, whether it uses CPUs or GPUs, to provide the best performance and resource\noptimization for your setup:",
    "optimization for your setup:\n1. Clone the taxonomy repository, either interactively or by specifying a path with the\n--taxonomy-path flag.\n2. Specify the path to your model. By default, it uses a quantized Granite model.",
    "3. Select a training profile. For systems without dedicated GPUs, choose No Profile (CPU,\nApple Metal, AMD ROCm).\nAfter initialization, the directories that are shown in Table5-1 are created.\nTable 5-1 ilab directory overview and details\nDirectory Description",
    "Directory Description\n~/.cache/instructlab/models/ Contains downloaded models.\n~/.local/share/instructlab/datasets/ Stores the dataset outputs that are\ngenerated during workflows.\n~/.local/share/instructlab/taxonomy/ Contains skill and knowledge data from the\ntaxonomy repository.",
    "taxonomy repository.\n~/.local/share/instructlab/checkpoint Contains model checkpoints from the\ns/ training process.\n~/.config/instructlab/config.yaml The configuration file that is generated\nduring initialization.\nChapter 5. Advanced capabilities of watsonx.ai 73",
    "--- Table 1 from page 89 ---\nDirectory | Description\n~/.cache/instructlab/models/ | Contains downloaded models.\n~/.local/share/instructlab/datasets/ | Stores the dataset outputs that are\ngenerated during workflows.\n~/.local/share/instructlab/taxonomy/ | Contains skill and knowledge data from the",
    "taxonomy repository.\n~/.local/share/instructlab/checkpoint\ns/ | Contains model checkpoints from the\ntraining process.\n~/.config/instructlab/config.yaml | The configuration file that is generated\nduring initialization.",
    "After you install the InstructLab CLI on your system, start by downloading the base model that\nyou want to train. The foundation of using InstructLab effectively is access to its models. The\nilab CLI simplifies this process by offering robust integration with repositories like Hugging",
    "Face or OCI. It provides authentication mechanisms, such as token-based access for\nHugging Face, and features like repository specification and download acceleration. These\ncapabilities help ensure secure and efficient downloads of pre-trained models.",
    "To quickly get started, download compact pre-trained versions of the following models:\n(cid:2) granite-7b-lab-GGUF\n(cid:2) merlinite-7b-lab-GGUF\n(cid:2) Mistral-7B-Instruct-v0.2-GGUF\nTo initiate the download of a model, can run the following command:\nilab model download –repository <MODEL-ID>",
    "ilab model download –repository <MODEL-ID>\nWhen this command runs, the ilab CLI interacts with the designated repositories to fetch the\nselected models. By default, the models are stored locally in the\n~/.cache/instructlab/models/ directory, which helps ensure efficient reuse because the",
    "downloaded models do not need to be fetched again for future operations unless explicitly\nremoved or updated.\nYou can download a non-default LLM from Hugging Face. If a Hugging Face token is required,\ncan add it by running ilab model download –repository <MODEL-ID>, but add the token after",
    "the argument -hf-token.\nYou can use OCI-compliant repositories. To do so, log in to the registry and use the following\ncommand:\nilab model download -rp docker://<MODEL_ID> -rl latest\nOnce the models are downloaded, they can be served locally for inference. ilab supports",
    "serving both default and custom models if the system prerequisites are met. To serve models\nlocally, ensure that the system has sufficient hardware resources, which include at least 8 GB\nof RAM and, for GPU-accelerated serving, an NVIDIA GPU with CUDA support. If multiple",
    "ilab clients attempt to connect to the same server, the first client connects successfully, and\nthe others create temporary servers, which require more resources. To prevent conflicts,\nmanage the connections.\nTo serve the model, run the following command, which provides a URL for API interaction:",
    "Ilab model serve –model-path <MODEL_PATH>\nIt is possible to interact with a served model directly within ilab by using the following\ncommand, with optional personalization of inference parameters, such as temperature:\nIlab model chat –model <MODEL_PATH> [-temperature <VALUE>]",
    "Now, you can start personalizing the model by adding new skills and knowledge.\nTo train an open source model with InstructLab, create knowledge and skills in the taxonomy\ndirectory. When you initialized the ilab CLI, it automatically cloned the InstructLab taxonomy",
    "repository, which is the source of truth for your model training.\nIn the context of skill contributions, the required content is smaller in volume compared to\nknowledge contributions. A complete skill addition to the taxonomy tree can be represented",
    "by a few lines in a qna.yaml file (short for “questions and answers”) and an attribution.txt\nfile to cite sources.\n74 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "To make a valid skills contribution, the pull request must include a qna.yaml file with key-value\nentries that contain at least five question-and-answer pairs and an attribution.txt file that\nlists the sources that are used. The taxonomy structure serves multiple purposes: selecting",
    "the relevant subset for data generation, ensuring interpretability for contributors and\nmaintainers, and forming part of the prompt for the LLM when generating synthetic samples.\nEach qna.yaml file must adhere to a standard structure with specific keys:\n(cid:2) version: Must be set to 2 (required).",
    "(cid:2) version: Must be set to 2 (required).\n(cid:2) task_description: A description of the skill (required).\n(cid:2) created_by: The GitHub username of the contributor (required).\n(cid:2) seed_examples: A collection of key-value entries with at least five examples (required for",
    "new files, although older files may contain fewer examples).\n(cid:2) context: Provides relevant information for grounded skills, which guide the model’s\nprocessing (not used for ungrounded skills).\n(cid:2) question: The model's input query (required).",
    "(cid:2) answer: The expected response (required).\nThe taxonomy tree also categorizes skills as either grounded (requiring context) or\nungrounded (not requiring context). For example, a grounded skill might be\ngrounded/linguistics/grammar, while an ungrounded skill might be",
    "linguistics/writing/poetry/haiku. The qna.yaml file is always in the final node of the taxonomy\npath. Importantly, there is a limit on the content length in question-answer pairs to ensure\nmodel compatibility; contributions should not exceed approximately 2,300 words for these",
    "pairs. By adhering to these guidelines, contributors can maintain consistency and utility within\nthe skill taxonomy framework.\nTo make the qna.yaml files faster for humans to read, it is best practice to specify version",
    "first, which is followed by task_description, then created_by, and finally seed_examples. In\nseed_examples, it is a best practice to specify context first (if applicable), followed by question\nand answer.\nExample5-1 shows an example of a qna.yaml file.\nExample 5-1 A qna.yaml file\nversion: 2",
    "Example 5-1 A qna.yaml file\nversion: 2\ntask_description: <string>\ncreated_by: <string>\nseed_examples:\n- question: <string>\nanswer: |\n<multi-line string>\n- context: |\n<multi-line string>\nquestion: <string>\nanswer: |\n<multi-line string>\n...",
    "answer: |\n<multi-line string>\n...\nCreate an attribution.txt file that includes the sources of your information, which can be\nself-authored sources.\nChapter 5. Advanced capabilities of watsonx.ai 75",
    "Knowledge contributions differ from skills by focusing on answering factual, data-driven, or\nreference-based questions, which are often supported by documents like textbooks, technical\nmanuals, encyclopedias, journals, or magazines. Although knowledge and skills share",
    "similarities in their taxonomy structures, knowledge nodes include additional elements to\naccommodate their document-based nature.\nFor contributors that use InstructLab 0.21.0 or later, knowledge contributions can include PDF",
    "files as valid document types, but earlier versions accept only markdown formats. Each\nknowledge node in the taxonomy tree contains a qna.yaml file that is similar in structure to the\none that is used for skills, but with additional fields to support knowledge-specific attributes.",
    "Notably, all knowledge submissions must be in a Git repository, such as one hosted on\nGitHub, and the qna.yaml file must reference this repository:\n(cid:2) Submit the most current version of the document.\n(cid:2) Contributions must be text-based. Images are ignored.",
    "(cid:2) Avoid using tables in your markdown freeform contributions.\nThe qna.yaml file for knowledge contributions must follow a specific format and include the\nfollowing fields:\n(cid:2) version: The version of the qna.yaml file format, which is set to 3.",
    "(cid:2) created_by: The GitHub username of the contributor.\n(cid:2) domain: The category of the knowledge.\n(cid:2) seed_examples: A collection of key-value entries.\n(cid:2) context: A chunk of information from the knowledge document. Each qna.yaml file must",
    "include at least five context blocks, with a maximum of 500 words per block.\n(cid:2) questions_and_answers: Holds the questions and answers based on the context. Each\ncontext block requires a minimum of three question-and-answer pairs, each with a\nmaximum word count of 250 words.",
    "maximum word count of 250 words.\n– question: A question for the model.\n– answer: The corresponding answer.\n(cid:2) document_outline: An overview of the document being submitted.\n(cid:2) document: The source document for the knowledge contribution.",
    "(cid:2) repo: The URL of the repository that contains the knowledge files.\n(cid:2) commit: The SHA of the commit in the repository for the knowledge files.\n(cid:2) patterns: A list of glob patterns specifying the files in the repository. Patterns starting with",
    "*, such as *.md, must be quoted (\"*.md\") to comply with YAML rules.\nBy adhering to these guidelines, knowledge contributions maintain a structured, accessible\nformat that aligns with the taxonomy framework and supports efficient integration into the\nsystem.",
    "system.\n76 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "When working with YAML files (for both skills and knowledge), it is crucial to adhere to\nspecific formatting rules to help ensure correctness and avoid parser errors. Indentation and\nspacing play a role, and YAML requires two spaces for each level of indentation (tabs must not",
    "be used under any circumstances). Also, avoid trailing spaces at the end of lines because they\ncan lead to issues during processing. For entries in seed_examples, each example begins with\na - placed before the first field, such as question or context. Subsequent keys within the",
    "same example should not include the -. Pay attention to special characters like \" and ', which\nmust be escaped by using a backslash (\\). To simplify handling these characters, YAML\nenables the use of the | character at the start of a value, which disables special character",
    "interpretation and supports multi-line strings. For example, lines that start with | are followed\nby an indented block that contains the string's content. To avoid unexpected YAML parser\nbehavior, it is a best practice to quote all values by using double quotation marks (\"). This",
    "approach prevents values such as Yes or No from being interpreted as Boolean types (True or\nFalse). For more information about managing multi-line strings and YAML nuances, see the\nyaml-multiline.info file.\nNow, after creating a YAML file for skills and knowledge, as shown in Figure5-7 on page68,",
    "you can validate your new data. Use the ilab taxonomy diff command to help ensure that\nilab is registering your new knowledge or skills and that your contributions are properly\nformatted. This command displays any new or modified YAML files within your taxonomy tree.",
    "You can also validate your entire taxonomy by performing a diff against an empty base by\nusing the --taxonomy-base=empty argument.\nAfter validation, it is possible to start the Synthetic Data Generation (SDG) pipeline. To",
    "generate a synthetic dataset based on newly added knowledge or skill sets in the taxonomy\nrepository, run the ilab data generate command. Before proceeding, ensure the existing\nmodel to which you are adding skills or knowledge is still running. Alternatively, you can",
    "initiate the server by using the ilab data generate command by specifying a fully qualified\nmodel path with the --model flag. At the time of writing, the full CLI pipeline supports only\nMixtral and Mistral Instruct Family models as the teacher model. For the simple pipeline,",
    "Merlinite 7b Lab is the only supported teacher model due to the specific model prompt\ntemplates that it uses. There is a plan to expand compatibility in the future, and on watsonx.ai\n(as described in Chapter6, “Artificial intelligence agents” on page87).",
    "To start generation, run the following command:\nilab data generate [--pipeline full --gpus <NUM_OF_GPUS> --model <MODEL_PATH>\nOptionally, you can start SDG by using GPUs when they are available. You can specify the\nteacher model that is used (the default one for the ilab CLI is Merlinte-7B).",
    "After generation finished, the synthetic dataset consists of two files in the\n~/.local/share/instructlab/datasets directory:\n(cid:2) skills_train_msgs_*.jsonl\n(cid:2) knowledge_train_msgs_*.jsonl\nYou can run the generate step against a different model through a compatible API, such as",
    "the one that is created by the ilab model serve or any remote or locally hosted LLM (through\nollama, LM Studio, or others). Run the following command:\nilab data generate --endpoint-url http://localhost:8000/v1",
    "Now that the curated dataset for a fine-tuning is ready, the fine-tuning process can be started.\nChapter 5. Advanced capabilities of watsonx.ai 77",
    "The InstructLab model train has three pipelines: simple, full, and accelerated. The default\nis full.\n(cid:2) simple uses an SFTTrainer on Linux and MLX on MacOS. This type of training takes\nroughly an hour and produces the lowest fidelity model but should indicate whether your",
    "data is being picked up by the training process.\n(cid:2) full uses a custom training loop and data processing functions for the Granite family of\nmodels. This loop is optimized for CPU and MPS function. Use --pipeline=full with",
    "--device=cpu (Linux) or --device=mps (MacOS). You can also use --device=cpu on a\nMacOS machine. However, MPS is optimized for better performance on these systems.\n(cid:2) accelerated uses the instructlab-training library, which supports GPU-accelerated",
    "and distributed training. The full loop and data processing functions are either pulled\ndirectly from or based on of the work in this library.\nTo limit training time, you can adjust the num_epoch parameter in the config.yaml file. The",
    "maximum number of epochs for running the InstructLab end-to-end workkflow is 10.\nThe following command shows how to start the automatic fine-tuning process with the\npreviously generated dataset. Furthermore, it can specify more than the pipeline, such as the",
    "device that you want the model to be trained on (CPU, MPS, or GPU).\nIlab model train [--pipeline <PIPE_ID> --device <DEVICE_ID> --data-path\n<DATA_PATH>]\nThis training step can potentially take from several minutes to several hours to complete,\nwhich depends on the available computing resources.",
    "After the fine-tuning pipeline completes, it is possible to verify the quality of the new model\nand whether the generated dataset with the defined skills and knowledge produced good\nresults. To thoroughly test and evaluate a newly trained model by using InstructLab, first run a",
    "series of commands that are designed to assess the performance and accuracy of the model\nafter training. The testing process involves using the ilab model test command to obtain\noutput from the model before and after the training process. With this output, you can see how",
    "well the model performs based on its previous state and after it has undergone the\nenhancements from the training process. The results from this test show the effectiveness of\nyour training and provide insight into areas where further improvement might be needed.",
    "When the model is tested, you can use the ilab model evaluate command to run the model\nthrough a set of predefined benchmarks to evaluate its performance across various\ncategories. At the time of writing, there are four primary benchmarks that are supported by\nInstructLab:",
    "InstructLab:\n(cid:2) Multitask Language Understanding (MMLU)\n(cid:2) MMLUBranch\n(cid:2) MTBench\n(cid:2) MTBenchBranch.\nThese benchmarks assess different aspects of a model's capabilities, such as its knowledge\nand skills:",
    "and skills:\n(cid:2) MMLU evaluates the model’s general knowledge on a wide range of topics.\n(cid:2) MMLUBranch compares the model’s performance to the performance of a base model to\nidentify improvements in knowledge.",
    "identify improvements in knowledge.\n(cid:2) MTBench evaluates how well the model applies its knowledge in multi-turn conversations.\n(cid:2) MTBenchBranch assesses improvements or regressions in specific skill areas when\ncompared to a base model.",
    "compared to a base model.\n78 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "For each benchmark, the evaluation generates detailed reports, which show scores and\nidentify areas where the model performs well and areas that need further work. For example,\nthe MMLU report provides a score for various subjects, such as abstract algebra, anatomy,",
    "and business ethics, which indicate how the model performs in each area. A typical output for\nMMLU looks like a series of subject categories with a score 0.0 - 1.0, with higher scores\nindicating better performance in the respective topics.",
    "Running MMLUBranch involves evaluating your model's contributions compared to a base\nmodel. The evaluation outputs a score for both the base model and the newly trained model,\nalong with a report on the improvements or regressions that are observed. For example, you",
    "might see that the model improved in one area, like “tonsils,” from a score of 0.74 to 0.78,\nwhich indicates that your training enhanced the model’s ability in that particular knowledge\ndomain.\nMTBench and MTBenchBranch follow a similar structure, but they focus on testing the",
    "model’s skills rather than just knowledge. MTBench evaluates the model's ability to perform in\nmulti-turn dialogs, which provide a score for each turn in the conversation, such as turn one\nand turn two. MTBenchBranch compares your model’s skill performance to a base model,",
    "which provides a detailed breakdown of areas where your model improved or regressed, and\nhighlighting any skills that showed no significant change. This detailed feedback helps\npinpoint specific areas where more fine-tuning might be necessary to enhance the model's",
    "abilities. For each benchmark, it is important to help ensure that the model that is evaluated is\nin a supported format, either safetensors or GGUF. Using models directly from Hugging Face\nwithout downloading them is not supported.",
    "without downloading them is not supported.\nAlso, while using models for MMLU and MMLUBranch evaluations, GGUF models are not\nsupported at the time of writing. When running MTBench and MTBenchBranch, it is a best\npractice to use the Prometheus-8x7b-v2.0 model as the judge model, but you can use a",
    "different judge model. You can download the Prometheus model by running the ilab model\ndownload command for local use in these evaluations.\nThe entire process of running these evaluations can take from several minutes to several",
    "hours, which depend on the size of the model and the dataset that is used. Be prepared to\nallocate enough time for the evaluations to complete, especially when working with large\ndatasets or multiple training epochs. The results from these evaluations provide a",
    "comprehensive view of the model's strengths and weaknesses, which offer valuable insights\nto guide further refinement and optimization of the model.\nAfter the process ends and the results are good enough for the specified use case, a new",
    "fine-tuned model with new synthetic data in GGUF format is available in the ilab specified\nfolder location in a GGUF format. Apart from the usage on ilab, it is possible to deploy it on\nhyperscalers such as watsonx.ai run time by using the Bring Your Own Model (BYOM)",
    "function, which fully enables a true, at-scale, enterprise-level fine-tuning process of FMs.\n5.4.3 InstructLab on watsonx.ai Software-as-a-Service\nAt the time of writing, InstructLab has demonstrated its usability primarily through the ilab",
    "CLI. This method enables users to interact with InstructLab features and functions in a\nstraightforward and efficient manner. However, the development team is working on\nintegrating InstructLab with watsonx.ai. This upcoming integration will enhance the user",
    "experience by providing a dedicated user interface (UI). This UI will streamline the entire\nprocess, which will make it more accessible and intuitive for users who might not be\ncomfortable with CLI operations. This development is expected to open new possibilities for a",
    "broader range of user by facilitating simpler access to powerful InstructLab tools and features.\nChapter 5. Advanced capabilities of watsonx.ai 79",
    "Figure5-11 shows the interface for tuning a generative LLM by using InstructLab. This\ninterface is designed to provide users with an intuitive platform for refining language models\nto meet specific needs and requirements. It also illustrates the starting point of the overall",
    "process: the addition of skills and knowledge.\nFigure 5-11 Tuning Studio interface\nUsers begin by selecting the FM, and then proceed to integrate various skills and knowledge\nareas into the model. The interface guides users through detailed steps, which include",
    "specifying configurations. Throughout this process, users can monitor their progress and\nmake necessary adjustments to ensure that the model's performance aligns with outcomes.\nMoreover, InstructLab offers advanced features such as feedback, performance metrics, and",
    "troubleshooting tools to enhance the tuning experience. This comprehensive approach helps\ncreate an accurate and efficient LLM, with continuous improvement and adaptation to\nevolving linguistic patterns and user needs.",
    "evolving linguistic patterns and user needs.\nFigure5-12 on page81 illustrates a detailed taxonomy tree of skill and knowledge files that\nare managed within Tuning Studio on watsonx.ai for InstructLab. The UI shows a",
    "comprehensive and organized view of the hierarchical structure of skills and knowledge\nareas. Each node in the taxonomy tree represents a distinct category, which facilitates\nnavigation and access to specific training data.\n80 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Figure 5-12 Taxonomy tree of skill and knowledge files that are managed in Tuning Studio on\nwatsonx.ai for InstructLab\nOne of the key functions of this UI is its ability to visualize and maintain version history for",
    "each training run, which includes the trained models, the associated skills and knowledge\ntaxonomy, grounding data, and the generated synthetic data. By tracking the evolution of\nthese elements over time, users can effectively monitor the progression and improvements",
    "that are made with each iteration. This version control mechanism is essential for helping\nensure the reproducibility and reliability of model training processes. Also, it will be possible to\nadd knowledge and ingest multiple data formats, such as PDFs, docx, HTML, MD, and more.",
    "The Tuning Studio capability to handle such a vast array of data types and their versions\nenables meticulous fine-tuning and enhances the overall model development lifecycle.\nThrough this meticulous documentation and management, users can draw insights from past",
    "training runs, identify best practices, and apply learned lessons to future projects.\nChapter 5. Advanced capabilities of watsonx.ai 81",
    "Figure5-13 shows the progress map during the tuning phase of InstructLab on watsonx.ai.\nFigure 5-13 Progress map during the tuning phase of InstructLab on watsonx.ai\nBefore commencing the SDG and fine-tuning process, conduct a peer review of the submitted",
    "skills and knowledge files. This review involves project members and SMEs to help ensure\ncomprehensive assessment and validation. If any modifications are identified as necessary\nduring this review phase, new branches can be created within the version control system to",
    "incorporate these changes without disrupting the main development line.\nAfter you incorporate the feedback and necessary adjustments, the fine-tuning process\ncommences. This step involves leveraging pre-trained models and adjusting them to specific",
    "requirements by training on customized datasets. The goal is to enhance the model's\nperformance in targeted areas to help ensure that it meets the specifications and accuracy\nlevels.\nOn successful fine-tuning, the newly optimized model can be exported in the GGUF format,",
    "which is a versatile and widely supported format for deploying quantized FMs. Alternatively,\nthe model can be directly deployed onto the watsonx.ai run time environment. This\ndeployment establishes a seamless large language model operations (LLMOps) pipeline",
    "within watsonx.ai for InstructLab, which enables automated monitoring, maintenance, and\niterative improvement of the model.\nBy integrating these processes within the watsonx.ai ecosystem, you help ensure continuous",
    "delivery and operational efficiency of AI solutions that align with best practices in modern ML\nworkflows.\nThe fine-tuning process that is shown in Figure5-14 on page83 shows the intricate and\nmethodical approach that is adopted by InstructLab within the watsonx.ai framework. This",
    "process is characterized by a 2-phase, fine-tuning methodology that is augmented with a\nreplay buffer, which helps ensure enhanced performance and accuracy of the models.\n82 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Figure 5-14 InstructLab fine-tuning process with real-time performance on the 2-phase fine-tuning\nprocess with replay buffer\nIn the initial phase, the pre-trained models undergo a rigorous training regimen by using",
    "targeted datasets that are pertinent to the specific knowledge areas that are identified within\nthe taxonomy tree. This phase focuses on adapting the general-purpose models to the\nspecialized requirements of the InstructLab projects, which hone their performance to meet",
    "the specifications. The second phase uses the skills and continues catastrophic forgetting by\nusing a dedicated replay buffer while fine-tuning the skills in the second phase.\nThe replay buffer mechanism plays a crucial role in this fine-tuning process. By systematically",
    "storing and replaying past experiences (training data and model states) during the training\nsessions, the buffer helps ensure that the models continuously learn and adapt from the\nprevious run. This approach mitigates catastrophic forgetting and reinforces learning from",
    "high-value data points, which improves the models' robustness and generalization\ncapabilities. Figure5-14 encapsulates a holistic and dynamic approach to model fine-tuning\nwithin the watsonx.ai ecosystem, which highlights the interplay of advanced methodologies",
    "and real-time performance monitoring to achieve superior gen AI solutions.\nInstructLab will be available on watonx.ai as Software-as-a-Service (SaaS), and it will be an\nimportant addition for gen AI on an enterprise level. It will enable a pool of resources to\nfine-tune and improve generative LLMs.",
    "fine-tune and improve generative LLMs.\nChapter 5. Advanced capabilities of watsonx.ai 83",
    "5.4.4 InstructLab use case examples\nIntroducing InstructLab on watsonx.ai heralds a new era of advancements in gen AI,\nparticularly in the realm of enterprise-level applications. This innovative platform integrates\nseamlessly within the watsonx.ai ecosystem, which enables a dynamic and robust",
    "environment for the fine-tuning and deployment of LLMs. As an SaaS offering, InstructLab\nprovides an unparalleled suite of tools and methodologies to optimize model performance\nthrough a meticulously structured 2-phase, fine-tuning process.",
    "The InstructLab capabilities are impactful across many use cases, and they empower\norganizations to tailor AI solutions to their unique requirements. By leveraging the advanced\nfeatures of InstructLab, users can achieve superior accuracy, efficiency, and scalability in their\nAI-driven initiatives.",
    "AI-driven initiatives.\nHere are some of the first prominent examples of use cases where InstructLab has\ndemonstrated significant improvements in its early life:\n(cid:2) Emergency medical services use case: A large hospital is looking to automate the",
    "processing of emergency medical records to improve the efficiency and accuracy of\ncritical tasks. The system should be capable of completing the following tasks:\n– Case classification: Assigning priority levels such as green (low urgency), orange",
    "(medium urgency), or red (high urgency) flags to cases based on their severity.\n– Peer medical review recommendations: Identifying cases that might require further\nreview by a medical peer to help ensure proper oversight and decision-making.",
    "– Clinical compliance and guideline deviation: Detecting discrepancies in medical reports\ncompared to established clinical guidelines, which enhance compliance and quality\nassurance.\n– Knowledge: The system will rely on hospital compliance data, which includes clinical",
    "guidelines and historical patient records to perform its tasks.\n– Skill requirements:\n(cid:129) Case classification to prio ritize emergency scenarios.\n(cid:129) Identifying deviations from clinical guide lines to help ensure regulatory compliance.",
    "(cid:2) Call transcript processing use case: A large North American telecommunications\ncompany requires an automated system to process and summarize incoming customer\nsupport call transcripts. The system must extract and organize information based on a",
    "predefined set of 80 questions. Examples include “Did the customer want to upgrade their\nplan?” or “Did the customer report bandwidth issues?”\n– Knowledge: The primary data source will be call transcripts, which contain varied\nhuman expressions and conversational styles.",
    "human expressions and conversational styles.\n– Skill requirements: Interpreting and analyzing natural language, which includes\nunderstanding diverse writing styles and linguistic nuances to extract relevant insights\nfrom conversations.",
    "from conversations.\n84 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "(cid:2) Personalized retail recommendations use case: A retailer aims to deploy a personalized\nrecommendation engine that suggests in-stock products that are tailored to a user’s\ndietary preferences, which include offering recommendations based on food allergies,",
    "nutritional goals, or ingredient restrictions.\n– Knowledge: The engine must use detailed product nutritional information and inventory\ndata to ensure accurate and timely recommendations.\n– Skill requirements:\n(cid:129) Classification of ingredients and their alignment with dietary preferences.",
    "(cid:129) Recommending products that match user profiles while considering inventory\navailability.\n– Agent capabilities:\n(cid:129) Understanding and analyzing inventory data in real time.\n(cid:129) Interpreting customer preferences and purchase history to generate meaningful\nsuggestions.",
    "suggestions.\n(cid:2) Intelligent auto claims processing use case: An insurance provider requires a solution to\nanalyze images of auto accidents and suggest insurance coverage recommendations that\nare based on the claimant's active policy. The system should improve the speed and",
    "accuracy of claims processing.\n– Knowledge: The system needs access to policy details, which include terms, coverage\nlimits, and exclusions.\n– Skill requirements:\n(cid:129) Classification of accident severity by analyzing damage in submitted images.",
    "(cid:129) Matching severity with appropriate coverage based on the policy.\n– Agent capabilities:\n(cid:129) Accessing and analyzing driver history to help ensure an accurate policy\napplication.\n(cid:129) Interpreting active insurance policies to provide recommendations that are aligned",
    "with coverage terms.\nChapter 5. Advanced capabilities of watsonx.ai 85",
    "86 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "6\nArtificial intelligence agents\nChapter 6.\nArtificial intelligence (AI) agents will soon become pivotal in modern computing by serving as\nautonomous entities that can perceive their environment, reason about their goals, and",
    "perform actions to achieve wanted outcomes. These agents will be central to many AI\nsystems, from personal assistants and autonomous vehicles to advanced simulations and\ndecision-making tools. The versatility and capability of AI agents arise from their ability to",
    "operate independently while adapting to dynamic and often unpredictable environments.\nThey combine advanced algorithms with AI and generative AI (gen AI) models, which enable\nthem to make intelligent decisions, adapt to changes, and optimize outcomes.",
    "This chapter delves into the intricacies of AI agents by exploring their fundamental\ncharacteristics, the motivation behind their development, and their applications across various\ndomains. Through a detailed exploration, readers will understand why AI agents represent a",
    "paradigm shift in how computational systems interact with and influence their surroundings.\nThe following topics are described in this chapter:\n(cid:2) 6.1, “What makes an AI agent” on page88\n(cid:2) 6.2, “Why AI agents are needed” on page94\n(cid:2) 6.3, “Multiple AI agents” on page95",
    "(cid:2) 6.3, “Multiple AI agents” on page95\n(cid:2) 6.4, “AI agents on watsonx.ai” on page100\n(cid:2) 6.5, “AI agents use case examples” on page107\n© Copyright IBM Corp. 2025. 87",
    "6.1 What makes an AI agent\nAn AI agent can be formally defined as a computational entity that is equipped with sensors to\nperceive its environment and use its actuators to interact with it. The core of an agent lies in",
    "its ability to reason and decide, which is driven by algorithms that enable it to analyze inputs,\npredict outcomes, and choose actions that are aligned with specific objectives. Unlike\ntraditional software systems, which operate on predefined instructions, AI agents exhibit",
    "autonomy and flexibility with dynamic execution flows, which enable them to handle complex\nscenarios with minimal human intervention. This autonomy stems from their design\nprinciples, which often draw from cognitive sciences, game theory, and control systems,",
    "enabling agents to emulate human-like decision-making processes. Furthermore, each AI\nagent can be designed for different levels of complexity and interaction. This adaptability\nmakes AI agents indispensable in fields such as robotics and natural language processing",
    "(NLP), where complex interactions with dynamic environments are required.\nFigure6-1 shows a schematic view of an AI agent.\nFigure 6-1 Schematic view of an AI agent\nAgents, in the context of AI, can be conceptualized as sophisticated orchestrators of",
    "intelligence that can address intricate and multi-faceted problems. They achieve these goals\nby leveraging the reasoning capabilities of large language models (LLMs), formulating\ndetailed plans to resolve challenges, and running these plans by using a diverse set of tools.",
    "An agent operates as a cohesive system, and its architecture can typically be broken down\ninto four key modules:\n(cid:2) The agent core\n(cid:2) Search and memory modules\n(cid:2) Planning modules\n(cid:2) Tools",
    "(cid:2) Planning modules\n(cid:2) Tools\nEach of these components plays a critical role in ensuring the agent's functions, adaptability,\nand effectiveness.\nThe agent core stands as the central coordination hub of the agent. It is often described as",
    "the “decision-making nucleus” due to its pivotal role in governing the agent's logic, behavior,\nand overall strategy. This module is responsible for synthesizing inputs, determining\nappropriate actions, and managing the interactions between other components.",
    "88 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "To design a robust and efficient agent core, you must define several foundation aspects that\nserve as the blueprint for the agent’s behavior:\n(cid:2) The general goals of the agent must be established. These goals act as the guiding",
    "principles that dictate the agent’s actions and responses, which help ensure that its\noperations align with the overarching objectives that it is designed to achieve. Without\nclear goals, the agent risks becoming unfocused or inefficient.",
    "(cid:2) Another critical aspect of the agent core is the explicit definition of the tools that are\navailable for execution, which involves creating a comprehensive “user manual” that\ncounts and describes all tools at the agent's disposal. Each tool’s capabilities, limitations,",
    "and specific use cases should be outlined to enable the agent to allocate resources\neffectively and run tasks with precision.\n(cid:2) Furthermore, the agent core must provide detailed guidance about the utilization of\nplanning modules. These modules are instrumental in enabling the agent to adapt",
    "dynamically to varying scenarios by selecting the most suitable planning strategy based\non the context. This adaptability is key to ensuring the agent’s effectiveness in complex\nand unpredictable environments.\nMemory integration is another cornerstone of the agent core. The memory system is",
    "designed to maintain and use relevant information from prior interactions or external\nresearch, which enhances the agent’s ability to generate accurate and context-aware\nresponses. This integration requires dynamic management of memory items to help ensure",
    "that only the most pertinent data is referenced during inference. Also, an optional persona\ndefinition can be incorporated into the agent core to influence the agent’s tone, preferences,\nand behavioral nuances. By defining a persona, the agent can be tailored to exhibit specific",
    "characteristics that align with its intended use case or audience, adding a layer of uniqueness\nto its interactions. Memory modules are integral to the agent’s ability to maintain contextual\nawareness and continuity. These modules are responsible for storing and managing",
    "information that supports the agent’s operations. Memory can be categorized into two main\ntypes:\n(cid:2) Short-term memory\nShort-term memory focuses on capturing the agent’s immediate actions, thoughts, and\nobservations during ongoing interactions, which include data that is retrieved from vector",
    "searches, outputs from API calls, and results from database queries. Short-term memory\nis essential for helping ensure that the agent can respond effectively to the immediate\ncontext of a user’s query.\n(cid:2) Long-term memory",
    "(cid:2) Long-term memory\nIn contrast, long-term memory serves as a repository for information that is accumulated\nover extended periods, which include summarized logs of prior interactions, personal\ndetails and preferences of the user, and other contextual information that might influence",
    "the agent’s behavior. Long-term memory enables the agent to maintain a consistent and\npersonalized approach in its interactions, which enhance user satisfaction and\nengagement. For example, in a conversational agent, long-term memory enables the",
    "retention of user preferences and past conversations, which create a more seamless and\nintuitive experience.\nWhen solving intricate problems, agents that are powered by LLMs are adept at navigating\ncomplexity by employing a combination of advanced methodologies that resembles planning",
    "an execution flow. One such methodology is task and question decomposition. This approach\ninvolves breaking down compound queries into smaller, more manageable sub-questions that\ncan be addressed sequentially.\nChapter 6. Artificial intelligence agents 89",
    "For example, to answer the question, “Will the temperature tomorrow be higher or lower than\nthe historical average?” the agent must decompose it into subquestions such as identifying\nthe location, determining the forecasted temperature for the specified location, and retrieving",
    "the historical average temperature for comparison. By addressing each sub-question\nindividually, the agent can construct a comprehensive and accurate response.\nAnother essential technique is the usage of reflection and critique frameworks. These",
    "methodologies, which include well-established prompting strategies such as ReAct,\nReflexion, Chain of Thought, and Graph of Thought, are designed to enhance the reasoning\ncapabilities of the agent. By incorporating elements of evidence-based reasoning and",
    "iterative self-critique, these frameworks enable the agent to refine its execution plans and\nimprove the quality of its responses. Reflection techniques enable the agent to evaluate the\nplausibility and coherence of its answers, which help ensures that its outputs meet high",
    "standards of reliability and relevance.\nClassification and the implementation of guardrails are extra mechanisms that enhance the\nagent’s decision-making process. By classifying questions and queries, the agent can filter",
    "search results, identify relevant sub-agents, or even deny a response if necessary. Guardrails,\nwhich serve as a specialized form of classification, act as safeguards to help ensure that the\nagent’s outputs adhere to predefined ethical and operational guidelines. These mechanisms",
    "are valuable in scenarios where precision, safety, and compliance are paramount.\nThe tools that are employed by an agent are another critical aspect of its function. These tools\nare executable workflows that enable the agent to perform specific tasks. Analogous to",
    "specialized third-party APIs, tools provide the agent with targeted capabilities that extend its\nproblem-solving abilities. Examples of tools include Retrieval-Augmented Generation (RAG)\npipelines for generating context-aware answers, code interpreters for handling complex",
    "programming tasks, and APIs for retrieving information from the internet. Also, utility APIs\nsuch as weather services or instant messaging platforms can be integrated to address\ndomain-specific needs. The versatility and effectiveness of an agent are enhanced by its",
    "ability to leverage these specialized tools.\nGoing one step forward, agents that rely solely on text inputs face inherent limitations in their\nability to interact with and analyze diverse data formats. Multi-modal agents address this",
    "limitation by incorporating the ability to process and reason over various input types, which\ninclude images, audio files, and structured datasets. This capability expands the range of\napplications for agents, which enable them to tackle tasks that require visual analysis, speech",
    "processing, or combined reasoning across multiple modalities. For example, a multi-modal\nagent can analyze an image to extract relevant features, process an accompanying audio file\nfor contextual information, and synthesize this data with text-based inputs to deliver a\ncomprehensive solution.",
    "comprehensive solution.\nBy following the main core modules of an AI agent and through thoughtful design and\ncontinuous refinement, agents are poised to become indispensable tools in the\never-expanding landscape of AI.",
    "ever-expanding landscape of AI.\nAn AI agent, as illustrated in Figure6-2 on page91, consists of a centralized planning system\nthat is supported by a general-purpose generative LLM. This LLM serves as the core\norchestrator, which can devise a structured plan of actions to address user queries. Its",
    "decision-making process is enhanced by a memory module, which maintains contextual\ninformation and is continuously updated based on past actions and outcomes, which help\nensure adaptive and dynamic responses.\n90 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Figure 6-2 High-level view at the core of an AI agent\nThe planning system interacts with an I/O communication layer to run the planned actions\neffectively. This layer acts as a bridge between the LLM and an extensive Tool Library, which",
    "enables the agent to perform specialized tasks beyond its intrinsic capabilities. The tools\ninclude functions such as code execution, document retrieval (RAG), calculations, weather\nqueries, time and location services, and AI and gen AI models, among others. The module of",
    "communication is a crucial part of an agent because it sets the formatting of the I/O\ncommunication in a standardized way to enable fast and transparent invocation of tools and\noutput comprehension.\nThe LLM at the core of this system is typically a large foundation model (FM) that is optimized",
    "for high performance across diverse benchmarks and tasks. For example, in the context of\nthe IBM watsonx.ai platform, this role can be fulfilled by models such as Mistral Large, Llama\n3.3 70B, or Llama 3.1 405B, which are known for their robust capabilities in generative\nreasoning and planning.",
    "reasoning and planning.\nBehind the scenes, an agent has its own called system prompt, which is the usual system\nprompt that can be set for any generative LLM. It describes in detail the main task for which a\ngenerative LLM should adhere to. The scheme behind the main prompt solution is shown in",
    "Figure6-3.\nFigure 6-3 Logical prompt schema definition for agents\nChapter 6. Artificial intelligence agents 91",
    "This system prompt is a structured set of instructions that encapsulates the core task, and\ndefines the role of the LLM, the expected output schema, and the instructions that the model\nshould follow.\nAs shown in Figure6-3 on page91, the system prompt integrates multiple critical elements:",
    "(cid:2) Memory: Provides contextual data or historical interactions that the LLM can leverage to\nmaintain continuity and relevance in responses. This approach helps ensure that the\nagent adapts dynamically to ongoing tasks or user needs.",
    "(cid:2) Role definition: Specifies the persona or role that the agent assumes (for example, a\ncustomer support assistant, a data scientist, or a creative writer), and aligns the behavior\nand tone of the LLM with the intended use case.\nExample6-1 shows an example of a role prompt definition:",
    "Example 6-1 Defining a role for the agent\n#Role\nAs a helpful assistant, your role is to provide users with actionable insights\nfrom their data files.\n(cid:2) Instructions: Detailed guidelines about how the LLM should process inputs, interpret user",
    "queries, and generate outputs. These instructions help ensure that the model stays on\ntask and adheres to the wanted operational framework.\nExample6-2 show an example of the instruction prompt definition.\nExample 6-2 Outline instructions for the agent\n# Instructions",
    "# Instructions\nThe user can see only the Final Answer. All answers must be provided there.\nFunctions must be used to retrieve factual or historical information to answer the\nmessage. If the user suggests using a function that is not available, answer that\nthe function is not available.",
    "the function is not available.\n(cid:2) Output schema: A predefined structure that dictates how the LLM should format its\nresponses, which may include JSON formats, bullet points, or other data representations\nthat are required by downstream tools or workflows. This prompt is important for the",
    "overall communication and interaction in the multiple steps in an agentic execution flow.\nExample6-3 shows an example of the output schema prompt definition.\nExample 6-3 Defining the output schema that the agent should follow\n#Output schema (also known as the agent’s control logic)",
    "You communicate only in instruction lines. The format is: \"Instruction: expected\noutput\". Only use these instruction lines and must not enter empty lines or\nanything else between instruction lines. Skip the instruction lines Function Name,",
    "Function Input, Function Caption, and Function Output if no function calling is\nrequired.\nMessage: User's message. You never use this instruction line.\nThought: A single-line step-by-step plan of how to answer the user's message. You",
    "can use the available functions that are defined above. This instruction line must\nbe immediately followed by Function Name if one of the available functions that\nare defined above needs to be called, or by Final Answer. Do not provide the\nanswer here.",
    "answer here.\nFunction Name: Name of the function. This instruction line must be immediately\nfollowed by Function Input.\nFunction Input: Function parameters. An empty object is a valid parameter.\n92 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Function Output: Output of the function in JSON format. Thought: Continue your\nthinking process.\nFinal Answer: Answer the user or ask for more information or clarification. It\nmust always be preceded by Thought.",
    "must always be preceded by Thought.\nAlso, the agent can enhance its functions with tools, which extend the LLM's capabilities by\nintegrating external APIs or specialized modules. Each tool includes the following items:\n(cid:2) A tool description: An explanation of its purpose and function.",
    "(cid:2) A tool schema: Specifications for how the LLM should interact with the tool, which includes\ninput and output formats.\nExample6-4 shows the example parsed output after a certain user request regarding an\nexplanation of a certain dataset file occurs.\nExample 6-4 Agent output example",
    "Example 6-4 Agent output example\nThought: The user wants to know what the file \"CSV- bill of materials.csv\" is\nabout. I can use the Python tool to read the file and provide a summary.\nFunction Name: Python\nFunction Input: {\"language\":\"python\",\"code\":\"import pandas as pd\\n\\ndf =",
    "pd.read_csv('CSV-billmaterials.csv')\\nprint(df.head()\",\"inputFiles\":{\"file_670d56a\nbb912d7771371652e\":\"CSV- bill of materials.csv\"}}\nFunction Output: The code ran successfully.\nStandard output: ‘’’\nQUANTITY ... PART\n0 3700 ... 144EC8-14101-20\n1 5500 ... 096EUF-T4101D20\n2 45000 ... 004ZTF-41Z01M20",
    "2 45000 ... 004ZTF-41Z01M20\n3 4334 ... NaN\n4 1564 ... NaN\n[5 rows x 4 columns] ‘’’\nBy combining these components, the system prompt orchestrates the interaction between the\nLLM and external resources, which help ensure consistency, precision, and task alignment.",
    "This modular approach enables AI agents to handle complex workflows and adapt to diverse\napplications.\nThis standard agent architecture can generate and run plans, and iteratively refine its\napproach by leveraging feedback and memory updates. The modular design of agents, which",
    "is supported by a versatile Tool Library, enables scalability and adaptability for a wide range of\nuse cases, and with the usage of agents on watsonx.ai, the scaling to enterprise-grade\nsolutions is once again possible.\nChapter 6. Artificial intelligence agents 93",
    "6.2 Why AI agents are needed\nThe necessity for AI agents stems from the growing complexity of tasks and environments\nthat surpass the capabilities of conventional software systems. Traditional systems often falter",
    "in scenarios requiring adaptive, context-aware decision-making, especially when dealing with\nvast datasets, uncertain outcomes, and real-time constraints. In domains like logistics,\nhealthcare, and finance, where decision-making must balance multiple variables",
    "simultaneously, the utility of AI agents becomes evident. These agents excel in scenarios\ndemanding real-time responses, such as autonomous vehicles navigating dynamic traffic\nconditions or virtual assistants managing multifaceted user requests.",
    "Beyond efficiency, AI agents also bring about significant cost savings by automating repetitive\ntasks, reducing human error, and improving operational accuracy. Their ability to adapt from\nexperience by using a memory module and to adapt to novel situations is crucial in",
    "addressing problems where predefined solutions are inadequate. For example, reinforcement\nlearning (RL) techniques enable AI agents to optimize behavior over time, which refines their\ndecision-making capabilities with minimal human input. By acting as adaptive problem",
    "solvers, AI agents provide a bridge between theoretical AI research and practical, real-world\nenterprise-grade applications.\nFigure6-4 shows the gen AI journey for AI agents.\nFigure 6-4 Generative AI journey for AI agents",
    "Figure 6-4 Generative AI journey for AI agents\nAgents that are powered by LLMs represent the next frontier in driving productivity gains for\nenterprises. As businesses increasingly rely on AI to streamline operations and elevate",
    "customer experiences, agents offer a revolutionary step forward by automating complex,\nmulti-step tasks that previously required human intervention. Unlike traditional LLMs, which\nexcel at handling FAQs or supporting nuanced informational queries through RAG",
    "approaches, agents bring advanced capabilities to orchestrate and run high-value workflows.\nThis ability to handle complex scenarios such as planning a marketing campaign, optimizing\nsupply chain logistics, or conducting sophisticated data analysis, positions agents as",
    "essential tools for transforming enterprise productivity.\n94 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "The true potential of agents lies in their capacity to integrate seamlessly with existing\nsystems, tools, and data sources, which enable them to act as intelligent intermediaries that\nconnect disparate workflows. For enterprises, this approach means automating entire",
    "business processes rather than isolated tasks. For example, an agent can retrieve customer\ndata, analyze purchasing patterns, generate personalized recommendations, and trigger\nactions like sending tailored offers or updating customer relationship management systems.",
    "By eliminating manual handoffs and streamlining processes, agents enable employees to\nfocus on strategic decision-making rather than repetitive or time-consuming tasks.\nFrom a business perspective, agents are the key to unlocking the next wave of productivity",
    "gains. They enhance operational efficiency, reduce costs, and drive faster time-to-market for\ncritical initiatives. Moreover, they enable businesses to scale their efforts without\nproportionally increasing resources, which are vital in today’s competitive and",
    "resource-constrained environments. Imagine an agent that can plan, run, and monitor an\nentire ad campaign or a product launch task in hours, which typically require weeks of human\neffort. This scalability improves efficiency, and it creates opportunities for innovation because",
    "teams can redirect their focus to higher-value, creative, and strategic activities. In this context,\nagents are a technological enhancement and a strategic imperative for the modern\nenterprise. They represent a shift from reactive to proactive operations, which enable",
    "businesses to anticipate needs, respond faster to market dynamics, and drive growth in ways\npreviously unimaginable. Enterprises that adopt agents will lead the charge in this new era of\nproductivity, and set the benchmark for operational excellence and innovation in their\nindustries.",
    "industries.\n6.3 Multiple AI agents\nWhen you extend the concept of individual agents in software systems, you get multi-agent\nsystems (MASs), which consist of multiple AI entities working collaboratively or competitively\nto solve complex problems within shared digital environments. These systems are",
    "transformative when augmented by gen AI capabilities because they enable agents to\nengage in more sophisticated tasks, such as content generation, dynamic interaction with\nusers, or collaborative reasoning. In a software-based MAS, each agent operates",
    "autonomously while adhering to predefined protocols for communication and collaboration.\nThis autonomy enables agents to handle tasks like distributed resource management,\nadaptive problem-solving, and even creative endeavors, such as generating ideas, plans, or",
    "personalized user experiences. Gen AI further amplifies their function by enabling natural\nlanguage generation, image synthesis, and decision-making support, which enhances the\nagents' ability to interpret, reason, and produce outputs in real time.",
    "A key challenge in software-based MASs, particularly ones that leverage gen AI, is achieving\neffective inter-agent communication and collaboration. Protocols that are based on\nmessage-passing, such as JSON over RESTful APIs or advanced graph-based",
    "communication models, help ensure that agents can share knowledge, negotiate\nresponsibilities, and resolve conflicts. Generative AI enhances these interactions by enabling\ncontext-aware dialog and summarization capabilities, which make inter-agent exchanges",
    "more natural and efficient. Moreover, coordination strategies within such systems are pivotal.\nIn leader-follower setups, generative agents with advanced reasoning capabilities may take\non supervisory roles to craft high-level plans or synthesize insights for the collective. Fully",
    "distributed MASs enable agents to collaborate dynamically by relying on peer-to-peer\nnegotiations or RL-based decision policies to achieve shared objectives. For example, in a\ncollaborative creative task such as automated marketing content generation, different agents",
    "in the system might specialize in headline creation, visual asset generation, and audience\nsentiment analysis. Together, these agents leverage gen AI to deliver cohesive, high-quality\noutputs that surpass the capabilities of individual components.\nChapter 6. Artificial intelligence agents 95",
    "The integration of a MAS with gen AI promises to unlock solutions for some of the most\nintricate software challenges, such as distributed knowledge synthesis, large-scale content\npersonalization, and dynamic adaptation to user behaviors. As these systems continue to",
    "evolve, they are poised to redefine how software systems interact, collaborate, and solve\nproblems, which pave the way for a new era of intelligent, cooperative, and gen AI-driven\napplications.\nThe decentralized nature of a MAS is critical in these addressed applications, which help",
    "scalability, robustness, and fault tolerance. For example, a generative MAS that is deployed in\ncustomer support can assign tasks dynamically across agents, with some agents generating\nempathetic responses, other agents crafting visually appealing solutions, and yet other",
    "agents analyzing sentiment data in real time. This division of labor enables the system to\nmanage workloads efficiently, respond quickly to changes, and help ensure seamless service\ncontinuity even if individual agents face disruptions.",
    "Being decentralized is one of the key advantages of a MAS. Unlike centralized systems where\na single entity controls decision-making and system-wide operations, MASs are designed to\noperate without a single point of failure. If one agent fails or is compromised, the rest of the",
    "system can continue to function, which makes it highly resilient. This decentralized approach\nalso allows MASs to scale efficiently because more agents can be added to the system\nwithout disrupting its overall performance. Furthermore, because the agents are distributed,",
    "they can operate in diverse environments, which include real-time scenarios, where traditional\ncentralized systems might struggle. The power of a MAS lies in this collective intelligence,\nwhere the sum of the parts is greater than the individual agents' abilities.",
    "As MASs continue to evolve, they are poised to unlock solutions for some of the most intricate\nchallenges in fields such as distributed computing, environmental monitoring, and\nautonomous exploration. For example, in distributed computing, a MAS can optimize",
    "resource usage across a network of machines, which enable more efficient computation and\ndata processing. In environmental monitoring, you can use a MAS to deploy a network of\nsensors that autonomously gather and process data, which provides real-time insights into",
    "environmental conditions. In autonomous exploration, a MAS can enable a fleet of robotic\nvehicles or drones to work together to explore unknown terrains, share information, and adapt\nto changes in the environment.\nA notable development in the realm of MASs is the advent of multi-agent orchestration",
    "frameworks. These frameworks provide a unified platform for coordinating conversations\namong multiple agents, serving as a high-level abstraction for using FMs. By integrating\nLLMs, tools, and human inputs, various frameworks enable the seamless orchestration of",
    "agent interactions in a way that enhances the capabilities of individual agents. These\nframeworks typically feature highly capable, customizable, and conversational agents, which\ncan collaborate through automated agent chats, which improve their collective ability to",
    "handle complex tasks. This integration enables MASs to function more efficiently by\nleveraging the power of language models, tools, and human expertise in a coordinated\nmanner. As a result, MAS frameworks offer an exciting potential for creating more intelligent",
    "and adaptive systems that can address a wide range of challenges in both industrial and\nresearch contexts.\nThe ongoing evolution of MASs promises to push the boundaries of what is possible in\nautonomous systems by enabling new capabilities and driving innovation across various",
    "industries. As researchers continue to explore the potential of these systems, new\nframeworks, coordination strategies, and interaction protocols continue to emerge, further\nenhancing the power and flexibility of MASs in tackling the most complex and large-scale\nproblems.",
    "problems.\n96 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "The architectural diagram in Figure6-5 outlines a sophisticated MAS framework that is\ndesigned to integrate diverse functions such as planning, memory management,\ncommunication, and task orchestration. The system employs a modular design that facilitates",
    "scalability, adaptability, and interoperability, which caters to complex problem-solving\nscenarios across various domains. Let us delve into each component and their interplay\nwithin the architecture.\nFigure 6-5 High-level view of a multi-agent system architecture",
    "A MAS that is enhanced by gen AI represents a sophisticated and transformative paradigm in\nAI, where multiple autonomous agents collaborate or interact within a shared environment to\nachieve complex goals. These systems are built to harness the synergy of diverse agent",
    "capabilities by leveraging the powerful reasoning, creativity, and contextual understanding of\nLLMs to tackle problems beyond the scope of any single agent. Figure6-5 lays out a\ncomprehensive blueprint for such a system. Now, we will see a detailed, expert-level",
    "explanation of its various interconnected components and their interplay.\nAt the heart of this MAS architecture lies the concept of the agent, which operates as an\nautonomous unit that is equipped with distinct roles, behaviors, and tools. Each agent is",
    "imbued with a profile or persona that defines its domain expertise, operational boundaries,\nand response style. For example, an agent might function as a scientific researcher that is\nskilled in computational chemistry or as a customer service representative with expertise in",
    "natural language understanding and resolution strategies. This persona is not static because\nit evolves in response to feedback and environmental changes, which help ensure that the\nagent remains relevant and effective in dynamic settings.",
    "Central to an agent’s operation is its belief state, which is an internal model that encapsulates\nits understanding of the world, its knowledge of tasks at hand, and its memory of past\ninteractions. This belief state is dynamic and constantly updated through observations,",
    "actions, and communications with other agents or external systems. The belief state also\nintegrates inputs from memory. Memory is bifurcated into long-term and short-term storage,\neach serving distinct roles. Short-term memory retains ephemeral information that is crucial",
    "for immediate task execution and contextual reasoning. For example, an agent might use\nshort-term memory to temporarily store user input or intermediate results of calculations. In\ncontrast, long-term memory preserves knowledge that is accumulated over time, such as",
    "procedural expertise, domain-specific facts, or historical interactions. The integration of\nmemory with the agent’s belief state helps ensure that decisions are informed by both the\nimmediate context and historical knowledge. This dual-layered memory structure enhances",
    "the system’s ability to handle complex, multi-step tasks that require contextual continuity and\nlong-term planning.\nChapter 6. Artificial intelligence agents 97",
    "--- Table 1 from page 113 ---\n |",
    "Driving the agent’s reasoning and interaction is its policy, a framework of decision-making\nrules and algorithms that govern how the agent interprets inputs, prioritizes tasks, and\ngenerates outputs. Policies can be simple, predefined heuristics or sophisticated, dynamically",
    "updated models that are informed by machine learning (ML) techniques, such as RL. These\npolicies are operationalized through the agent’s LLM core, which is the generative engine that\nprocesses natural language inputs, synthesizes insights, generates context-aware responses,",
    "and even creates novel solutions to problems. The LLM also serves as the agent’s interface to\ninterpret ambiguous or unstructured information, which effectively bridges the gap between\nhuman language and computational logic.",
    "human language and computational logic.\nAgents in this system are equipped with advanced communication capabilities that are\ndefined through a structured I/O schema that enables them to interact with other agents,\nexternal tools, and the environment. Communication is about exchanging messages,",
    "negotiating shared understandings, aligning goals, and ensuring coordinated action. To\naccomplish these tasks, agents can rely on tools that are integrated within the system. These\ntools extend the agent’s functional repertoire and include utilities like API access for external",
    "data retrieval, calculators for mathematical computations, code interpreters for debugging and\nrunning programs, and multimodal models for handling complex data types, such as images\nor video.\nFor domain-specific applications, tools like chemistry simulators or Robotic Process",
    "Automation (RPA) frameworks can be deployed, which enable agents to specialize in tasks\nthat range from molecular modeling to workflow optimization. These tools are seamlessly\nintegrated into the agent’s workflow, which enables it to run specialized tasks without\nrequiring extra human intervention.",
    "requiring extra human intervention.\nThere is no defined limit or minimum requirement regarding the number of tools that are\navailable to a set of agents: it is something dependent on the specific use cases that are",
    "addressed. What is proposed here is just a potential set of tools that might be useful for\nvarious potential use cases in a specific set of industries. In a MAS, a set of tools might\ncontain hundreds of tools, depending on what is the goal of the system.",
    "At a higher level, the MAS orchestrates the activities of these agents to help ensure that they\nwork collaboratively toward shared objectives. This orchestration is managed by several\ncritical components.\nThe first is the goal and task decomposition mechanism, which takes high-level goals and",
    "breaks them into smaller, manageable tasks and subtasks. For example, if the system’s goal\nis to generate a comprehensive market analysis, this task might be decomposed into tasks\nlike gathering data, analyzing trends, and generating a summary report, with each task that is",
    "further divided into specific steps like querying databases or visualizing data.\nTo enable task execution, the system relies on a planner that assigns tasks to the most\nsuitable agents based on their profiles, current workloads, and skillsets. The planner plays a",
    "pivotal role in the MAS by orchestrating the decomposition of high-level goals into granular\ntasks and subtasks. The planning process leverages the agent’s policy and LLM to identify\nthe optimal sequence of actions that are required to achieve the outcome. The planner also",
    "monitors progress, adapting task sequences or agent roles as needed to accommodate\nchanging conditions or unexpected challenges. For example, if one agent encounters a\nbottleneck in data retrieval, the planner can reassign related tasks to another agent with\noverlapping capabilities.",
    "overlapping capabilities.\n98 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "For critical use cases, the architecture incorporates the ability to receive human feedback\nduring the execution of the plan to help ensure that automation in high-stakes scenarios\nremains governable and aligned with human oversight. By enabling human intervention, the",
    "system can address unforeseen complexities, validate decisions, and maintain control over\ncritical operations.\nTask decomposition ensures that even complex objectives are approached methodically, with\nsubtasks delegated to the appropriate agents or tools.",
    "Collaboration within the MAS is further enhanced through sophisticated communication\npatterns, which define how agents interact and share information. The architecture supports\nmultiple communication patterns:",
    "multiple communication patterns:\n(cid:2) Layered communication: Hierarchical interactions where agents operate at different levels\nof abstraction by passing information up or down the chain.\n(cid:2) Decentralized communication: Peer-to-peer exchanges among agents to help ensure",
    "flexibility and reduce bottlenecks.\n(cid:2) Centralized communication: A hub-and-spoke model where a central coordinator\nmanages all interactions.\n(cid:2) Shared message pool: A collaborative mechanism where agents exchange messages\nthrough a shared repository.",
    "through a shared repository.\nHybrid patterns, such as shared message pools or global workspaces, enable agents to post\nintermediate results or discoveries to a common repository, which enables asynchronous\ncollaboration and emergent problem-solving. The blackboard pattern or global workspace",
    "serves as a central knowledge-sharing hub within the MAS. Agents use this shared repository\nto post updates, intermediate calculations, or unresolved queries, which create a\ncollaborative environment where other agents can contribute insights or take over pending",
    "tasks. For example, an agent working on a data analysis task might upload partial results to\nthe blackboard, which another agent can use to generate visualizations or summaries.\nThe MAS is supported by an agents and skills repository, which is a centralized directory that",
    "catalogs the capabilities, expertise, and tools that are associated with each agent. This\nrepository enables dynamic discovery and allocation of agents for specific tasks, which helps\nensure that the system can scale and adapt to diverse challenges. It also facilitates the",
    "incorporation of new skills or agents so that the system can evolve as new tools or\nrequirements emerge.\nThe architecture helps ensure that the output that is generated by the system is coherent,\ncontextually relevant, and correctly formatted. The I/O schema governs the structure of inputs",
    "and outputs by standardizing interactions across agents, tools, and external systems. This\nschema helps ensure compatibility and consistency regardless of the task or domain. The\nsystem’s communication module also plays a role in tailoring outputs to the intended",
    "audience. For example, technical results might be presented in a concise, data-rich format for\ndomain experts, but layperson-oriented outputs would emphasize clarity and simplicity.\nChapter 6. Artificial intelligence agents 99",
    "Despite its sophisticated design, the MAS must handle potential errors that can arise during\nexecution:\n(cid:2) Failed API calls to tools: A tool might be temporarily unavailable or malfunction, which can\nlead to incomplete or erroneous task execution. Human feedback can help decide whether",
    "to retry the tool, use an alternative tool, or modify the task parameters.\n(cid:2) Infinite loops: Erroneous task decomposition or planning might result in a loop where the\nsystem repeatedly runs the same actions without progress. Human intervention can",
    "identify and correct the root cause to prevent resource wastage.\n(cid:2) Rogue paths or wrong tool selection or input: The system might choose an inappropriate\ntool or misinterpret input data, which can lead to incorrect outputs. Human feedback can",
    "help realign the system’s actions to help ensure that the task remains on track.\nBy integrating human feedback into the loop, particularly for infinite loops or rogue paths, the\nMAS can maintain a high degree of reliability and robustness by implementing a fail-safe",
    "technique on human-in-the-loop interaction after deviant agents’ behavior is detected by the\noverall orchestration system. This hybrid approach of automation with human oversight is\nespecially critical in high-stakes domains where errors can have significant consequences.",
    "The MAS operates within an environment, such as the external world or a digital context\nwhere tasks are performed and results are applied. Agents interact with the environment by\nobserving its state, acting to modify it, and processing feedback to refine their belief states",
    "and policies. The environment is also where the whole MAS runs, and it is possible to have\nenvironments that are composed of multiple locations, such as cloud environments, virtual\nmachines (VMs) and containers, or further proprietary software.\n6.4 AI agents on watsonx.ai",
    "6.4 AI agents on watsonx.ai\nIBM watsonx.ai enhances the development of an enterprise-grade agentic technology stack\n(Figure6-6) by providing powerful tools, models, and middleware capabilities that are tailored\nfor scalable, intelligent, and adaptive operations.",
    "Figure 6-6 Enterprise agentic tech stack\n100 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "At the foundation level, watsonx.ai delivers robust LLMs optimized for specific enterprise use\ncases to enable agents to interpret and act on complex queries with high accuracy and\nrelevance. These LLMs seamlessly integrate into the Agentic Framework, which serves as",
    "the operational backbone for orchestrating tasks, planning workflows, and retaining memory\nfor context-driven decision-making.\nIBM watsonx.ai also supports the Agentic Service Deployment & Operation Platform, which",
    "offers a streamlined way to deploy and manage agentic services while ensuring reliability,\nscalability, and security at an enterprise level. To ensure enterprise-grade observability and\nmonitoring, watsonx.ai incorporates observability mechanisms that provide real-time insights",
    "into agent performance, operational health, and user interactions. These mechanisms\nfacilitate continuous optimization and help ensure that agentic services align with business\ngoals. By bridging models, middleware, and applications, watsonx.ai creates a cohesive and",
    "modular enterprise tech stack that can address the evolving demands of modern businesses\nwith precision, adaptability, and innovation.\nwatsonx.ai agents are a transformative innovation in the domain of AI. These agents are",
    "designed to provide businesses with unparalleled capabilities in automating tasks, processes,\nand decision-making. Through a blend of interfaces, cutting-edge technologies, and robust\nintegration options, watsonx.ai enables the development, deployment, and optimization of",
    "intelligent agents that cater to a diverse range of enterprise needs.\nAt the core of watsonx.ai's agentic capabilities is the Agent Builder (Figure6-7), which is an\nintuitive and powerful tool that accelerates the development lifecycle. The visual interface of",
    "the Agent Builder enables developers to construct agents with ease, which reduces the\ncomplexity that is typically associated with designing and managing such systems. Agents\nwithin watsonx.ai are defined through natural language instructions, and they can be",
    "equipped with various tools to expand their functions. These tools act as modular building\nblocks to enable developers to create sophisticated workflows that are tailored to specific\nrequirements.\nFigure 6-7 Agent Builder view on the watsonx.ai UI\nChapter 6. Artificial intelligence agents 101",
    "One of the standout features of the Agent Builder is its seamless integration with multiple\nagent frameworks. In addition to IBM proprietary technologies, developers can also leverage\npopular open-source frameworks like LangChain and LangGraph. This flexibility helps ensure",
    "that businesses can use the best tools and methodologies that are available in the\necosystem, and adapt them to their unique operational needs. The ability to integrate\nopen-source solutions with IBM advanced technologies provides a level of customization and",
    "extensibility that is critical for modern enterprises, as shown in Figure6-8.\nFigure 6-8 Overview of customizations on watsonx.ai Agent Builder\nTesting and debugging are essential components of the agent development process, and",
    "watsonx.ai Agent Builder excels in this area. Real-time testing capabilities enable developers\nto identify and resolve issues as they arise, which minimizes downtime and iteration cycles.\nThis feature is complemented by the “one-click” deployment mechanism, which simplifies the",
    "process of making agents operational. Once developed, agents can be deployed as\nwatsonx.ai AI services, which effectively turn them into API endpoints that can be accessed\nby various applications and systems. This streamlined workflow reduces time-to-market and",
    "helps ensure that agents can be quickly integrated into enterprise operations.\nTo further enhance the functions of agents, watsonx.ai offers an extensive Tool Library of\nenterprise-ready tools that are designed to augment the capabilities of agents. The tools are",
    "divided into the following categories:\n(cid:2) Web Search\n(cid:2) Document Search (RAG)\n(cid:2) Code Execution\n(cid:2) Data Connectors\n(cid:2) Custom Tool Builder\nFor example, the Web Search tool empowers agents to perform real-time internet searches,",
    "which provide them with up-to-date information to enhance their decision-making and\nresponses. The ability to access fast and relevant search results helps ensure that agents\nremain informed and capable of handling dynamic queries.",
    "102 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Another critical component of the Tool Library is the Document Search function, which uses\nRAG. This tool enables agents to efficiently index and retrieve documents from an\norganization’s knowledge base, which helps ensure that they can deliver accurate and",
    "context-aware responses. By leveraging RAG, agents can access vast amounts of\ninformation and distill it into actionable insights, which makes them invaluable for\nknowledge-intensive tasks.\nThe Tool Library also includes a Code Execution feature, which enables agents to run Python",
    "code in real time. This capability opens up a wide range of possibilities, from performing\ncomplex calculations to automating repetitive tasks. By integrating this feature, agents can\noperate as dynamic problem solvers that can adapt to various scenarios.",
    "Data accessibility is another cornerstone of the watsonx.ai agentic framework. With Data\nConnectors, agents can seamlessly interact with enterprise databases and data warehouses,\nwhich grant them access to critical organizational data. This tool helps ensure that agents",
    "operate with a comprehensive understanding of the business context, which enables more\ninformed and effective decision-making.\nThe Tool Library supports the creation of custom tools so that organizations can extend the",
    "functions of their agents by integrating them with external services and unique enterprise\nsystems. This level of customization helps ensure that agents can meet the specific demands\nof any organization.\nDeployment is a critical phase in the lifecycle of any AI system, and watsonx.ai provides a",
    "robust, framework-neutral solution for deploying agents. The deployment process is scalable,\nsecure, and highly available, which helps ensure that agents can meet the demands of\nenterprise-scale operations. Whether an organization requires a single agent for a specific",
    "task or a fleet of agents to handle complex workflows, the watsonx.ai deployment capabilities\ncan handle the challenge. Once deployed, the performance and reliability of agents must be\nmonitored to ensure that they operate as intended. watsonx.ai includes comprehensive",
    "monitoring tools that track key performance indicators (KPIs) and analyze logs. These tools\nprovide valuable insights into agent behavior, which enable developers and administrators to\nidentify areas for improvement and help ensure that agents deliver optimal results.",
    "watsonx.ai emphasizes transparency and explainability, which are crucial for building trust in\nAI systems. By offering detailed explanations of agent decisions and actions, the platform\nhelps organizations maintain compliance with regulatory requirements and ethical standards.",
    "Looking to the future, watsonx.ai is poised to introduce the Flows Engine, which is a\nlightweight agentic framework and tool-building platform that will further enhance the\ncapabilities of the platform. The Flows Engine will enable developers to rapidly build custom",
    "tools and integrate them with enterprise IT systems, which will provide unparalleled flexibility.\nThis framework is designed to facilitate reasoning and complex decision-making, which will\nmake agents more effective in handling intricate tasks. Also, the Flows Engine will include a",
    "chat UI widget that can be easily integrated into third-party applications, which will enable\nseamless AI-mediated interactions.\nChapter 6. Artificial intelligence agents 103",
    "Complementing watsonx.ai is IBM watsonx Orchestrate, which is a platform that focuses on\nstarting AI assistants and agents for business processes and task automation (Figure6-9).\nBy combining the capabilities of watsonx.ai and watsonx Orchestrate, organizations can",
    "achieve end-to-end automation to streamline operations and drive efficiency across their\nworkflows. This synergy highlights IBM’s commitment to providing comprehensive AI\nsolutions that address the diverse needs of modern enterprises.",
    "Figure 6-9 Overview of watsonx Orchestrate for Agents capabilities\nThe integration of watsonx Orchestrate and watsonx.ai offers an advanced, enterprise-grade\nframework that enhances agentic support by uniting robust automation, intelligent workflows,",
    "and conversational AI capabilities. watsonx Orchestrate functions as a supervisory agent that\nleverages LLMs to coordinate interactions across customers, employees, subject matter\nexperts (SMEs), and applications. By employing Skills Studio, watsonx Orchestrate enables",
    "the discovery, creation, and management of gen AI and digital automations by combining\ntasks, workflows, and skills to drive seamless operational efficiency. Skills and automations\ncan be trained and published into a comprehensive skills catalog, which includes prebuilt",
    "integrations with enterprise solutions such as SAP, Salesforce, and ServiceNow, to help\nensure compatibility with diverse enterprise systems. Simultaneously, watsonx.ai powers\nintelligent AI assistants, enabling conversational experiences that feature advanced functions",
    "like slot filling, LLM routing, disambiguation, and digressions. This combination helps ensure\npersonalized and context-aware interactions. Together, watsonx Orchestrate and watsonx.ai\nestablish a scalable ecosystem to enable enterprises to simplify complex processes, reduce",
    "operational bottlenecks, and deliver intuitive, guided experiences that align AI-driven solutions\nwith business objectives in a dynamic, flexible, and secure manner.\nBecause of these agentic frameworks on watsonx.ai that are combined with watsonx",
    "Orchestrate, you can use advanced assistants that can perform many types of tasks.\nFigure6-10 on page105 provides a comprehensive depiction of Assistants with Agents on\nwatsonx, which shows the integration of watsonx.ai and watsonx Orchestrate to enable",
    "sophisticated AI-driven solutions for enterprise workflows.\n104 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Figure 6-10 Assistant with Agents that use watsonx.ai and watsonx Orchestrate for agentic use cases\nAt the core of this design is the concept of a unified, intelligent assistant framework that\ncombines modularity, scalability, and precision to deliver seamless experiences across",
    "diverse use cases and tasks. Each component in Figure6-10 plays a critical role in\norchestrating complex interactions between users, agents, tools, and workflows, which create\na robust and adaptable ecosystem for AI-powered operations.",
    "At the top of the architecture is the Unified Assistant, which serves as the single, user-facing\ninterface. This layer is designed to provide a consistent and coherent interaction experience\nby simplifying user engagement and abstracting the complexities of the underlying system.",
    "The Unified Assistant is supported by the Supervisory AI Meta-Agent, which is a central\ncoordinator that facilitates all workflows and helps ensure that user requests are processed\naccurately and efficiently. The meta-agent acts as the brain of the system by orchestrating the",
    "activities of multiple subordinate agents and tools to fulfill user intents. Its responsibilities\ninclude routing tasks to the appropriate agents, resolving ambiguities in user input, starting\nthe necessary tools, reasoning through multi-step problems, and planning actions to achieve",
    "outcomes. These capabilities are made possible by the integration of LLMs that are powered\nby watsonx.ai, which provides the advanced natural language understanding, contextual\nawareness, and reasoning abilities that are needed for high-quality interactions.",
    "Chapter 6. Artificial intelligence agents 105",
    "--- Table 1 from page 121 ---\n |",
    "Beneath the Supervisory AI Meta-Agent lies a network of specialized assistants that are\nlabeled as Assistant 1, Assistant 2, and so on, which represents a modular and scalable\napproach to task execution. Each assistant is tailored to handle specific domains or functions,",
    "and they operate autonomously while contributing to the overall system. These assistants are\ndesigned to run actions independently, and they leverage the power of LLMs for NLP,\nmulti-turn conversations, and decision-making. This autonomy enables them to reduce",
    "manual intervention so that organizations can achieve higher levels of efficiency and\nproductivity. The assistants also facilitate user interactions by maintaining context,\nunderstanding intent, and responding dynamically to evolving requirements.",
    "A defining feature of this architecture is its reliance on tool calling, which forms the backbone\nof the system’s operational flexibility and extensibility. Figure6-10 on page105 emphasizes\nthe integration of various categories of tools that the agents can call to complete tasks. These",
    "tools include fixed flows for handling repetitive and standardized operations; knowledge\nrepositories for answering questions and providing insights; APIs for interacting with external\nsystems; business automation modules for streamlining enterprise processes; and",
    "multi-agent frameworks for coordinating complex tasks that require collaboration among\nseveral AI agents. The usage of LLM-based tool calling helps ensure that the system can\nadapt to various workflows, which enables interoperability with existing IT infrastructures and",
    "third-party applications. watsonx.ai enhances this capability through its extensive Tool Library,\nwhich includes features such as RAG for knowledge discovery; real-time Python code\nexecution for computational tasks; and seamless data connectors for integrating with",
    "enterprise databases and services. Custom tools can also be developed and incorporated,\nwhich enable organizations to tailor the system to their unique needs and challenges.\nThe bottom part of Figure6-10 on page105 implicitly connects to watsonx Orchestrate,",
    "which is the IBM platform for deploying and managing AI-driven workflows. By leveraging\nwatsonx Orchestrate, this architecture gains enterprise-grade capabilities for task automation,\ngovernance, and monitoring. This integration enables organizations to deploy AI assistants",
    "quickly and securely, with the ability to scale the system as the number of tasks, agents, and\nintegrations grows. Security and compliance are also ensured, which addresses the stringent\nrequirements of enterprise environments. The orchestration layer further optimizes",
    "performance by streamlining the deployment and management of agents, which minimizes\noperational overhead while maximizing system reliability.\nThe overall interaction flow in Figure6-10 on page105 begins with the user engaging with the",
    "Unified Assistant. The user’s input is processed by the Supervisory AI Meta-Agent, which\napplies its routing, reasoning, and tool-calling capabilities to determine the best course of\naction. Then, tasks are delegated to the appropriate assistants, which run them",
    "autonomously by using the integrated tools and workflows. The outputs and actions from\nthese assistants are aggregated by the meta-agent and presented to the user, which helps\nensure a cohesive and intuitive experience. This flow highlights the system’s ability to handle",
    "diverse and complex tasks while maintaining a simple interface.\nIn conclusion, watsonx.ai agents represent a revolutionary approach to enterprise AI by\noffering a powerful combination of flexibility, function, and scalability. From the intuitive Agent",
    "Builder to the expansive Tool Library and forthcoming innovations like the Flows Engine,\nwatsonx.ai empowers businesses to create intelligent agents that drive productivity and\ninnovation. By leveraging these advanced capabilities, organizations can harness the full",
    "potential of AI to achieve their strategic goals and maintain a competitive edge in an\nincreasingly digital world.\n106 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "6.5 AI agents use case examples\nAI agents are redefining the landscape of business operations and service delivery by\nunlocking unprecedented efficiencies and enabling more personalized, data-driven\ndecision-making. Across industries, these intelligent systems have found applications in",
    "areas such as customer service and support, sales and marketing automation, operational\nefficiency, financial advisory, healthcare, and supply chain management.\nThe following sections provide a detailed exploration of select use cases, and highlight their\nimpact and capabilities.",
    "impact and capabilities.\nCustomer service and support agents AI agents\nAI agents are revolutionizing customer service by offering continuous assistance. By using\ntools like watsonx.ai Web Search and Document Search (powered by RAG), these agents",
    "provide timely and accurate responses to customer inquiries. By leveraging NLP and ML, they\ncan understand context, resolve issues, and escalate complex cases to human agents when\nnecessary. For example, chatbots that are built by using watsonx.ai Agent Builder can be",
    "deployed as API endpoints to handle FAQs, help with troubleshooting, and manage order\ntracking. These agents help ensure instant support while reducing operational costs,\nimproving customer satisfaction, and fostering loyalty.\nSales and marketing automation",
    "Sales and marketing automation\nAI agents are indispensable tools in sales and marketing, where personalization and timely\ninteractions drive success. Powered by watsonx.ai Data Connectors, agents can analyze\ncustomer behavior, qualify leads, and deliver tailored recommendations. By automating",
    "follow-ups and dynamically adjusting strategies by using real-time insights, these agents\nenhance engagement and drive conversions. For example, e-commerce platforms can use\nwatsonx.ai agents to suggest personalized items, send promotional offers, and re-engage",
    "customers who abandon carts. Such integrations help businesses maximize revenue\npotential while delivering highly customized customer experiences.\nOperational efficiency and process automation\nAutomating repetitive and routine tasks is a hallmark of AI agents, and watsonx.ai provides",
    "the tools to optimize these processes. By using the Code Execution capability, agents can run\nPython scripts in real time to process data, verify documents, or automate workflows. Beyond\ntask automation, these agents can monitor workflows, identify bottlenecks, and recommend",
    "improvements. For example, administrative tasks like employee onboarding can be fully\nautomated with watsonx.ai by processing documentation, setting up accounts, and\nscheduling orientation sessions, which free employees to focus on more strategic\nresponsibilities.",
    "responsibilities.\nHealthcare assistants and patient care\nIn healthcare, AI agents improve patient care and operational efficiency by acting as virtual\nassistants. By integrating with watsonx.a Tool Library and leveraging custom tools, these",
    "agents can manage schedules, send medication reminders, and provide basic medical\nguidance. For example, telemedicine platforms can deploy agents to conduct preliminary\nsymptom checks, monitor health metrics such as heart rate or blood pressure, and alert",
    "providers to abnormalities. These capabilities reduce the workload on healthcare staff while\nensuring proactive and timely patient care, which enhances outcomes and satisfaction.\nChapter 6. Artificial intelligence agents 107",
    "Supply chain and logistics optimization\nAI agents are transforming supply chain management by combining real-time data with\npredictive analytics. Tools like watsonx.ai Data Connectors enable agents to forecast\ndemand, manage inventory, and plan delivery routes effectively. Logistics companies can",
    "leverage these capabilities to analyze historical patterns, predict future needs, and maintain\noptimal stock levels. Also, AI-driven route optimization, when informed by traffic and weather\ndata, helps ensure timely deliveries and reduces costs. With watsonx.ai, organizations can",
    "build scalable, secure agents that streamline supply chain operations, which enhance both\nefficiency and customer satisfaction.\n108 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "7\nUse cases\nChapter 7.\nThis chapter describes two separate use cases and shows what problems IBM watsox.ai\ntools can solve. It also describes a framework that outlines how companies who are trying to\nprepare for the future are thinking about use cases of the future to keep ahead of the curve.",
    "The following topics are described in this chapter:\n(cid:2) 7.1, “Using RAG to aid a medical school admissions office” on page110\n(cid:2) 7.2, “Embedding workflow automation to streamline recommendations” on page111\n© Copyright IBM Corp. 2025. 109",
    "7.1 Using RAG to aid a medical school admissions office\nAs a quick refresher, Retrieval-Augmented Generation (RAG) is a technique that combines\ninformation retrieval and language model generation to provide precise and contextually",
    "relevant responses to user queries. It works by first retrieving relevant documents or\npassages from a large corpus of documents by using a retrieval step, and then feeds these\npassages along with the original query into a large language model (LLM) to generate a\nresponse.",
    "response.\nRAG is one of the most commonly used techniques that are used in production AI workloads.\nIt is used in various ways, such as in question-answering systems, chatbots, and digital\nworkers. One powerful tool that can be used in these systems is a summary of ingested",
    "documents. This section describes how a leading medical school in the US turned to the\nwatsonx.ai platform to enable it to accomplish its goals.\n7.1.1 The challenge\nA leading medical school in the US decides to offer tuition-free education to its admitted",
    "students. They anticipated a surge in applications. To help manage the expected increase in\napplications, the institution turned to watsonx.ai. They hoped that IBM could provide a\ntechnology solution to help the admissions committee efficiently process and review the\nincoming applications.",
    "incoming applications.\n7.1.2 The solution\nWorking with the medical school, the IBM team developed an innovative solution by using\nwatsonx.ai to generate one-page abstracts that summarized the incoming 50 - 70-page",
    "applications. The incoming applications included essays, with each application containing\n5 - 8 essays that varied from several paragraphs to several pages. The\nIBM granite-13b-chat-v2 model within the watsonx.ai platform was used to generate a 1 - 2",
    "paragraph summary of each of the essays, which was included with the application abstracts.\nFigure7-1 shows the watsonx.ai workflow that accomplished this task.\nFigure 7-1 watsonx.ai workflow example\n110 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "7.1.3 Special considerations\nFrom the start of this project, both IBM and the medical school recognized the importance of\ndeveloping a solution that met the institution's needs and aligned with IBM AI Ethics. The",
    "usage of AI in the admissions process raised important ethical considerations, such as\nhelping ensure fairness, transparency, and accountability. The goal was to create a system\nthat would augment human decision-making rather than replace it, and provide the",
    "admissions committee with the tools that they needed to make informed decisions.\nThe project incorporated a range of both technical and non-technical guardrails to address AI\nethics considerations throughout the project lifecycle:",
    "(cid:2) Augmenting human decision making: The solution was designed to support human\ndecision-makers, and not replace them. The AI-powered pipeline generated summaries\nand identified key information, but all decisions remained in the hands of humans.",
    "(cid:2) Education and training: The IBM team provided ongoing education and training on AI to\nboth technical and business users to help ensure that everyone that was involved in the\nproject understood the capabilities and limitations of the technology.",
    "(cid:2) Thresholds and AI notices: The team implemented technical guardrails by using\nwatsonx.governance to detect and prevent potential biases or errors in the system.\n(cid:2) Feedback mechanism: The IBM team established a continuous feedback loop with the",
    "client to refine and improve the solution over time.\nBy considering these guardrails from the beginning, the project was able to meet the\ninstitutions needs while aligning to their governance frameworks regarding fairness,",
    "transparency, and accountability. It also limited the school’s risk exposure and reduced the\nlikelihood of having to redesign the system to incorporate new safeguards because decision\npoints were integrated from the start.\n7.2 Embedding workflow automation to streamline\nrecommendations",
    "recommendations\nThis use case describes at how workflow automation can lead to directly addressing the\nwants and needs of a financial institution, and act as a potential opportunity pipeline for the\nbanks serving their customers.\n7.2.1 The challenge",
    "7.2.1 The challenge\nSmall local and regional banks, especially ones serving customers in more rural settings,\noften face different challenges than the large banks that service most of the total addressable\nmarket. Some of these challenges are self-evident, such as having comparatively limited",
    "access to capital, but other challenges are less apparent, such as an increased need to rely\non low- and no-code solutions to maintain technological parity with the custom-built\napplications that are designed and managed by large centralized IT teams that are staffed by\nlarger finance institutions.",
    "larger finance institutions.\nFor one of these banks servicing rural clients in the Midwest region of the US, they wanted to\naddress some of the market trends they read about in 5 banking customer experience trends",
    "to consider for 2024, with a specific focus on providing customers with immediate service and\npersonalized recommendations.\nChapter 7. Use cases 111",
    "In these rural settings, it is often difficult for a bank’s customer base to travel to the nearest\nbranch, and at specific times of the year, the journey takes time away from their\nresponsibilities at farms, ranches, and processing centers, which directly impacts their annual\nincome.",
    "income.\n7.2.2 The solution\nBy leveraging a collection of tools, this regional bank was able to build a solution that\nincorporated three IBM tools to reduce friction with its users while leveraging largely pre-built",
    "skills and solutions. The solution centered on a watsonx Orchestrate business automation\napplication that takes in loan applications and responds to the loan applicant with\nnear-real-time approval or rejection notices based on thresholds that are set by the bank.",
    "Figure7-2 shows the watsonx Orchestrate workflow that was used in this use case.\nFigure 7-2 watsonx Orchestrate workflow example\nCustomers connected to the bank through IBM watsonx Assistant, which leveraged an IBM",
    "LLM to initiate a natural language dialog with the client and collect various loan application\ndocuments, which included custom forms that are specific to this bank. These documents\nwere passed from watsonx Assistant to watsonx Orchestrate, which called on several of its",
    "prebuilt skills, and custom skills that were developed in the Skills Studio feature to access\nadditional services outside of the bank (such as credit reports).\nAfter running through a decision tree based on the inputs, the client received an approval or",
    "rejection notification. In either case, the notice was sent back to the applicant through a\ncustom-generated response that was tailored by watsonx.ai and specific to that customer,\nwith an explanation of the decision based on that customer’s specific application.",
    "Furthermore, the bank leveraged its own client knowledge to augment the loan decision with\nadditional offers or suggestions to the customer based on their specific customer profile. For\nthose customers that applied for an automotive loan and had a mortgage and business",
    "account with the bank, the bank suggested that they sign up for a wealth management\naccount that was serviced by the bank. For those customers who applied for a small business\nloan who did not have a checking or savings account that was associated with the EIN on the",
    "application, the bank suggested that they open a full suite of business accounts with the loan.\nThese examples pulled on watsonx.ai generative capabilities to create personalized\nrecommendations based on individual customers rather than “one-size-fits-all” templates that",
    "are applied as a blanket policy to all applications. Holistically, this entire solution can be\nsummarized as an AI agent.\n112 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "7.2.3 Special considerations\nWith a smaller workforce to dedicate to this solution and limited previous AI experience, the\nbank leveraged existing tools to reduce the workload and expertise requirements on the\nbank’s workforce. This approach was one of the key reasons IBM and the client focused on",
    "leveraging watsonx Orchestrate over designing a custom application that integrated AI tools\nthrough API calls. Instead of building a solution, the bank relied on built-in AI functions that\nautomatically combined pre-packaged skills dynamically and in-context based on",
    "organizational knowledge and prior interactions to help workers design the workflow of their\napplication. Users provided natural language inputs to select and sequence the required skills\nfor a task, and watsonx Orchestrate connected them with the associated applications, tools,",
    "data, and historical details. This approach enabled the team to automate processes without\nneeding highly specialized IT skills or expert knowledge of business processes and\napplications.\nChapter 7. Use cases 113",
    "114 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Abbreviations and acronyms\nAI artificial intelligence\nAIaaS AI as a Service\nBYOM Bring Your Own Model\nCLI command-line interface\nCNN convolutional neural network\nDL deep learning\nDQN Deep Q-Network\nESG environmental, social, and\ngovernance\nFM foundation model\ngen AI Generative AI",
    "FM foundation model\ngen AI Generative AI\nIBM International Business Machines\nCorporation\nKNN k-nearest neighbor\nKPI key performance indicator\nLLM large language model\nLLMOps large language model operations\nLoRA low-rank adaptation\nMAS multi-agent system\nMDP Markov decision processes",
    "MDP Markov decision processes\nML machine learning\nMLOps machine learning operations\nMMLU Massive Multitask Language\nUnderstanding\nNLP natural language processing\nPCA Principal Component Analysis\nQLoRA quantized low-rank adaptation\nRAG Retrieval-Augmented Generation\nRL reinforcement learning",
    "RL reinforcement learning\nRNN recurrent neural network\nRPA Robotic Process Automation\nSaaS Software-as-a-Service\nSDG synthetic data generation\nSME subject matter expert\nUI user interface\n© Copyright IBM Corp. 2025. 115",
    "116 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai",
    "Related publications\nThe publications that are listed in this section are considered suitable for a more detailed\ndescription of the topics that are covered in this book.\nIBM Redbooks\nThe following IBM Redbooks publications provide additional information about the topics in",
    "this document. Some publications that are referenced in this list might be available in softcopy\nonly.\n(cid:2) Simplify Your AI Journey: Ensuring Trustworthy AI with IBM watsonx.governance,\nSG24-8573\n(cid:2) Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.data, SG24-8570",
    "You can search for, view, download, or order these documents and other Redbooks,\nRedpapers, web docs, drafts, and additional materials, at the following website:\nibm.com/redbooks\nOnline resources\nThese websites are also relevant as further information sources:",
    "(cid:2) Code samples of common machine learning scenarios:\nhttps://github.com/IBM/watson-machine-learning-samples\n(cid:2) Examples of using Instructlab and AI agents:\nhttps://github.com/IBM/watsonx-ai-platform-demos\n(cid:2) IBM AI risk atlas",
    "(cid:2) IBM AI risk atlas\nhttps://www.ibm.com/docs/en/watsonx/w-and-w/2.1.x?topic=ai-risk-atlas\n(cid:2) IBM watsonx documentation (Includes links to all watsonx products)\nhttps://www.ibm.com/docs/en/watsonx\n(cid:2) IBM watsonx.governance product\nhttps://www.ibm.com/products/watsonx-governance",
    "https://www.ibm.com/products/watsonx-governance\n(cid:2) IBM watsonx product portfolio\nhttps://www.ibm.com/watsonx\n© Copyright IBM Corp. 2025. 117",
    "Help from IBM\nIBM Support and downloads\nibm.com/support\nIBM Global Services\nibm.com/services\n118 Simplify Your AI Journey: Unleashing the Power of AI with IBM watsonx.ai\n\n90<->249\npages\n0.17”<->0.473”\n(0.2”spine)\nSimplify\nYour\nAI\nJourney:\nUnleashing\nthe\nPower\nof\nAI\nwith\nIBM\nwatsonx.ai",
    "--- Table 1 from page 135 ---\nSimplify\nYour\nAI\nJourney:\nUnleashing\nthe\nPower\nof\nAI\nwith\nIBM\nwatsonx.ai\n\n\nBack cover\nSG24-8574-00\nISBN 0738461989\nPrinted in U.S.A.\n®\nibm.com/redbooks"
]